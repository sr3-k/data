Name,Status,Certainty,Single cluster?,Max OP/s (log),H100 equivalents,Chip type (primary),Chip quantity (primary),Country,Owner,First Operational Date,Note,Sector,Power Capacity (MW),Hardware Cost,Location,Users,Hardware note,Quote,First Operational Date Note,Certainty Note,Energy Efficiency (log),Builds Upon,Superseded by,Possible Duplicate,Possible Duplicate Of,Chip type (secondary),Chip quantity (secondary),Total number of AI chips,GPU Supplier (primary),GPU supplier (secondary),Include in Standard Analysis,Exclude,Rank when first operational,16-bit OP/s (log),Max OP/s,8-bit OP/s,16-bit OP/s,32-bit OP/s,Calculated Power Capacity (MW),Reported Power Capacity (MW),Energy Efficiency,Calculated Cost,Reported Cost,Reported Cost (Inflation adjusted),Cost Quote,Noteworthy,Decommissioned Date (if applicable),Largest existing cluster when first operational,% of largest cluster when first operational,Source 1,Source 2,Source 3,Source 4,Source 5
xAI 5 year plan,Planned,Unlikely,No,22.99563519,50025265.29,,,,xAI,,Plans for their overall capacity in 2030,,,,,,Uncertain,"""The xAI goal is 50 million in units of H100 equivalent-AI compute (but much better power-efficiency) online within 5 years""",Planned 2030,,,,,,,,,,Unknown,,,,,,9.90E+22,9.90E+22,,,,,,,,,,,,,,https://archive.ph/RTF6i,,,,
Abu Dhabi UAE/USA 5GW Campus Phase 2,Planned,Likely,Unclear,22.60314437,20262758.97,,,United Arab Emirates,,,FLOP/s is a very rough estimate based on stated power capacity and the completion year,Public/Private,5000,,"Abu Dhabi, United Arab Emirates","US Companies,G42",Uncertain,"""Breaking ground on 1GW AI Datacenter, part of a planned 5GW UAE-US artificial intelligence campus; largest such deployment outside of the US""
""The campus, which will span 10 square miles within the Emirate, will be built by G42 and operated in partnership with several US companies.""",Planned 2030,,12.60422605,Stargate UAE Phase 2,,,,,,,Unknown,,,,,22.30319606,4.01E+22,4.01E+22,2.01E+22,,,5000,4.02E+12,,,,,TRUE,,,,https://web.archive.org/web/20250515200336/https://www.commerce.gov/news/press-releases/2025/05/uae-and-us-presidents-attend-unveiling-phase-1-new-5gw-ai-campus-abu,,,,
Meta $200B Campus Rumor,Planned,Unlikely,Yes,22.60314437,20262758.97,,,United States of America,Meta AI,,"FLOP/s is a very rough estimate based on stated power capacity  (~5-7GW) and the completion year. Plans for this cluster are very much still rumors/vauge, and it's highly likely that they're changed. 
2030 is a rough estiamted date for when this might be completed",Private,5000,2E+11,,Meta,Uncertain,"""could cost over $200 billion""",Planned 2030,,12.60422605,,,,,,,,Unknown,,,,,22.30319606,4.01E+22,4.01E+22,2.01E+22,,,5000,4.02E+12,,2E+11,2E+11,$200B,,,,,https://web.archive.org/web/20250226194100/https://finance.yahoo.com/news/meta-weighs-200b-ai-data-134016347.html,,,,
DataVolt Neom 1.5 GW Phase 2,Planned,Likely,Unclear,22.00432137,5103587.671,,,Saudi Arabia,DataVolt,,Planned year is a very rough guess,Private,1500,,"27°33'21.6""N 35°32'18.4""E",,Uncertain,"""At the LEAP 2025 event this week DataVolt and Neom signed a $5bn agreement to develop a 1.5GW data center campus in Neom’s Oxagon industrial area.
The first 300MW phase of the site will reportedly go live in 2028""",Planned 2031,,12.52374647,DataVolt Neom 1.5 GW Phase 1,,,,,,,Unknown,,,,,21.69983773,1.01E+22,1.01E+22,5.01E+21,,,1500,3.34E+12,,5000000000,,,,,,,http://web.archive.org/web/20250404231606/https://www.datacenterdynamics.com/en/news/datavolt-plans-15gw-data-center-campus-in-neoms-oxagon/,,,,
Meta Louisiana Datacenter,Planned,Likely,Unclear,22.00432137,5103587.671,,,United States of America,Meta AI,,FLOP/s is a very rough estimate based on stated power capacity and the completion year,Private,2260,,"Holly Ridge
Louisiana 71269",Meta,Uncertain,"""Mark Zuckerberg said the site would total more than 2GW at full build-out and would be used to train its Llama AI models.""
""This will include the construction of three combined-cycle combustion turbines with a combined capacity of 2,260MW""",Planned 2030,,12.34572929,,,,,,,,Unknown,,,,,21.69983773,1.01E+22,1.01E+22,5.01E+21,,,2260,2.21681E+12,,,,,,,,,https://archive.ph/rS3ef,,,,
South Korea Planned 3GW Cluster,Planned,Likely,Unclear,22.00432137,5103587.671,,,Korea (Republic of),,,FLOP/s is a very rough estimate based on stated power capacity and the completion year,Private,3000,,southwest South Korea,,Uncertain,"If completed as envisioned by its backers, the data center will cost as much as $35 billion and pack up to 3 gigawatts of power.",Planned 2028,,12.22271647,,,,,,,,Unknown,,,,,21.69983773,1.01E+22,1.01E+22,5.01E+21,,,3000,1.67E+12,,,,,,,,,https://www.wsj.com/tech/ai/ai-data-center-with-up-to-3-gigawatts-of-power-is-envisioned-for-south-korea-5141bd77,,,,
OpenAI Stargate Abilene Oracle OCI Supercluster Phase 3,Planned,Likely,Yes,22.00432137,5103587.671,,,United States of America,Oracle,,"""That cluster is expected to be leased to Oracle, which in turn will lease it to Microsoft, who will rent it to OpenAI""
Crusoe is involved in this
FLOP/s figure is a very rough estimate based off some rumored specifications about chip performance",Private,2200,,"5502 Spinks Rd, Abilene, TX 79601","Cloud,Microsoft,OpenAI","VR200,GB300,GB200","""The initial plan here was a 1GW facility; SA now thinks it’s going to have a total of 2.2GW of power supplied and a critical IT capacity of 1.8GW.""
""SA estimates the campus will house a previously-announced 100k GB200 cluster, as well as a 200k GB300 cluster and a 400k VR200 cluster""",Planned 2027,Low confidence since plans are still somewhat up in the air,12.35741505,OpenAI Stargate Abilene Oracle OCI Supercluster Phase 2,,,,,,,Unknown,,,,,21.69983773,1.01E+22,1.01E+22,5.01E+21,,,2200,2.27727E+12,,,,,TRUE,,,,https://archive.ph/B3HJm#selection-1045.0-1045.83,https://archive.ph/UeNZs,https://web.archive.org/web/20250124161138/https://www.transformernews.ai/p/unravelling-the-stargate-spin,,
Stargate UAE Phase 2,Planned,Likely,Unclear,21.69983773,2531581.607,,,United Arab Emirates,Stargate (OpenAI),,"Part of the broader planned 5GW UAE/USA cluster
Planned date is a rough guess
FLOP/s is a very very rough guess based on power capacity and expected completion date",Public/Private,1000,,"Abu Dhabi, United Arab Emirates",OpenAI,Uncertain,"""Breaking ground on 1GW AI Datacenter, part of a planned 5GW UAE-US artificial intelligence campus; largest such deployment outside of the US""
""The campus, which will span 10 square miles within the Emirate, will be built by G42 and operated in partnership with several US companies.""
""Stargate UAE is a next-generation AI infrastructure cluster that will run in the newly established 5-gigawatt UAE–U.S. AI Campus in Abu Dhabi. Stargate UAE, a 1-gigawatt compute cluster""",Planned 2029,,12.39967372,Stargate UAE Phase 1,,,,,,,Unknown,,,,,21.39967372,5.01E+21,5.01E+21,2.51E+21,,,1000,2.51E+12,,,,,,,,,https://web.archive.org/web/20250515200336/https://www.commerce.gov/news/press-releases/2025/05/uae-and-us-presidents-attend-unveiling-phase-1-new-5gw-ai-campus-abu,https://archive.ph/hQWdM,https://archive.ph/PfVwI,,
"""OpenAI/Microsoft Mt Pleasant, Wisconsin Phase 2""",Planned,Likely,Yes,21.54406804,1768569.985,NVIDIA GB200,700000,United States of America,"OpenAI,Microsoft",,"Rough estimate of chip count from stated power capacity of datacenter. 

Estimate from Semianalysis that Microsoft plans to build four of the 300MW buildings
What type of GPUs is unclear, but I'm assuming they're B200s",Private,1500,,"M39X+XG, Mt Pleasant, WI 53177","OpenAI,Microsoft","GB200,Uncertain","""1.5GW within 2.5 years""
""Microsoft plans to build four 300MW buildings... 1.5GW is like a million GPUs""",Planned 2026,"They announced Jan 2025 that they are currently pausing some planned construction
Estimate from Semianalysis that Microsoft plans to build four of the 300MW buildings, which translates to ~1 million GPUs. Type of GPU uncertain",12.06694679,"OpenAI/Microsoft Mt Pleasant, Wisconsin Phase 1",,,,,,700000,NVIDIA,,,,,21.24303805,3.50E+21,3.50E+21,1.75E+21,8.75E+20,1681.68,1500,1.16667E+12,,,,,,,,,https://web.archive.org/web/20240717135010/https://www.tomshardware.com/tech-industry/artificial-intelligence/openai-and-microsoft-reportedly-planning-dollar100-billion-datacenter-project-for-an-ai-supercomputer,https://web.archive.org/web/20240915193816/https://www.datacenterdynamics.com/en/news/microsoft-openai-consider-100bn-5gw-stargate-ai-data-center-report/,https://www.youtube.com/watch?v=hobvps-H38o,,
HUMAIN Saudi Arabia Phase 2,Planned,Likely,Unclear,21.4785665,1520970.187,,,Saudi Arabia,Humain,,"Seems to be partnering with AMD and NVIDIA
FLOP/s is a very rough estimate based on stated power capacity and the completion year",Public/Private,500,,,,Uncertain,"""HUMAIN is making a major investment to build AI factories in the Kingdom of Saudi Arabia with a projected capacity of up to 500 megawatts powered by several hundred thousand of NVIDIA’s most advanced GPUs over the next five years. The first phase of deployment will be an 18,000 NVIDIA GB300 Grace Blackwell AI supercomputer with NVIDIA InfiniBand networking.""",Planned 2030,,12.47741069,HUMAIN Saudi Arabia/NVIDIA Phase 1,,,,,,,Unknown,,,,,21.17638069,3.01E+21,3.01E+21,1.50E+21,,,500,3.002E+12,,,,,,,,,https://archive.ph/vvAxQ,https://archive.ph/CFcnD,http://web.archive.org/web/20250517061228/https://ir.amd.com/news-events/press-releases/detail/1250/amd-and-humain-form-strategic-10b-collaboration-to-advance-global-ai,,
Fluidstack France Gigawatt Campus,Planned,Likely,Yes,21.39794001,1263264.275,NVIDIA GB200,500000,France,,,Unclear if the 500k chips refers to GPUs or entire chips (which have 2 GPUs),Public/Private,1000,,France,Mistral,"GB200,Uncertain","""France and UAE to invest billions into 1GW European AI data center""
""The facility’s Phase 1 will ultimately host close to 500,000 next-generation AI chips.""",Planned 2028,,12.09691001,,,,,,,500000,NVIDIA,,,,,21.09691001,2.50E+21,2.50E+21,1.25E+21,6.25E+20,1201.2,1000,1.25E+12,,,,,,,,,https://web.archive.org/web/20250211011508/https://www.datacenterdynamics.com/en/news/france-and-uae-to-invest-billions-into-1gw-european-ai-data-center/,https://web.archive.org/web/20250210220905/https://www.hpcwire.com/off-the-wire/fluidstack-to-build-1gw-ai-supercomputer-in-france/,,,
Reliance Industries Supercomputer,Planned,Likely,Yes,21.35218252,1136937.847,NVIDIA GB200,450000,India,Reliance Industries,,Chip count estimated from power capacity. Chip type uncertain,Private,1000,,"Jamnagar, India","Reliance Jio,Cloud",GB200,will supply its Blackwell AI processors for a one-gigawatt data centre Reliance is building in the western state of Gujarat,Planned 2027,,12.05115252,,,,,,,450000,NVIDIA,,,,,21.05115252,2.25E+21,2.25E+21,1.13E+21,5.625E+20,1081.08,1000,1.125E+12,,,,,,,,,https://archive.ph/tTvHo,,,,
xAI Colossus 2 Memphis Phase 2,Planned,Likely,Yes,21.22693484,852097.0187,NVIDIA GB200,330000,United States of America,xAI,,"550k GB200/GB300 planned. Unclear what the proportion of GB200 vs GB300 will be. The first 110k will be GB200s. I will naively assume that the remaining will be split 50/50 GB200/GB300, so 
330k GB200s
220k GB300s
- Elon has claimed that this will be complete in 6-9 months, so presumably early-mid 2026",Private,1409.408,,"5420 Tulane Rd, Memphis, TN 38109",xAI,"550k GB200/GB300s,Blackwell,GB200,GB300","At Colossus 2, the first batch of 550k GB200s & GB300s, also for training, start going online in a few weeks.",Planned 2026,,11.98926597,xAI Colossus 2 Memphis Phase 1,,,,NVIDIA GB300 (Blackwell Ultra),220000,550000,NVIDIA,NVIDIA,,,,21.1383027,1.69E+21,1.69E+21,1.38E+21,6.875E+20,1409.408,,9.75587E+11,,,,,TRUE,,,,https://archive.ph/oHQSA#selection-585.0-597.43,https://www.cnbc.com/2025/05/20/elon-musk-says-ai-could-run-into-power-capacity-issues-by-middle-of-next-year.html,https://web.archive.org/web/20241205161745/https://memphischamber.com/blog/general/xai-memphis-announces-expansion-of-supercomputer-with-addition-of-tech-companies-in-digital-delta/,https://archive.ph/WYQTZ,
Meta Prometheus New Albany,Planned,Likely,Yes,21.18554215,774633.6534,NVIDIA GB200,300000,United States of America,Meta AI,,"Not sure what the distribution of GB200/GB300 is, I'm assuming it's 60% GB200",Private,1281.28,,"1 Community Cir, New Albany, OH 43054",Meta,"GB200,GB300","GB200/GB300: 500,000",Planned H2 2026,,11.98926597,,,,,NVIDIA GB300 (Blackwell Ultra),200000,500000,NVIDIA,NVIDIA,,,,21.09691001,1.53E+21,1.53E+21,1.25E+21,6.25E+20,1281.28,,9.75587E+11,,,,,,,,,https://web.archive.org/web/20250716094956/https://semianalysis.com/2025/07/11/meta-superintelligence-leadership-compute-talent-and-data/,,,,
OpenAI/Microsoft Atlanta,Planned,Likely,Yes,21.13033377,682162.7084,NVIDIA B200,300000,United States of America,"OpenAI,Microsoft",,"Rough estimate of chip count from stated power capacity of datacenter. 
Assuming chips are B200s",Private,700,12521119483,"1435 Hwy 54 W, Fayetteville, GA 30214","OpenAI,Microsoft","B200,Uncertain","""700MW within 1.5 years""",Planned 2026,Rought estimate from power capacity,11.98420573,,,,,,,300000,NVIDIA,,,,,20.82930377,1.35E+21,1.35E+21,6.75E+20,3.375E+20,600.6,700,9.64286E+11,12521119483,,,,,,,,https://www.youtube.com/watch?v=hobvps-H38o (this cluster mentioned at 15:01),,,,
DataVolt Neom 1.5 GW Phase 1,Planned,Likely,Unclear,21.00432137,510358.7671,,,Saudi Arabia,DataVolt,,FLOP/s is a very rough estimate based on stated power capacity and the completion year,Private,300,,"27°33'21.6""N 35°32'18.4""E",,Uncertain,"""At the LEAP 2025 event this week DataVolt and Neom signed a $5bn agreement to develop a 1.5GW data center campus in Neom’s Oxagon industrial area.
The first 300MW phase of the site will reportedly go live in 2028""",Planned 2028,,12.22271647,,,,,,,,Unknown,,,,,20.69983773,1.01E+21,1.01E+21,5.01E+20,,,300,1.67E+12,,,,,,,,,http://web.archive.org/web/20250404231606/https://www.datacenterdynamics.com/en/news/datavolt-plans-15gw-data-center-campus-in-neoms-oxagon/,,,,
Applied Digital Ellendale Possible Phase 3,Planned,Likely,Yes,20.95424251,454775.139,NVIDIA GB200,180000,United States of America,Applied Digital,,"It's unclear whether all the GPUs will be intereconnected. The CEO says that it's designed in a way that they can be all interconnected, though it's unclear if they will be. It is also somewhat unclear what type of GPUs they will be using, though I am assuming Blackwell GPUs here
Will be rented to CoreWeave
Number of GPUs estimated from power capacity
CoreWeave holds the option for this building",Private,400,3290034407,"9685 87th Ave SE, Ellendale, ND 58436","Cloud,CoreWeave",Uncertain,"Applied Digital expects the first 100 MW data center for CoreWeave to be ready for service in the fourth quarter of calendar 2025. The second building, which is expected to house a 150 MW data center, is currently under construction and is expected to be ready for service in the middle of 2026. Additionally, CoreWeave holds an option for the third 150 MW building, which is currently in the planning stages, anticipated to be ready for service in 2027.",Planned 2027,,12.05115252,Applied Digital CoreWeave Ellendale Phase 2,,,,,,180000,NVIDIA,,,,,20.65321251,9.00E+20,9.00E+20,4.50E+20,2.25E+20,432.432,400,1.125E+12,3290034407,,,,,,,,https://web.archive.org/web/20240622003111/https://marketscale.com/industries/software-and-technology/applied-digital-and-nvidia-are-solving-ai-applications-efficiency-crisis-with-next-gen-data-centers/,https://web.archive.org/web/20250603030527/https://ir.applieddigital.com/news-events/press-releases/detail/123/applied-digital-announces-250mw-ai-data-center-lease-with,,,
Nebius New Jersey,Planned,Likely,Yes,20.87506126,378979.2825,NVIDIA GB200,150000,United States of America,Nebius AI,,Rough estimate of chip count from stated power capacity of datacenter. ,Private,300,,"New Jersey, USA",Cloud,"GB200,Blackwell,Uncertain","""AI infrastructure company Nebius has announced plans to build a 300MW data center in New Jersey.""
""Nebius today confirmed that its recently announced data center in New Jersey will be dedicated solely to NVIDIA Blackwell-architecture GPUs""",Planned 2026,,12.09691001,,,,,,,150000,NVIDIA,,,,,20.57403127,7.50E+20,7.50E+20,3.75E+20,1.875E+20,360.36,300,1.25E+12,,,,,,,,,https://archive.ph/QP6zP#selection-1349.0-1349.96,https://archive.ph/N4MzN,,,
Sesterce Grand Est France B,Planned,Likely,Yes,20.87506126,378979.2825,NVIDIA GB200,150000,France,Sesterce,,"Unclear what type of GPUs it will use
It seems very likely that they're using Blackwell. However, given that, the power capacity they cite does not line up with the chip counts they cite. I will assume that the MW figure they cite is accurate, and extrapolate chip counts from there",Private,300,,"Grand Est, Fance",Cloud,Uncertain,"Sesterce is aiming to add 600MW of capacity across two data centers in Grand Est with 500,000 GPUs by 2028, and 1.2 GW with more than one million GPUs by 2030.",Planned 2028,,12.09691001,,,,,,,150000,NVIDIA,,,,,20.57403127,7.50E+20,7.50E+20,3.75E+20,1.875E+20,360.36,300,1.25E+12,,,,,,,,,https://archive.ph/33xgc,,,,
Sesterce Grand Est France A,Planned,Likely,Yes,20.87506126,378979.2825,NVIDIA GB200,150000,France,Sesterce,,"Unclear what type of GPUs it will use
It seems very likely that they're using Blackwell. However, given that, the power capacity they cite does not line up with the chip counts they cite. I will assume that the MW figure they cite is accurate, and extrapolate chip counts from there",Private,300,,"Grand Est, Fance",Cloud,Uncertain,"Sesterce is aiming to add 600MW of capacity across two data centers in Grand Est with 500,000 GPUs by 2028, and 1.2 GW with more than one million GPUs by 2030.",Planned 2028,,12.09691001,,,,,,,150000,NVIDIA,,,,,20.57403127,7.50E+20,7.50E+20,3.75E+20,1.875E+20,360.36,300,1.25E+12,,,,,,,,,https://archive.ph/33xgc,,,,
"""OpenAI/Microsoft Mt Pleasant, Wisconsin Phase 1""",Planned,Likely,Yes,20.87506126,378979.2825,NVIDIA GB200,150000,United States of America,"OpenAI,Microsoft",,"Rough estimate of chip count from stated power capacity of datacenter. 
Estimate from Semianalysis that that this is a 300MW building
What type of GPUs is unclear, but I'm assuming they're GB200s",Private,300,,"M39X+XG, Mt Pleasant, WI 53177","OpenAI,Microsoft","GB200,Uncertain","Ahead of Stargate, the companies plan to build a Phase 4 supercomputer in Mount Pleasant, Wisconsin, with operations starting in 2026.",Planned 2026,Rough estiamate of number of GPUs from power consupmtion estimate. Type of GPU uncertain,12.09691001,,,,,,,150000,NVIDIA,,,,,20.57403127,7.50E+20,7.50E+20,3.75E+20,1.875E+20,360.36,300,1.25E+12,,,,,,,,,https://web.archive.org/web/20240717135010/https://www.tomshardware.com/tech-industry/artificial-intelligence/openai-and-microsoft-reportedly-planning-dollar100-billion-datacenter-project-for-an-ai-supercomputer,https://web.archive.org/web/20240915193816/https://www.datacenterdynamics.com/en/news/microsoft-openai-consider-100bn-5gw-stargate-ai-data-center-report/,https://www.youtube.com/watch?v=hobvps-H38o,,
Sesterce Southern France 250MW,Planned,Likely,Yes,20.77815125,303183.426,NVIDIA GB200,120000,France,Sesterce,,"Unclear what type of GPUs it will use
It seems very likely that they're using Blackwell. However, given that, the power capacity they cite does not line up with the chip counts they cite. I will assume that the MW figure they cite is accurate, and extrapolate chip counts from there",Private,250,,Southern France,Cloud,Uncertain,"the company has said it will establish a 250MW supercomputer with 200,000 GPUs in southern France",Planned 2028,,12.07918125,,,,,,,120000,NVIDIA,,,,,20.47712125,6.00E+20,6.00E+20,3.00E+20,1.5E+20,288.288,250,1.2E+12,,,,,,,,,https://archive.ph/33xgc,,,,
Oracle OCI Supercluster B200s,Planned,Likely,Yes,20.77072244,298041.4351,NVIDIA B200,131072,United States of America,Oracle,,Uncertain if this will actually be built,Private,262.406144,6564672691,,Cloud,B200,"OCI Supercluster with up to 131,072 NVIDIA B200 Tensor Core GPUs",Planned 2025,,12.05071844,,,,,,,131072,NVIDIA,,,,,20.46969244,5.90E+20,5.90E+20,2.95E+20,1.47456E+20,262.406144,,1.12388E+12,6564672691,,,,,,,,https://archive.ph/B3HJm#selection-1045.0-1045.83,,,,
Applied Digital CoreWeave Ellendale Phase 2,Planned,Likely,Yes,20.74036269,277918.1405,NVIDIA GB200,110000,United States of America,Applied Digital,,"It's unclear whether all the GPUs will be intereconnected. The CEO says that it's designed in a way that they can be all interconnected, though it's unclear if they will be. It is also somewhat unclear what type of GPUs they will be using, though I am assuming Blackwell GPUs here
Will be rented to CoreWeave
Number of GPUs estimated from power capacity",Private,250,3290034407,"9685 87th Ave SE, Ellendale, ND 58436","Cloud,CoreWeave",Uncertain,"Applied Digital expects the first 100 MW data center for CoreWeave to be ready for service in the fourth quarter of calendar 2025. The second building, which is expected to house a 150 MW data center, is currently under construction and is expected to be ready for service in the middle of 2026. Additionally, CoreWeave holds an option for the third 150 MW building, which is currently in the planning stages, anticipated to be ready for service in 2027.",Planned 2026,,12.04139269,Applied Digital CoreWeave Ellendale Phase 1,,,,,,110000,NVIDIA,,,,,20.43933269,5.50E+20,5.50E+20,2.75E+20,1.375E+20,264.264,250,1.1E+12,3290034407,,,,,,,,https://web.archive.org/web/20240622003111/https://marketscale.com/industries/software-and-technology/applied-digital-and-nvidia-are-solving-ai-applications-efficiency-crisis-with-next-gen-data-centers/,https://web.archive.org/web/20250603030527/https://ir.applieddigital.com/news-events/press-releases/detail/123/applied-digital-announces-250mw-ai-data-center-lease-with,,,
xAI Colossus 2 Memphis Phase 1,Planned,Likely,Yes,20.74036269,277918.1405,NVIDIA GB200,110000,United States of America,xAI,,,Private,264.264,,"5420 Tulane Rd, Memphis, TN 38109",xAI,GB200,"""we're about to bring 110 thousand GB200s online at a second data center also in the Memphis area""",Planned H2 2025,,12.01729469,,,,,,,110000,NVIDIA,,,,,20.43933269,5.50E+20,5.50E+20,2.75E+20,1.375E+20,264.264,,1.04063E+12,,,,,,,,,https://archive.ph/oHQSA#selection-585.0-597.43,https://archive.ph/WYQTZ,,,
xAI Colossus Memphis Phase 3,Existing,Confirmed,Yes,20.73703353,275795.8565,NVIDIA H100 SXM5 80GB,200000,United States of America,xAI,7/22/2025,"we know from earlier that it has 150k H100s and 50k H200s, which makes the total
150k H100s
50k H200s
30k GB200s",Private,352.352,,"3231 Riverport Rd, Memphis, TN 38109",xAI,"150k H100s, 50k H200s, 30k GB200s,H100,GB200,H200","""230k GPUs, including 30k GB200s, are operational for training Grok 
@xAI
 in a single supercluster called Colossus 1""",7/22/2025,,11.88899497,xAI Colossus Memphis Phase 2,,,,NVIDIA GB200,30000,230000,NVIDIA,NVIDIA,TRUE,,,20.43597171,5.46E+20,5.46E+20,2.73E+20,1.3644E+20,352.352,,7.74453E+11,,,,,TRUE,,,,https://archive.ph/oHQSA,,,,
OpenAI Stargate Abilene Oracle OCI Supercluster Phase 2,Planned,Likely,Yes,20.72672734,269328.0268,NVIDIA GB300 (Blackwell Ultra),200001,United States of America,Oracle,,"Unclear exact FLOPs for GB300, but sources indicate that it is 1.5x a GB200. 
""That cluster is expected to be leased to Oracle, which in turn will lease it to Microsoft, who will rent it to OpenAI""
Crusoe is involved in this",Private,800.8028028,,"5502 Spinks Rd, Abilene, TX 79601","Cloud,Microsoft,OpenAI","GB200,GB300","""SA estimates the campus will house a previously-announced 100k GB200 cluster, as well as a 200k GB300 cluster and a 400k VR200 cluster""",Planned 2026,Low confidence since plans are still somewhat up in the air,11.97153713,OpenAI Stargate Abilene Oracle OCI Supercluster Phase 1,,,,NVIDIA GB200,100000,300001,NVIDIA,NVIDIA,,,,20.87506271,5.33E+20,5.33E+20,7.50E+20,3.75001E+20,800.8028028,,9.36563E+11,,,,,,,,,https://archive.ph/B3HJm#selection-1045.0-1045.83,https://archive.ph/UeNZs,https://web.archive.org/web/20250124161138/https://www.transformernews.ai/p/unravelling-the-stargate-spin,,
Middle East 100k Cluster,Planned,Unlikely,Yes,20.69897,252652.855,NVIDIA GB200,100000,,,,We think this is likely the same cluster being administered/operated by Microsoft for G42,Private,240.24,,,,GB200,There's a 100k GB200 cluster going up in the Middle East,Planned,Alluded to by Semianalysis,12.01729469,,,TRUE,,,,100000,NVIDIA,,,,,20.39794001,5.00E+20,5.00E+20,2.50E+20,1.25E+20,240.24,,1.04063E+12,,,,,,,,,https://web.archive.org/web/20241006181731/https://www.dwarkeshpatel.com/p/dylan-jon,,,,
OpenAI Stargate Abilene Oracle OCI Supercluster Phase 1,Planned,Likely,Yes,20.69897,252652.855,NVIDIA GB200,100000,United States of America,Oracle,,"States that it will have over 100k GPUs, implying it will have over 50k GB200s

""That cluster is expected to be leased to Oracle, which in turn will lease it to Microsoft, who will rent it to OpenAI""

Crusoe is involved in this",Private,240.24,,"5502 Spinks Rd, Abilene, TX 79601","Cloud,Microsoft,OpenAI",GB200,"OCI Supercluster with over 100,000+ GPUs in NVIDIA GB200 Grace Blackwell Superchips",Planned Q2 2025,The first phase is under construction already and is very likely to be completed,12.01729469,,,,,,,100000,NVIDIA,,,,,20.39794001,5.00E+20,5.00E+20,2.50E+20,1.25E+20,240.24,,1.04063E+12,,,,,,,,,https://archive.ph/B3HJm#selection-1045.0-1045.83,https://archive.ph/UeNZs,,,
CoreWeave Denton GB200s OpenAI/Microsoft,Planned,Likely,Yes,20.69897,252652.855,NVIDIA GB200,100000,United States of America,CoreWeave,,"Rough estimate of chip count from stated power capacity of datacenter. 
Core Scientific is also involved. 
Said to be ""one of the largest GPU supercomputers in North America""
Rumored to be using GB200s and rented to Microsoft for OpenAI",Private,260,,"8161 Jim Christal Rd, Denton, TX 76207","OpenAI,Microsoft",GB200,The new agreement brings an additional $1.2 billion in contracted revenue across Core Scientific’s Denton TX location,Planned 2027,,11.98296666,,,,,,,100000,NVIDIA,,,,,20.39794001,5.00E+20,5.00E+20,2.50E+20,1.25E+20,240.24,260,9.61538E+11,,,,,,,,,https://web.archive.org/web/20250228154647/https://investors.corescientific.com/news-events/press-releases/detail/110/core-scientific-and-coreweave-announce-1-2-billion-expansion-at-denton-tx-site,https://web.archive.org/web/20241119062654/https://semianalysis.com/2024/09/04/multi-datacenter-training-openais/,,,
xAI Colossus Memphis Phase 2,Existing,Likely,Yes,20.59747579,200000,NVIDIA H100 SXM5 80GB,150000,United States of America,xAI,2/18/2025,"GPU types is from a tweet. Unclear exactly when it was first operational. It was confirmed to be operational Feb 17, 2025, but some evidence suggests that it was operational by December 4th, 2024",Private,280.28,7120201209,"3231 Riverport Rd, Memphis, TN 38109",xAI,"H100,H200","This weekend, the 
@xAI
 team brought our Colossus 100k H100 training cluster online.... Moreover, it will double in size to 200k (50k H200s) in a few months.",2/18/2025,,11.84882077,xAI Colossus Memphis Phase 1,xAI Colossus Memphis Phase 3,,,NVIDIA H200 SXM,50000,200000,NVIDIA,NVIDIA,TRUE,,1,20.29641288,3.96E+20,3.96E+20,1.98E+20,9.893E+19,280.28,,7.06026E+11,7120201209,,,,,,xAI Colossus Memphis Phase 2,1,https://archive.ph/z39uN,https://web.archive.org/web/20241217184541/https://memphischamber.com/economic-development/xai/,https://web.archive.org/web/20241217184541/https://memphischamber.com/economic-development/xai/,https://www.youtube.com/watch?v=AUAJ82H12qs&t=260s,
SK Group AWS Uslan Phase 2,Planned,Likely,Yes,20.47712125,151591.713,NVIDIA GB200,60000,Korea (Republic of),"SK Telecom,Amazon",,"unclear what type of GPUs they'll have, assuming for now that they'll be Blackwells",Private,103,,Buk-gu Ulsan South Korea,"SK Telecom,Amazon",Uncertain,"""is set to house 60,000 GPUs""
""The first phase will see 41MW of data center capacity built by November 2027, expanding to 103MW by February 2029. The campus could expand to 1GW in the future.""",Planned 2029-02,,12.16325403,,,,,,,60000,NVIDIA,,,,,20.17609126,3.00E+20,3.00E+20,1.50E+20,7.5E+19,144.144,103,1.45631E+12,,,,,,,,,https://web.archive.org/web/20250725082756/https://www.datacenterdynamics.com/en/news/sk-group-and-aws-to-build-ai-data-center-in-ulsan-south-korea/,,,,
Project Rainier,Planned,Likely,Yes,20.42618583,134815.5634,Amazon Trainium2,400000,United States of America,Amazon,,,Private,350,,"55253 Elderberry Rd, New Carlisle, IN 46552",Anthropic,Trainium2,a cluster with 400k Trainium2 chips for Anthropic,Planned 2025,,11.88211778,,,,,,,400000,Amazon AWS,,,,,20.42618583,2.67E+20,,2.67E+20,,400.4,350,7.62286E+11,,,,,,,,,https://web.archive.org/web/20241204130049/https://semianalysis.com/2024/12/03/amazons-ai-self-sufficiency-trainium2-architecture-networking/,,,,
Applied Digital CoreWeave Ellendale Phase 1,Planned,Likely,Yes,20.35218252,113693.7847,NVIDIA GB200,45000,United States of America,Applied Digital,,"It's unclear whether all the GPUs will be intereconnected. The CEO says that it's designed in a way that they can be all interconnected, though it's unclear if they will be. It is also somewhat unclear what type of GPUs they will be using, though I am assuming Blackwell GPUs here
Will be rented to CoreWeave
Number of GPUs estimated from power capacity",Private,100,3290034407,"9685 87th Ave SE, Ellendale, ND 58436","Cloud,CoreWeave",Uncertain,"Applied Digital expects the first 100 MW data center for CoreWeave to be ready for service in the fourth quarter of calendar 2025. The second building, which is expected to house a 150 MW data center, is currently under construction and is expected to be ready for service in the middle of 2026. Additionally, CoreWeave holds an option for the third 150 MW building, which is currently in the planning stages, anticipated to be ready for service in 2027.",Planned Q4 2025,,12.05115252,,,,,,,45000,NVIDIA,,,,,20.05115252,2.25E+20,2.25E+20,1.13E+20,5.625E+19,108.108,100,1.125E+12,3290034407,,,,,,,,https://web.archive.org/web/20240622003111/https://marketscale.com/industries/software-and-technology/applied-digital-and-nvidia-are-solving-ai-applications-efficiency-crisis-with-next-gen-data-centers/,https://web.archive.org/web/20250603030527/https://ir.applieddigital.com/news-events/press-releases/detail/123/applied-digital-announces-250mw-ai-data-center-lease-with,,,
Nscale Loughton,Planned,Likely,Yes,20.35218252,113693.7847,NVIDIA GB200,45000,United Kingdom of Great Britain and Northern Ireland,,,Slightly unclear if they refer to GB200 GPUs or entire chips (2 GPUs),Private,90,,"Loughton, England",Cloud,GB200,"The site is equipped to support 50MW of AI and HPC capacity, with the ability to scale the power allocation up to 90MW, all utilising advanced liquid cooling to support generative AI GPU deployments. The site is scheduled to be live in Q4 2026. This UK facility can house up to 45,000 of the latest Nvidia GB200 GPUs.",Planned Q4 2026,,12.09691001,,,,,,,45000,NVIDIA,,,,,20.05115252,2.25E+20,2.25E+20,1.13E+20,5.625E+19,108.108,90,1.25E+12,,,,,,,,,https://web.archive.org/web/20250116140650/https://www.globenewswire.com/news-release/2025/01/13/3008191/0/en/AI-hyperscaler-Nscale-to-invest-2-5-2-billion-in-the-UK-data-centre-industry-over-the-next-three-years.html,,,,
EU AI Gigafactory #3,Planned,Likely,Yes,20.30103,101061.142,NVIDIA GB200,40000,,,,"Still in the soliciting proposal phase, so details are not fully clear, yet. They say they should be at least 100k Hopper-equivalent AI chips, which would be ~40k current frontier chips (Blackwells)
Will be located in Europe, sites still uncertain
They are planning on building five",Public/Private,96.096,,Europe,,Uncertain,"They will be integrating massive computing power (e.g., beyond 100,000 advanced AI chips.... Reference here is NVIDIA’s H100 GPUs or equivalent",Planned H2 2026,,12.01729469,,,,,,,40000,NVIDIA,,,,,20,2.00E+20,2.00E+20,1.00E+20,5E+19,96.096,,1.04063E+12,,,,,,,,,https://web.archive.org/web/20250409164352/https://eurohpc-ju.europa.eu/document/download/47492db7-592e-4ad8-b672-9c822f94afa0_en?filename=Call%20for%20expression%20of%20interest%20in%20AI%20gigafactories%20-%202025%2004%2009%20-%20FINAL.pdf,,,,
EU AI Gigafactory #4,Planned,Likely,Yes,20.30103,101061.142,NVIDIA GB200,40000,,,,"Still in the soliciting proposal phase, so details are not fully clear, yet. They say they should be at least 100k Hopper-equivalent AI chips, which would be ~40k current frontier chips (Blackwells)
Will be located in Europe, sites still uncertain
They are planning on building five",Public/Private,96.096,,Europe,,Uncertain,"They will be integrating massive computing power (e.g., beyond 100,000 advanced AI chips.... Reference here is NVIDIA’s H100 GPUs or equivalent",Planned H2 2026,,12.01729469,,,,,,,40000,NVIDIA,,,,,20,2.00E+20,2.00E+20,1.00E+20,5E+19,96.096,,1.04063E+12,,,,,,,,,https://web.archive.org/web/20250409164352/https://eurohpc-ju.europa.eu/document/download/47492db7-592e-4ad8-b672-9c822f94afa0_en?filename=Call%20for%20expression%20of%20interest%20in%20AI%20gigafactories%20-%202025%2004%2009%20-%20FINAL.pdf,,,,
CoreWeave Muskogee,Planned,Likely,Yes,20.30103,101061.142,NVIDIA GB200,40000,United States of America,CoreWeave,,"Rough estimate of chip count from stated power capacity of datacenter. 
Core Scientific is also involved
NVIDIA GPUs, so presumably GB200s",Private,100,,"Muskogee, Oklahoma 35.693709, -95.386957","CoreWeave,Undisclosed Client","GB200,Uncertain","ground had been broken on a 100MW facility in the Port of Muskogee in West Oklahoma.
Set to go live in 2026, the facility will host CoreWeave’s Nvidia GPUs for an undisclosed client",Planned 2026,,12,,,,,,,40000,NVIDIA,,,,,20,2.00E+20,2.00E+20,1.00E+20,5E+19,96.096,100,1E+12,,,,,,,,,https://web.archive.org/web/20250115175242/https://www.datacenterdynamics.com/en/news/coreweave-and-core-scientific-break-ground-on-100mw-data-center-in-muskogee-oklahoma/,,,,
EU AI Gigafactory #1,Planned,Likely,Yes,20.30103,101061.142,NVIDIA GB200,40000,,,,"Still in the soliciting proposal phase, so details are not fully clear, yet. They say they should be at least 100k Hopper-equivalent AI chips, which would be ~40k current frontier chips (Blackwells)
Will be located in Europe, sites still uncertain
They are planning on building five",Public/Private,96.096,,Europe,,Uncertain,"They will be integrating massive computing power (e.g., beyond 100,000 advanced AI chips.... Reference here is NVIDIA’s H100 GPUs or equivalent",Planned H2 2026,,12.01729469,,,,,,,40000,NVIDIA,,,,,20,2.00E+20,2.00E+20,1.00E+20,5E+19,96.096,,1.04063E+12,,,,,,,,,https://web.archive.org/web/20250409164352/https://eurohpc-ju.europa.eu/document/download/47492db7-592e-4ad8-b672-9c822f94afa0_en?filename=Call%20for%20expression%20of%20interest%20in%20AI%20gigafactories%20-%202025%2004%2009%20-%20FINAL.pdf,,,,
Sesterce Valence,Planned,Likely,Yes,20.30103,101061.142,NVIDIA GB200,40000,France,Sesterce,,"Unclear what type of GPUs they'll use, but I'm assuming it'll be Blackwell GPUs",Private,96.096,,"Valence, France",Cloud,GB200,"The data center is set to be located in the Rovaltain business park in Valence Romans Agglo, and will house 40,000 GPUs for the training and deployment of AI.",Planned 2026,,12.01729469,,,,,,,40000,NVIDIA,,,,,20,2.00E+20,2.00E+20,1.00E+20,5E+19,96.096,,1.04063E+12,,,,,,,,,https://archive.ph/33xgc,,,,
EU AI Gigafactory #2,Planned,Likely,Yes,20.30103,101061.142,NVIDIA GB200,40000,,,,"Still in the soliciting proposal phase, so details are not fully clear, yet. They say they should be at least 100k Hopper-equivalent AI chips, which would be ~40k current frontier chips (Blackwells)
Will be located in Europe, sites still uncertain
They are planning on building five",Public/Private,96.096,,Europe,,Uncertain,"They will be integrating massive computing power (e.g., beyond 100,000 advanced AI chips.... Reference here is NVIDIA’s H100 GPUs or equivalent",Planned H2 2026,,12.01729469,,,,,,,40000,NVIDIA,,,,,20,2.00E+20,2.00E+20,1.00E+20,5E+19,96.096,,1.04063E+12,,,,,,,,,https://web.archive.org/web/20250409164352/https://eurohpc-ju.europa.eu/document/download/47492db7-592e-4ad8-b672-9c822f94afa0_en?filename=Call%20for%20expression%20of%20interest%20in%20AI%20gigafactories%20-%202025%2004%2009%20-%20FINAL.pdf,,,,
EU AI Gigafactory #5,Planned,Likely,Yes,20.30103,101061.142,NVIDIA GB200,40000,,,,"Still in the soliciting proposal phase, so details are not fully clear, yet. They say they should be at least 100k Hopper-equivalent AI chips, which would be ~40k current frontier chips (Blackwells)
Will be located in Europe, sites still uncertain
They are planning on building five",Public/Private,96.096,,Europe,,Uncertain,"They will be integrating massive computing power (e.g., beyond 100,000 advanced AI chips.... Reference here is NVIDIA’s H100 GPUs or equivalent",Planned H2 2026,,12.01729469,,,,,,,40000,NVIDIA,,,,,20,2.00E+20,2.00E+20,1.00E+20,5E+19,96.096,,1.04063E+12,,,,,,,,,https://web.archive.org/web/20250409164352/https://eurohpc-ju.europa.eu/document/download/47492db7-592e-4ad8-b672-9c822f94afa0_en?filename=Call%20for%20expression%20of%20interest%20in%20AI%20gigafactories%20-%202025%2004%2009%20-%20FINAL.pdf,,,,
xAI Colossus Memphis Phase 1,Existing,Confirmed,Yes,20.29644579,100000,NVIDIA H100 SXM5 80GB,100000,United States of America,xAI,9/2/2024,,Private,150,3802927639,"3231 Riverport Rd, Memphis, TN 38109",xAI,H100,"""With 100k liquid-cooled H100s on a single RDMA fabric""

""This weekend, the @xAI team brought our Colossus 100k H100 training cluster online.... Moreover, it will double in size to 200k (50k H200s) in a few months""",9/2/2024,Reported to be online by Musk,11.81928065,,xAI Colossus Memphis Phase 2,,,,,100000,NVIDIA,,TRUE,,1,19.99537191,1.98E+20,1.98E+20,9.89E+19,4.947E+19,142.688,150,6.596E+11,3802927639,,,,,,xAI Colossus Memphis Phase 1,1,https://archive.ph/Bh9Tq,https://web.archive.org/web/20240727065656/https://www.commercialappeal.com/story/news/government/city/2024/07/09/elon-musk-xai-memphis-electricity-water-mlgw/74331544007/,https://archive.ph/z39uN,,
Tesla Cortex Phase 3,Planned,Likely,Yes,20.29644579,100000,NVIDIA H100 SXM5 80GB,100000,United States of America,Tesla,,,Private,140.14,4700049153,"1 Tesla Rd, Austin, TX 78725",Tesla,H100,This will be ~100k H100/H200 with massive storage for video training of FSD & Optimus.,Planned 2026,From a Musk Tweet,11.84880979,Tesla Cortex Phase 2,,,,,20000,120000,NVIDIA,,,,,19.99537191,1.98E+20,1.98E+20,9.89E+19,4.947E+19,140.14,,7.06008E+11,4700049153,,,,,,,,https://web.archive.org/web/20240904184237/https://www.tomshardware.com/desktops/servers/elon-musk-shows-off-cortex-ai-supercluster-first-look-at-teslas-50000-nvidia-h100s,https://web.archive.org/web/20240822222555/https://www.fool.com/earnings/call-transcripts/2024/07/24/tesla-tsla-q2-2024-earnings-call-transcript/,https://archive.ph/0jHdq#selection-479.86-479.172,,
Meta 100k,Existing,Likely,Yes,20.29644579,100000,NVIDIA H100 SXM5 80GB,100000,United States of America,Meta AI,10/30/2024,Reported as operational in October 2024 earnings call,Private,142.688,3810062229,United States of America,Meta,H100,"Meta Platforms is putting the final touches on one such cluster, which will be a little bigger than 100,000 H100s",10/30/2024,Reported by The Information,11.84098446,,,,,,,100000,NVIDIA,,TRUE,,1,19.99537191,1.98E+20,1.98E+20,9.89E+19,4.947E+19,142.688,,6.93401E+11,3810062229,,,,TRUE,,xAI Colossus Memphis Phase 1,1,https://archive.ph/85KB1,https://web.archive.org/web/20240901171617/https://www.semianalysis.com/p/100000-h100-clusters-power-network,https://web.archive.org/web/2/https://www.tomshardware.com/tech-industry/artificial-intelligence/meta-is-using-more-than-100-000-nvidia-h100-ai-gpus-to-train-llama-4-mark-zuckerberg-says-that-llama-4-is-being-trained-on-a-cluster-bigger-than-anything-that-ive-seen,https://web.archive.org/web/20241228090751/https://www.tomshardware.com/tech-industry/artificial-intelligence/meta-is-using-more-than-100-000-nvidia-h100-ai-gpus-to-train-llama-4-mark-zuckerberg-says-that-llama-4-is-being-trained-on-a-cluster-bigger-than-anything-that-ive-seen,
OpenAI/Microsoft Goodyear Arizona,Existing,Likely,Yes,20.29644579,100000,NVIDIA H100 SXM5 80GB,100000,United States of America,"Microsoft,OpenAI",10/2/2024,"It seems likely that this became operational in phases. It seems like 75k were likely available sometime mid 2024, and ~160k will be available in the future. 
There is very little official public data about this cluster, it's almost all infrences/rumors
Microsoft appears to own it and rent it to OpenAI
The Semianalysis lecture estimates that there are 32k H100s (48MW) per building. Satellite data as of Dec 2024 seems to show that 5 building are now complete (though this doesn't imply that the clusters inside have been built or are operational). This would imply that the cluster is growing to ~160k GPUs. It's also unclear if the newer buildings would still have Hoppers or would have Blackwell chips
I personally suspect that this cluster was one of the primary clusters for training GPT4.5",Private,142.688,3802450831,"33°24'28.7""N 112°21'54.3""W
Goodyear, Arizona, USA",OpenAI,H100,"""You already have 100K clusters. OpenAI in Arizona""
""Our data shows one of OpenAI’s next training supercomputers in Arizona was going to have more than 75,000+ GPUs in a singular site by the middle of next year""",10/2/2024,"This estimate is from the SemiAnalysis number of GPUs/building, and satellite imagery showing that they've built 5 buildings",11.84098446,,,,,,,100000,NVIDIA,,TRUE,,1,19.99537191,1.98E+20,1.98E+20,9.89E+19,4.947E+19,142.688,,6.93401E+11,3802450831,,,,,,xAI Colossus Memphis Phase 1,1,https://web.archive.org/web/20241006181731/https://www.dwarkeshpatel.com/p/dylan-jon,https://web.archive.org/web/20240609051243/https://www.semianalysis.com/p/microsoft-swallows-openais-core-team,https://www.youtube.com/watch?v=hobvps-H38o,,
Anonymized Chinese System,Planned,Unlikely,Unclear,20.30103,100000,,,China,,,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,20.30103,2.00E+20,,2.00E+20,,,,,,,,,,,,,,,,,
together.ai 36k GB200s,Planned,Likely,Yes,20.25527251,90955.02779,NVIDIA GB200,36000,United States of America,Together,,,Private,86.4864,,,Cloud,GB200,"This infrastructure backbone that will power the next generation of AI innovations enables the following customer benefits:
36,000 NVIDIA Blackwell GPUs in GB200 NVL72 Cluster starting Q1 2025",Planned Q1 2025,,12.01729469,,,,,,,36000,NVIDIA,,,,,19.95424251,1.80E+20,1.80E+20,9.00E+19,4.5E+19,86.4864,,1.04063E+12,,,,,,,,,https://web.archive.org/web/20241120213534/https://www.together.ai/blog/nvidia-gb200-together-gpu-cluster-36k,,,,
Oracle OCI Supercluster H200s,Existing,Likely,Yes,20.11292572,65536,NVIDIA H200 SXM,65536,United States of America,Oracle,11/21/2024,"Unclear exactly when this first became operational, if it was ever built. The announcement November 2024 states that the largest supercomputer in the cloud is now generally available, implying that this cluster has been built and is operational. However, it's possible that they're just advertising their capacity to scale H200 clusters to be the largest supercomputer in the cloud. 
It seems very likely that this cluster has been built, but it's not fully certain. I estimate the first operational date to be Q1 2025 to be slightly conservative on the timing",Private,93.51200768,2345609101,,Cloud,H200,"We’re excited to announce the general availability of Oracle Cloud Infrastructure (OCI) Supercluster with NVIDIA H200 Tensor Core GPUs. The largest AI supercomputer available in the cloud*, our latest Supercluster scales up to an industry-leading 65,536 GPUs.",11/21/2024,,11.84102835,,,,,,,65536,NVIDIA,,TRUE,,4,19.81189573,1.30E+20,1.30E+20,6.48E+19,3.24076E+19,93.51200768,,6.93471E+11,2345609101,,,,,,xAI Colossus Memphis Phase 1,0.65536,https://archive.ph/B3HJm#selection-1045.0-1045.83,https://web.archive.org/web/20241214043143/https://www.datacenterdynamics.com/en/news/oracles-65000-gpu-supercluster-now-generally-available/,,,
Nebius Finland Phase 2,Planned,Likely,Yes,20.07459704,60000,NVIDIA H100 SXM5 80GB,60000,Finland,Nebius AI,,"Using H100s and H200s, and possibly Blackwell chips",Private,84.084,2820029492,"Moreenikatu 6, 04600 Mäntsälä, Finland",Cloud,"H100,H200","""The expansion will see Nebius place upwards of 60,000 GPUs at the facility, growing its capacity from 25MW to 75MW.""
""The expansion in Finland will include deployment of NVIDIA H200 Tensor Core GPUs, which will be available to customers from November, in addition to already installed NVIDIA H100 Tensor Core GPUs that form the backbone of Nebius’s fleet of NVIDIA GPUs""",Planned H2 2025,,11.84880979,Nebius 8k Finland Phase 1,,,,,,60000,NVIDIA,,,,,19.77352316,1.19E+20,1.19E+20,5.94E+19,2.9682E+19,84.084,,7.06008E+11,2820029492,,,,,,,,https://web.archive.org/web/20241009115430/https://www.datacenterdynamics.com/en/news/nebius-to-triple-capacity-of-mantsala-data-center-in-finland/,https://group.nebius.com/newsroom/nebius-to-triple-capacity-at-finland-data-center-to-75-mw,,,
G42 Microsoft 100Mw UAE Cluster,Planned,Likely,Yes,20.03680848,55000,NVIDIA H100 SXM5 80GB,55000,United Arab Emirates,"G42,Microsoft",,"Estimating ""50,000–60,000"" as 55k
Seems likely to be the Ajman cluster, but uncertain here
Assuming they're using H100s here",Public/Private,100,2585027034,"Ajman, United Arab Emirates","Microsoft,Azure",H100,"Private conversations with Microsoft and G42 executives revealed that G42 and Khazna are building three new data centers in the UAE for Microsoft. The centers will be 30, 30, and 100 megawatts each, the equivalent of nearly 100,000 H100 chips (approximately 15,000, 15,000, and 50,000–60,000 respectively).",Planned Q3 2025,,11.7357346,,,,,,,55000,NVIDIA,,,,,19.7357346,1.09E+20,1.09E+20,5.44E+19,2.72085E+19,77.077,100,5.4417E+11,2585027034,,,,,,,,https://web.archive.org/web/20250221170709/https://www.csis.org/analysis/united-arab-emirates-ai-ambitions,https://web.archive.org/web/20241006181731/https://www.dwarkeshpatel.com/p/dylan-jon,,,
Project Ceiba Phase 2,Planned,Confirmed,Yes,20.01569499,52390.09601,NVIDIA GB200,20736,United States of America,"Amazon,NVIDIA",,,Private,49.8161664,,,NVIDIA,GB200,"Alongside the 20,736 B200 GPUs, Ceiba will boast 10,368 Grace CPUs.
Ceiba will be built using the new liquid-cooled GB200 NVL72 platform with fifth generation NVLink",Planned 2025,Plans were officially reported by NVIDIA,12.01729469,Project Ceiba Phase 1,,,,,,20736,NVIDIA,,,,,19.71466499,1.04E+20,1.04E+20,5.18E+19,2.592E+19,49.8161664,,1.04063E+12,,,,,,,,,https://web.archive.org/web/20240522175821/https://www.datacenterdynamics.com/en/news/aws-upgrades-project-ceiba-to-feature-20736-nvidia-blackwell-gpus-boosting-power-6x-to-414-ai-exaflops/,https://web.archive.org/web/20240806110553/https://nvidianews.nvidia.com/news/aws-nvidia-generative-ai-innovation,,,
Tesla Cortex Phase 1,Existing,Confirmed,Yes,19.9954158,50000,NVIDIA H100 SXM5 80GB,50000,United States of America,Tesla,11/15/2024,,Private,71.344,1905031115,"1 Tesla Rd, Austin, TX 78725",Tesla,H100,"Tesla completed the deployment of its 50,000 Nvidia H100 GPU training cluster during Q4 2024.",Q4 2024,,11.84098446,,,,,,,50000,NVIDIA,,TRUE,,4,19.69434191,9.90E+19,9.90E+19,4.95E+19,2.4735E+19,71.344,,6.93401E+11,1905031115,,,,,,xAI Colossus Memphis Phase 1,0.5,https://web.archive.org/web/20250217061801/https://www.datacenterdynamics.com/en/news/teslas-50000-gpu-cortex-supercomputer-went-live-in-q4-2024/,https://web.archive.org/web/20240822222555/https://www.fool.com/earnings/call-transcripts/2024/07/24/tesla-tsla-q2-2024-earnings-call-transcript/,https://web.archive.org/web/20240904184237/https://www.tomshardware.com/desktops/servers/elon-musk-shows-off-cortex-ai-supercluster-first-look-at-teslas-50000-nvidia-h100s,,
Tesla Cortex Phase 2,Planned,Likely,Yes,19.9954158,50000,NVIDIA H100 SXM5 80GB,50000,United States of America,Tesla,,,Private,70.07,2350024577,"1 Tesla Rd, Austin, TX 78725",Tesla,"H100, 50k H100, additional 20k of Tesla's own hardware is also expected","So it will be an incremental for 50,000 H100s plus 20,000 of our hardware for AI5 at Tesla AI computer",Planned 2025,"Musk gave details about this in an earnings call
Some sources mention that the 50k H100s will come online before Tesla's own chips will",11.84880979,Tesla Cortex Phase 1,,,,,20000,70000,NVIDIA,,,,,19.69434191,9.90E+19,9.90E+19,4.95E+19,2.4735E+19,70.07,,7.06008E+11,2350024577,,,,,,,,https://web.archive.org/web/20240904184237/https://www.tomshardware.com/desktops/servers/elon-musk-shows-off-cortex-ai-supercluster-first-look-at-teslas-50000-nvidia-h100s,https://web.archive.org/web/20240822222555/https://www.fool.com/earnings/call-transcripts/2024/07/24/tesla-tsla-q2-2024-earnings-call-transcript/,https://web.archive.org/web/20250217061801/https://www.datacenterdynamics.com/en/news/teslas-50000-gpu-cortex-supercomputer-went-live-in-q4-2024/,,
Lawrence Livermore NL El Capitan Phase 2,Existing,Confirmed,Yes,19.9413111,44143.35159,AMD Instinct MI300A,44544,United States of America,US Department of Energy,11/18/2024,,Public,35,607324736.7,"Lawrence Livermore National Laboratory
7000 East Ave, Livermore, CA 94550","US Government,Lawrence Livermore NL,Scientific Research",AMD MI300A,"the system has 11,136 nodes packed with 44,544 of AMD's MI300A APUs",11/18/2024,,12.09621306,Lawrence Livermore NL El Capitan Phase 1,,,,,,44544,AMD,,TRUE,,5,19.6402811,8.74E+19,8.74E+19,4.37E+19,2.18399E+19,69.0068521,35,1.248E+12,1826641892,600000000,607324736.7,$600M was quoted as the max cost ,TRUE,,xAI Colossus Memphis Phase 1,0.441433516,https://web.archive.org/web/20241122140255/https://www.tomshardware.com/pc-components/cpus/amd-powered-el-capitan-is-now-the-worlds-fastest-supercomputer-with-1-7-exaflops-of-performance-fastest-intel-machine-falls-to-third-place-on-top500-list,,,,
CoreWeave H200s,Existing,Likely,Yes,19.91969508,42000,NVIDIA H200 SXM,42000,United States of America,CoreWeave,8/28/2024,,Private,59.92896,1499797837,,CoreWeave,H200,"These instances are deployed in clusters with up to 42,000 GPUs",8/28/2024,,11.84102835,,,,,,,42000,NVIDIA,,TRUE,,1,19.61866509,8.31E+19,8.31E+19,4.16E+19,2.0769E+19,59.92896,,6.93471E+11,1499797837,,,,,,CoreWeave H200s,1,https://web.archive.org/web/20250107015942/https://www.coreweave.com/blog/coreweave-is-the-first-cloud-provider-to-deploy-nvidia-h200-tensor-core-gpus,,,,
YTL AI Johor,Planned,Likely,Yes,19.88727963,38979.28247,NVIDIA GB200,15428,Malaysia,YTL Power,,"It states the cluster will be over 300E18 FLOP/s, and that a single GB200 NVL72 has 1.4E18 FLOP/s, implying they are referring to FP4. (300/1.4)*36=7,714 GB200s=15,428 GPUs (B200s)",Private,37.0642272,,"81000 Kulai, Johor, Malaysia
1.6230215444990799, 103.52989382701362",Cloud,GB200,"YTL is among the first companies to adopt NVIDIA GB200 NVL72, which is a multi-node, liquid-cooled, rack-scale system with fifth-generation NVLink. The supercomputer will be interconnected by NVIDIA Quantum InfiniBand networking platform. The platform acts as a single GPU with 1.4 exaflops of AI performance and 30TB of fast memory and is designed for the most compute-intensive workloads. The YTL AI Supercomputer will surpass more than 300 exaflops of AI compute, making it one of the fastest supercomputers in the world.",Planned Q3 2025,Plans to use Blackwell chips,12.01729469,,,,,,,15428,NVIDIA,,,,,19.58624964,7.71E+19,7.71E+19,3.86E+19,1.9285E+19,37.0642272,,1.04063E+12,,,,,,,,,https://web.archive.org/web/20240717202025/https://www.ytlaicloud.com/press-releases/ytl-creates-one-of-the-worlds-most-advanced-supercomputers-powered-by-nvidia-grace-blackwell-based-dgx-cloud/,,,,
Nebius Kansas City Phase 2,Planned,Likely,Yes,19.85504644,36191.00556,NVIDIA B200,8000,United States of America,Nebius AI,,"Additional capacity was planned to go online Q2 2025
Is using Blackwell GPUs and H200s, unclear what proportion of each. For this, I am assuming have of the MW go to H200s vs B200s",Private,40,1250521491,"Kansas City, USA",Cloud,"H200,Blackwell","will house thousands of state-of-the-art NVIDIA GPUs, primarily NVIDIA Hopper GPUs in the initial phase, with the energy-efficient NVIDIA Blackwell platform expected to arrive in 2025. The colocation can be expanded from an initial 5 MW up to 40 MW, or about 35 thousand GPUs, at full potential capacity",Planned Q2 2025,,11.95195646,Nebius Kansas City Phase 1,,,,NVIDIA H200 SXM,18000,26000,NVIDIA,NVIDIA,,,,19.55401645,7.16E+19,7.16E+19,3.58E+19,1.7901E+19,41.2412,40,8.95275E+11,1250521491,,,,,,,,https://web.archive.org/web/20250109005023/https://group.nebius.com/newsroom/nebius-expands-in-us-with-first-gpu-cluster-in-kansas-city-offices-in-san-francisco-dallas-and-new-york,,,,
Tesla Earnings Call Claim,Existing,Unlikely,Unclear,19.84051384,35000,NVIDIA H100 SXM5 80GB,35000,United States of America,Tesla,3/31/2024,"Unclear if these are all in the same cluster. I personally doublt that it is. Musk diverted 12k H100s intended for Tesla to XAi, and it's unclear how that relates to this cluster. Also, NVIDIA stated that some of Tesla's statements ""did not align with what they had sold Tesla""",Private,49.9408,1249366458,,Tesla,H100,"We've installed and commissioned, meaning they're actually working 35,000 H100 computers or GPUs",3/31/2024,Likely not a single superocmputer,11.84098446,,,,,,,35000,NVIDIA,,,,,19.53943995,6.93E+19,6.93E+19,3.46E+19,1.73145E+19,49.9408,,6.93401E+11,1249366458,,,,,,,,https://web.archive.org/web/20241203032810/https://www.fool.com/earnings/call-transcripts/2024/04/23/tesla-tsla-q1-2024-earnings-call-transcript/,https://web.archive.org/web/20241111165856/https://www.datacenterdynamics.com/en/news/elon-musk-told-nvidia-to-ship-thousands-of-gpus-reserved-for-tesla-to-x-and-xai/,,,
Lambda Labs H100/H200,Existing,Likely,No,19.80159577,32000,NVIDIA H100 SXM5 80GB,32000,United States of America,Lambda Labs,12/5/2023,"Likely in the US since majority of operations happen there https://lambdalabs.com/service/gpu-cloud/faqs
Unclear if the 32k refers to one single supercomputer, the aggregation of many supercomputers, or the potential that they could hypotheticlaly scale up to",Private,46.47552,1056677222,,Cloud,"H100,H200","Reserved cloud clusters from 64 to 32,000 H100 or H200 SXM GPUs",12/5/2023,"Very little information on this. Unclear if this is one cluster or several, and how many are in the largest cluster",11.83329763,,,,,NVIDIA H200 SXM,,32000,NVIDIA,NVIDIA,,,,19.50052188,6.33E+19,6.33E+19,3.17E+19,1.58304E+19,46.47552,,6.81236E+11,1056677222,,,,,,,,https://web.archive.org/web/20231205014622/https://lambdalabs.com/nvidia-h100-nvidia-h200-gpus,https://archive.ph/Zzfyw,https://web.archive.org/web/20240413041051/https://www.futuriom.com/articles/news/lambda-scores-320-million-for-ai-infrastructure-cloud/2024/02,,
NVIDIA CoreWeave Eos-DFW Rumored Phase 2,Planned,Likely,Yes,19.80159577,32000,NVIDIA H100 SXM5 80GB,32000,United States of America,"NVIDIA,CoreWeave",,,Private,44.8448,1504015729,"1000 Coit Rd, Plano, TX 75075","NVIDIA,CoreWeave",H100,"NVIDIA is doing the same thing in Plano, Texas for a 32,000 GPU cluster that they're building",Planned 2025,Alluded to in Dylan Patel interview,11.84880979,NVIDIA CoreWeave Eos-DFW Phase 1,,,,,,32000,NVIDIA,,,,,19.50052188,6.33E+19,6.33E+19,3.17E+19,1.58304E+19,44.8448,,7.06008E+11,1504015729,,,,,,,,https://web.archive.org/web/20241006181731/https://www.dwarkeshpatel.com/p/dylan-jon,https://web.archive.org/web/20240901052405/https://www.nextplatform.com/2024/03/06/a-tale-of-two-nvidia-eos-supercomputers/,,,
Anonymized Chinese System,Existing,Confirmed,No,19.69897,30000,,,China,,12/15/2024,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,19.69897,5.00E+19,,5.00E+19,,,,,,,,,,,,,,,,,
Sesterce Pegasus,Planned,Likely,Yes,19.71999383,26518.44366,NVIDIA GB200,10496,,Sesterce,,,Private,25.2155904,,,Cloud,GB200,"""10496 GPUs""",Planned,,12.01729469,,,,,,,10496,NVIDIA,,,,,19.41896383,5.25E+19,5.25E+19,2.62E+19,1.312E+19,25.2155904,,1.04063E+12,,,,,,,,,https://web.archive.org/web/20240823055517/https://www.sesterce.com/reserved-cloud,,,,
Google A3 VMs,Planned,Likely,Yes,19.71141914,26000,NVIDIA H100 SXM5 80GB,26000,United States of America,Google,,"The A3 supercomputer seems to refer to an amalgamation of different clusters available as a virtual machine via the cloud and is operational. 
26k is the size of the largest individual cluster they say they can build as part of this. It is unclear if a cluster of this size has been constructed. If it has, it could have been in 2024",Private,36.4364,1222012780,,Cloud,H100,"""“For our largest customers, we can build A3 supercomputers up to 26,000 GPUs in a single cluster and are working to build multiple clusters in our largest regions,” a Google spokeswoman said in an email""",Planned 2025,This refers to the largest clusters Google says it can build for its customers,11.84880979,,,,,,,26000,NVIDIA,,,,,19.41034525,5.15E+19,5.15E+19,2.57E+19,1.28622E+19,36.4364,,7.06008E+11,1222012780,,,,,,,,https://web.archive.org/web/20240716093004/https://cloud.google.com/blog/products/compute/introducing-a3-supercomputers-with-nvidia-h100-gpus,https://web.archive.org/web/20240829141902/https://www.hpcwire.com/2023/05/10/googles-new-ai-focused-a3-supercomputer-has-26000-gpus/,,,
ParTec ELBJUWEL,Planned,Confirmed,Yes,19.69897,25265.2855,,,Germany,Helmholtz-Zentrum Dresden-Rossendorf (HZDR),,,Public,,,"Bautzner Landstraße 400, 01328 Dresden, Germany","Cloud,Academia,Industry",,with capabilities extending to about 50 exaflops for 8-bit floating-point operations,Planned,,,,,,,,,,Unknown,,,,,19.40140054,5.00E+19,5.00E+19,2.52E+19,,,,,,,,,,,,,https://web.archive.org/web/20241117210443/https://thequantuminsider.com/2024/10/22/partec-ag-and-hzdr-to-build-ai-supercomputer-supporting-research-in-ai-quantum-computing-and-hpc/,,,,
Meta GenAI 2024b,Existing,Confirmed,Yes,19.68695699,24576,NVIDIA H100 SXM5 80GB,24576,United States of America,Meta AI,3/12/2024,At least one of these was used to train Llama 3,Private,35.06700288,880071195.8,,Meta,H100,"Today, we’re sharing details on two versions of our 24,576-GPU data center scale cluster at Meta",3/12/2024,From official Meta announcement. At least one of these was used to train Llama 3.1,11.84098446,,,,,,,24576,NVIDIA,,TRUE,,1,19.3858831,4.86E+19,4.86E+19,2.43E+19,1.21577E+19,35.06700288,,6.93401E+11,880071195.8,,,,,,Meta GenAI 2024a,1,http://web.archive.org/web/20240828225612/https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/,https://web.archive.org/web/20240827223115/https://www.semianalysis.com/p/gpt-4-architecture-infrastructure,,,
Meta GenAI 2024a,Existing,Confirmed,Yes,19.68695699,24576,NVIDIA H100 SXM5 80GB,24576,United States of America,Meta AI,3/12/2024,At least one of these was used to train Llama 3,Private,35.06700288,880071195.8,,Meta,H100,"Today, we’re sharing details on two versions of our 24,576-GPU data center scale cluster at Meta",3/12/2024,From official Meta announcement. At least one of these was used to train Llama 3.1,11.84098446,,,,,,,24576,NVIDIA,,TRUE,,1,19.3858831,4.86E+19,4.86E+19,2.43E+19,1.21577E+19,35.06700288,,6.93401E+11,880071195.8,,,,TRUE,,Meta GenAI 2024a,1,http://web.archive.org/web/20240828225612/https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/,https://web.archive.org/web/20240827223115/https://www.semianalysis.com/p/gpt-4-architecture-infrastructure,,,
"""Jupiter, Jülich""",Existing,Confirmed,Yes,19.66817845,23536,NVIDIA GH200,23536,Germany,"EuroHPC JU,Julich Supercomputing Center",6/10/2025,,Public,18,290000685.9,"Jülich Supercomputing Centre (JSC)
52428 Jülich, Germany",,GH200,"""JUPITER marks the debut of a quad NVIDIA GH200 Grace Hopper Superchip node configuration, based on Eviden’s BullSequana XH3000 liquid-cooled architecture, with a booster module comprising close to 24,000 NVIDIA GH200 Superchips interconnected with the NVIDIA Quantum-2 InfiniBand networking platform.""

""If you look at the Top500 spreadsheet, it tells you the total number of cores used in the machine, plus the number of cores used by accelerators. (These cores are synonymous with a streaming multiprocessor in a GPU architecture, not a count of CUDA cores or tensor cores.) The Jupiter GPU Booster used for the HPL test that have it the number four ranking had 4,801,344 cores and of these, there are 3,106,752 cores allocated to the GPUs, which leaves 1,694,592 cores in the CPU hosts. At 72 cores per Grace, that is 23,536 Grace CPUs and therefore 23,536 Grace-Hopper units and therefore 23,536 Hopper H200 GPUs.""",6/10/2025,,12.11187595,,,,,,,23536,NVIDIA,,TRUE,,,19.36714845,4.66E+19,4.66E+19,2.33E+19,1.16386E+19,32.9833504,18,1.29383E+12,1089491618,287000000,290000685.9,"EuroHPC says in a statement that the cost of building, delivering, installing, and maintaining the Jupiter machine is €273 million ($287.3 million), and presumably the remaining part of that €500 million is to build or retrofit a datacenter for Jupiter and pay for power and cooling for the machine.",,,,,https://web.archive.org/web/20240825120701/https://nvidianews.nvidia.com/news/nvidia-grace-hopper-superchip-powers-jupiter-defining-a-new-class-of-supercomputers-to-propel-ai-for-scientific-discovery,https://archive.is/oyvz9,https://archive.ph/wip/HgCqo,https://www.anandtech.com/show/21136/nvidia-at-sc23-h200-accelerator-with-hbm3e-and-jupiter-supercomputer-for-2024,https://top500.org/system/180357/
Inflection AI Cluster,Planned,Likely,Unclear,19.63886848,22000,NVIDIA H100 SXM5 80GB,22000,United States of America,Inflection AI,,"This was in the process of being built, but it is unclear if it ever got built",Private,31,1034010814,,Inflection,H100,"Along with its partners CoreWeave and NVIDIA, Inflection AI is building the largest AI cluster in the world comprising 22,000 NVIDIA H100 Tensor Core GPUs",Planned,Unclear if this is still happening after the shakeup at Inflection A. They stated they're building this on their website in June 2023,11.84643289,,,,,,,22000,NVIDIA,,,,,19.33779459,4.35E+19,4.35E+19,2.18E+19,1.08834E+19,30.8308,31,7.02155E+11,1034010814,,,,,,,,https://archive.ph/xKdtC,https://web.archive.org/web/20230708074006/https://wccftech.com/inflection-ai-develops-supercomputer-equipped-with-22000-nvidia-h100-ai-gpus/amp/,,,
Oracle OCI MI300x,Existing,Likely,Yes,19.63187502,21648.57079,AMD Instinct MI300X,16384,United States of America,Oracle,11/15/2024,"Unclear exactly when this first became operational, if it was ever built. The announcement September 2024 states that MI300x clusters are now available, and can scale up to 16k GPUs. It's unclear whether this implies that they have already built a 16k MI300x cluster, or if they are just accepting orders to. It seems very likely that this cluster has been built, but I estimate the first operational date as Q4 2024 to be more conservative",Private,25.0478592,446003052.9,,Cloud,MI300X,"OCI Supercluster with AMD Instinct MI300X accelerators provides high-throughput, ultra-low latency RDMA cluster network architecture for up to 16,384 MI300X GPUs",Q4 2024,,11.93205781,,,,,,,16384,AMD,,TRUE,,8,19.33082842,4.28E+19,4.28E+19,2.14E+19,1.07102E+19,25.0478592,,8.55181E+11,446003052.9,,,,,,xAI Colossus Memphis Phase 1,0.216485708,https://archive.ph/gdAb4,,,,
Andreessen Horowitz Oxygen,Existing,Likely,Unclear,19.59747579,20000,NVIDIA H100 SXM5 80GB,20000,United States of America,Andreessen Horowitz,10/23/2024,"They have already secured thousands of chips, but plan to expand it to over 20k. 
Unclear if this is one cluster or many, if all the chips are H100s, and if they own these chips or are just renting them",Private,28.5376,762012445.8,,Startups via cloud,H100,"Earlier this year, took its focus on the generative technology further as it began accumulating A.I. chips—including the sought-after Nvidia (NVDA) H100 GPUs—with plans to expand its stockpile to more than 20,000, the Information reported this week",10/23/2024,"Unclear if this is one cluster or many, and if they own these chips or are just renting them",11.84098446,,,,,,,20000,NVIDIA,,,,,19.2964019,3.96E+19,3.96E+19,1.98E+19,9.894E+18,28.5376,,6.93401E+11,762012445.8,,,,,,,,https://web.archive.org/web/20240724120811/https://observer.com/2024/07/andreessen-horowitz-stocking-ai-chips-win-deals-gpu-shortage/,https://web.archive.org/web/20241110000626/https://techcrunch.com/2024/10/23/andreessen-horowitz-helps-founders-meet-compute-needs-with-oxygen-private-gpu-cluster/,,,
AWS EC2 P5 UltraClusters,Existing,Likely,Unclear,19.59747579,20000,NVIDIA H100 SXM5 80GB,20000,United States of America,Amazon,7/26/2023,Likely several such clusters in the same availability zone,Private,29.0472,719603493.4,United States of America,Cloud,H100,"To address customer needs for large-scale and low latency, P5 instances are deployed in the second-generation EC2 UltraClusters, which now provide customers with lower latency across up to 20,000+ NVIDIA H100 Tensor Core GPUs.",7/26/2023,Seems likely these are several clusters,11.83329763,,,,,,,20000,NVIDIA,,,,,19.2964019,3.96E+19,3.96E+19,1.98E+19,9.894E+18,29.0472,,6.81236E+11,719603493.4,,,,,,,,https://web.archive.org/web/20240823004421/https://aws.amazon.com/blogs/aws/new-amazon-ec2-p5-instances-powered-by-nvidia-h100-tensor-core-gpus-for-accelerating-generative-ai-and-hpc-applications/,https://web.archive.org/web/20240826022116/https://aws.amazon.com/ec2/ultraclusters/,,,
Anonymized Chinese System,Existing,Confirmed,No,19.60205999,20000,,,China,,3/15/2024,,Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,19.60205999,4.00E+19,,4.00E+19,,,,,,,,,,,,,,,,,
Anonymized Chinese System,Existing,Confirmed,No,19.60205999,20000,,,China,,7/15/2024,,,,,China,,,,,,,,,TRUE,,,,,Anonymized,Anonymized,,,,19.60205999,4.00E+19,,4.00E+19,,,,,,,,,,,,,,,,,
Anonymized Chinese System,Planned,Likely,Yes,19.60205999,20000,,,China,,,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,19.60205999,4.00E+19,,4.00E+19,,,,,,,,,,,,,,,,,
Anonymized Chinese System,Existing,Likely,Unclear,19.60205999,20000,,10000,China,,11/15/2024,,Private,30,700000000,China,,,,,,11.82390874,,,,,,10000,20000,Anonymized,Anonymized,,,,19.30103,4.00E+19,4.00E+19,2.00E+19,9E+18,30,,6.66667E+11,700000000,,,,,,,,,,,,
Anonymized Chinese System,Planned,Likely,Yes,19.47712125,20000,,,China,,,,Public,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,19.47712125,3.00E+19,,3.00E+19,,,,,,,,,,,,,,,,,
Project Ceiba Phase 1,Planned,Unlikely,Yes,19.51086573,16384,NVIDIA GH200,16384,United States of America,"Amazon,NVIDIA",,This plan has been updated to use Blackwell chips. Unclear if this supercomputer was ever completed,Private,22.9605376,751209101.2,,"Cloud,NVIDIA",GH200,"The system will feature 16,384 GH200 NVL32 Grace Hopper Superchips and is expected to be the world’s largest cloud AI supercomputer.",Planned,"This was officially planned, but it appears they have since updated their plan (see Project Ceiba Phase 2)",11.84885369,,,,,,,16384,NVIDIA,,,,,19.20983574,3.24E+19,3.24E+19,1.62E+19,8.10189E+18,22.9605376,,7.0608E+11,751209101.2,,,,,,,,https://web.archive.org/web/20240228165625/https://www.datacenterdynamics.com/en/news/project-ceiba-aws-and-nvidia-plan-to-build-worlds-largest-cloud-ai-supercomputer/,,,,
Paper on Llama 3.1,Existing,Confirmed,Yes,19.51086573,16384,NVIDIA H100 SXM5 80GB,16384,,Meta AI,7/23/2024,,Private,23.37800192,584357946.5,,Meta,H100,,7/23/2024,,11.84098446,,,TRUE,"Meta GenAI 2024a,Meta GenAI 2024b",,,16384,NVIDIA,,,,3,19.20979185,3.24E+19,3.24E+19,1.62E+19,8.10516E+18,23.37800192,,6.93401E+11,584357946.5,,,,,,Meta GenAI 2024a,0.666666667,https://web.archive.org/web/20250213040824/https://scontent-sjc3-1.xx.fbcdn.net/v/t39.2365-6/468347782_9231729823505907_4580471254289036098_n.pdf?_nc_sid=3c67a6&_nc_zt=14&oe=67B094C0&_nc_gid=Amq2wPDHtohnYaTcmwzmEM_&_nc_cat=110&_nc_ohc=qCynUx1yT0sQ7kNvgGyJpa7&ccb=1-7&oh=00_AYCdVmbLn7nRo_-1yU_lxmEbGI1Bby-xGC_1TZe4QDWaPA&_nc_ht=scontent-sjc3-1.xx,,,,
Yotta Shakti Cloud D1,Planned,Likely,Yes,19.51086573,16384,NVIDIA H100 SXM5 80GB,16384,India,Yotta Data Services,,,Private,22.9605376,770056053.2,"Yotta Data Center Park, Greater Noida
Plot No 7, Tusiana Village, Knowledge Park V, Greater Noida, Tusyana, Uttar Pradesh 201301, India",Cloud,H100,"Yotta will deploy the first cluster of 16,384 GPUs at NM1... Next, Yotta will deploy a similar-sized cluster at D1",Planned 2025,"Size is officially stated to be ""similar"" as NM1 cluster",11.84880979,,,,,,,16384,NVIDIA,,,,,19.20979185,3.24E+19,3.24E+19,1.62E+19,8.10516E+18,22.9605376,,7.06008E+11,770056053.2,,,,,,,,https://web.archive.org/web/20240521205830/https://yotta.com/media/press-release-yotta-data-services-collaborates-with-nvidia-to-catalyze-indias-ai-transformation/,,,,
Yotta Shakti Cloud NM1 Phase 2,Planned,Confirmed,Yes,19.51086573,16384,NVIDIA H100 SXM5 80GB,16384,India,Yotta Data Services,,,Private,22.9605376,770056053.2,"Yotta Datacenter Park - Panvel Hiranandani Fortune City. Survey No. 30, MH SH 76, Panvel, Navi Mumbai, Maharashtra 410206, India",Cloud,H100,"Yotta will deploy the first cluster of 16,384 GPUs at NM1",Planned 2025,,11.84880979,Yotta Shakti Cloud NM1 Phase 1,,,,,,16384,NVIDIA,,,,,19.20979185,3.24E+19,3.24E+19,1.62E+19,8.10516E+18,22.9605376,,7.06008E+11,770056053.2,,,,,,,,https://web.archive.org/web/20240527112515/https://shakticloud.ai/,https://web.archive.org/web/20240423043934/https://yotta.com/media/press-release-yotta-receives-indias-first-ever-consignment-of-nvidia-h100-the-fastest-gpus-in-the-world-at-its-nm1-data-center-park/,https://web.archive.org/web/20240521205830/https://yotta.com/media/press-release-yotta-data-services-collaborates-with-nvidia-to-catalyze-indias-ai-transformation/,,
NexGen Cloud Hyperstack AQ Compute Supercomputer,Existing,Likely,Yes,19.51086573,16384,NVIDIA H100 SXM5 80GB,16384,Norway,NexGen Cloud,4/19/2024,"They seem to be stating that all 16k GPUs are in the same physical location, but I'm not fully confident in this",Private,23.37800192,582815283.2,"Follummoveien 94, 3516 Hønefoss, Norway",Cloud,H100,"""With up to 16,384 H100 80G cards operating in a single cluster""

""by June the supercloud will be comprised of 20,000 Nvidia H100 Tensor Core GPUs""",4/19/2024,,11.84098446,,,,,,,16384,NVIDIA,,TRUE,,3,19.20979185,3.24E+19,3.24E+19,1.62E+19,8.10516E+18,23.37800192,,6.93401E+11,582815283.2,,,,,,Meta GenAI 2024a,0.666666667,https://web.archive.org/web/20240911111117/https://www.datacenterdynamics.com/en/news/nexgen-cloud-to-host-ai-supercloud-in-norwegian-data-center/,https://archive.ph/XcQQY,https://web.archive.org/web/20240524063011/https://www.hyperstack.cloud/nvidia-h100-sxm,,
Oracle OCI Supercluster H100s,Existing,Likely,Yes,19.51086573,16384,NVIDIA H100 SXM5 80GB,16384,United States of America,Oracle,4/15/2024,"Unclear when this was first operational. In November 2023, they state that OCI superclusters ""can scale up to"" 16k H100s. However, it's unclear whether they had actually built a 16k cluster yet, or were just accepting orders. Musk's xAI was said to be renting 16k of these GPUs in May 2024, signaling that at least one Oracle cluster of this size was operational by then. I mark the first operational date as April 2024 to be slightly conservative. 
Also, it's very likely that Oracle has several of these 16k H100 clusters",Private,23.37800192,584846286.9,,"Cloud,Cohere,xAI",H100,"""With high-performance local storage, high-performance computing (HPC) storage, cluster networking and memory, bare metal instances can be part of an OCI Supercluster that can scale to tens of thousands of NVIDIA H100 GPUs.""
""An OCI Supercluster with H100 GPUs can scale up to 16,384 GPUs""",2024-04,,11.84098446,,,,,,,16384,NVIDIA,,TRUE,,3,19.20979185,3.24E+19,3.24E+19,1.62E+19,8.10516E+18,23.37800192,,6.93401E+11,584846286.9,,,,,,Meta GenAI 2024a,0.666666667,https://web.archive.org/web/20231023100410/https://blogs.oracle.com/cloud-infrastructure/post/general-availability-oci-compute-nvidia-h100,https://archive.ph/B3HJm,https://archive.ph/iLAnE,https://archive.ph/cgqbk,
Sakura's B200s Phase 2,Planned,Confirmed,Yes,19.47529338,15095.50278,NVIDIA B200,4000,Japan,Sakura Internet,,"This builds on their previous 2k H100s
""Japan's Sakura Internet has secured around 20 billion yen worth ($130 million) of Nvidia's next-generation B200 chips"" ~= 4k B200s

10000 GPUs
at least 2k of these are H100s
Unclear amount of B200s
I estimate 4k B200s, and 6k H100s, based on a statement that they acquired $130 million worth of B200s
",Public/Private,16.4164,482340860.9,"Ishikari City, Hokkaido Japan",Cloud,"B200,H100","The company plans to expand the number of GPUs deployed in “Koukaryoku” to fivefold from the initially planned quantity, aiming to incorporate around 10,000 GPUs, including NVIDIA’s latest “NVIDIA HGX B200 system” introduced in March",Planned Q1 2028,Unclear how many are H100s vs B200s,11.95896801,Sakura's H100s Phase 1,,,,NVIDIA H100 SXM5 80GB,6000,10000,NVIDIA,NVIDIA,,,,19.17424594,2.99E+19,2.99E+19,1.49E+19,7.4682E+18,16.4164,,9.09846E+11,482340860.9,,,,,,,,https://web.archive.org/web/20240603205526/https://www.trendforce.com/news/2024/04/22/news-japans-sakura-ai-gpu-procurement-reportedly-increases-fivefold-including-purchase-of-nvidia-b200/,https://web.archive.org/web/20240913153045/https://www.smbom.com/news/13481,,,
Microsoft NVIDIA partnership,Planned,Unlikely,Unclear,19.47253705,15000,NVIDIA H100 SXM5 80GB,15000,United States of America,Microsoft,5/15/2023,"The plans in this article are very vague. This superocmputer probably ended up getting built, but we don't know when, and we likely capture it as another independent entry",Private,21.7854,715407335.7,,,"H100,A100","It is the first public cloud to incorporate NVIDIA’s advanced AI stack, adding tens of thousands of NVIDIA A100 and H100 GPUs, NVIDIA Quantum-2 400Gb/s InfiniBand networking and the NVIDIA AI Enterprise software suite to its platform.",5/15/2023,"Possible duplicate, number of GPUs says ""thousands"" so this is a very rough estimate",11.83329763,,,,,,,15000,NVIDIA,,,,,19.17146317,2.97E+19,2.97E+19,1.48E+19,7.4205E+18,21.7854,,6.81236E+11,715407335.7,,,,,,,,https://web.archive.org/web/20240819113910/https://nvidianews.nvidia.com/news/nvidia-microsoft-accelerate-cloud-enterprise-ai,,,,
G42 Microsoft 30Mw UAE Cluster A,Planned,Likely,Yes,19.47253705,15000,NVIDIA H100 SXM5 80GB,15000,United Arab Emirates,"G42,Microsoft",,At least one of the 30MW data centers is in Abu Dhabi,Public/Private,30,705007373,"Abu Dhabi, United Arab Emirates","Microsoft,Azure",H100,"Private conversations with Microsoft and G42 executives revealed that G42 and Khazna are building three new data centers in the UAE for Microsoft. The centers will be 30, 30, and 100 megawatts each, the equivalent of nearly 100,000 H100 chips (approximately 15,000, 15,000, and 50,000–60,000 respectively).",Planned,,11.69434191,,,,,,,15000,NVIDIA,,,,,19.17146317,2.97E+19,2.97E+19,1.48E+19,7.4205E+18,21.021,30,4.947E+11,705007373,,,,,,,,https://web.archive.org/web/20250221170709/https://www.csis.org/analysis/united-arab-emirates-ai-ambitions,https://web.archive.org/web/20241006181731/https://www.dwarkeshpatel.com/p/dylan-jon,,,
G42 Microsoft 30Mw UAE Cluster B,Planned,Likely,Yes,19.47253705,15000,NVIDIA H100 SXM5 80GB,15000,United Arab Emirates,"G42,Microsoft",,,Public/Private,30,705007373,,"Microsoft,Azure",H100,"Private conversations with Microsoft and G42 executives revealed that G42 and Khazna are building three new data centers in the UAE for Microsoft. The centers will be 30, 30, and 100 megawatts each, the equivalent of nearly 100,000 H100 chips (approximately 15,000, 15,000, and 50,000–60,000 respectively).",Planned,,11.69434191,,,,,,,15000,NVIDIA,,,,,19.17146317,2.97E+19,2.97E+19,1.48E+19,7.4205E+18,21.021,30,4.947E+11,705007373,,,,,,,,https://web.archive.org/web/20250221170709/https://www.csis.org/analysis/united-arab-emirates-ai-ambitions,https://web.archive.org/web/20241006181731/https://www.dwarkeshpatel.com/p/dylan-jon,,,
iGenius Colosseum,Planned,Likely,Yes,19.45939249,14552.80445,NVIDIA GB200,5760,Italy,iGenius,,80 servers x 72 GPUs/server = 5760 GPUs,Private,13.837824,,Southern Italy,iGenius,GB200,iGenius aims to develop colossal AI models using 80 GB200 NVL72 servers,Planned 2025,,12.01729469,,,,,,,5760,NVIDIA,,,,,19.15836249,2.88E+19,2.88E+19,1.44E+19,7.2E+18,13.837824,,1.04063E+12,,,,,,,,,https://web.archive.org/web/20250306213958/https://opentools.ai/news/igenius-and-nvidia-to-transform-ai-landscape-with-colosseum-project,,,,
Microsoft Azure Eagle,Existing,Confirmed,Yes,19.45480829,14400,NVIDIA H100 SXM5 80GB,14400,United States of America,Microsoft,11/15/2023,Said to be part of a larger system,Private,20.913984,476613908.5,,Cloud,H100,"""With 14,400 NVIDIA H100 GPUs (14400/8 = 1800 nodes?), InfiniBand, and Intel Xeon Sapphire Rapids CPUs, the new""",11/15/2023,,11.83329763,,,,,,,14400,NVIDIA,,TRUE,,1,19.1537344,2.85E+19,2.85E+19,1.42E+19,7.12368E+18,20.913984,,6.81236E+11,476613908.5,,,,,,Microsoft Azure Eagle,1,https://web.archive.org/web/20240901103636/https://www.servethehome.com/microsoft-azure-eagle-is-a-paradigm-shifting-cloud-supercomputer-nvidia-intel/,,,,
TensorWave MI300X Cluster 2,Planned,Likely,Yes,19.41745509,13213.23901,AMD Instinct MI300X,10000,United States of America,TensorWave,,Unclear how many would be in each datacenter. Assuming half in each,Private,15.015,271858907.9,,Cloud,MI300X,"By the end of 2024, TensorWave aims to have 20,000 MI300X accelerators deployed across two facilities",Planned H1 2025,,11.93988314,,,,,,,10000,AMD,,,,,19.11640848,2.61E+19,2.61E+19,1.31E+19,6.537E+18,15.015,,8.70729E+11,271858907.9,,,,,,,,https://web.archive.org/web/20241009165502/https://www.theregister.com/2024/04/16/amd_tensorwave_mi300x/,,,,
TensorWave MI300X Cluster 1 Phase 2,Planned,Likely,Yes,19.41745509,13213.23901,AMD Instinct MI300X,10000,United States of America,TensorWave,,Unclear how many would be in each datacenter. Assuming half in each,Private,15.015,271858907.9,,Cloud,MI300X,"""By the end of 2024, TensorWave aims to have 20,000 MI300X accelerators deployed across two facilities""",Planned H1 2025,,11.93988314,TensorWave MI300X Cluster 1 Phase 1,,,,,,10000,AMD,,,,,19.11640848,2.61E+19,2.61E+19,1.31E+19,6.537E+18,15.015,,8.70729E+11,271858907.9,,,,,,,,https://web.archive.org/web/20241009165502/https://www.theregister.com/2024/04/16/amd_tensorwave_mi300x/,,,,
Foxconn Big Innovation Cloud AI factory,Planned,Likely,Yes,19.41204033,13049.51996,NVIDIA GB300 (Blackwell Ultra),5000,Taiwan,Foxconn,,Unclear what proportion of the Blackwell GPUs will be GB300s vs GB200s. I assume it will be 50% each,Private,26.026,,,"Taiwan National Science and Technology Council,TSMC,Foxconn",Blackwell,"the supercomputer will consist of 10,000 Nvidia Blackwell Ultra GPUs... will include the GB300 NVL72",Planned 2026,,11.98253258,,,,,NVIDIA GB200,5000,10000,NVIDIA,NVIDIA,,,,19.39794001,2.58E+19,2.58E+19,2.50E+19,1.25E+19,26.026,,9.60578E+11,,,,,,,,,https://web.archive.org/web/20250519123959/https://www.datacenterdynamics.com/en/news/nvidia-and-foxconn-plan-taiwanese-ai-supercomputer/,,,,
Saudi Data & AI Authority Sovereign AI factory,Planned,Likely,Yes,19.39794001,12632.64275,NVIDIA GB200,5000,Saudi Arabia,Saudi Data and Artificial Intelligence Authority,,,Public,12.012,,,,"Blackwell,Uncertain","NVIDIA and the Saudi Data & AI Authority (SDAIA) will deploy up to 5,000 Blackwell GPUs for a sovereign AI factory and enable smart city solutions",Planned,,12.01729469,,,,,,,5000,NVIDIA,,,,,19.09691001,2.50E+19,2.50E+19,1.25E+19,6.25E+18,12.012,,1.04063E+12,,,,,,,,,https://archive.ph/vvAxQ,,,,
xAI Fulton Georgia,Planned,Likely,Yes,19.38344394,12217.94442,NVIDIA H100 SXM5 80GB,12112,United States of America,xAI,,"The article indicates that the 12,448 GPUs are 97% Hoppers, 3% Amperes

Location isn't confirmed, but an article from a year earlier reports that xAI received a ""10-year tax break on a $700 million project to deploy IT equipment at QTS’ 1025 Jefferson Street site"", which matches the description that this site is in Fulton, GA",Private,20,,"1025 Jefferson St NW, Atlanta, GA 30318","xAI,X","H100,A100","""The Georgia facility will house an estimated 12,448 Nvidia GPUs. The vast majority of these are Hopper generation H100 GPUs""
""Roughly 3% of the chips are Nvidia's less-powerful A100 GPUs""",Planned 2025,,11.78134044,,,,,NVIDIA A100,336,12448,NVIDIA,NVIDIA,,,,19.08237043,2.42E+19,2.42E+19,1.21E+19,6.04422E+18,17.2428256,20,6.04422E+11,,700000000,,"Of the $700 million in accelerated computing hardware going into the facility, $442 million is allocated to X, and $258 million is allocated to xAI",,,,,https://archive.ph/Oeulg,https://web.archive.org/web/20250125054811/https://www.datacenterdynamics.com/en/news/elon-musks-twitterx-granted-10-million-tax-break-for-ai-hardware-in-georgia/,,,
Foxconn Hon Hai Kaohsiung Supercomputer,Planned,Confirmed,Yes,19.36248247,11642.24356,NVIDIA GB200,4608,Taiwan,Foxconn,,"There was a later announcement for a 10k Blackwell Foxconn supercomputer, so slightly unclear if this just got upgraded or if they are seperate supercomputers",Private,11.0702592,,"806, Taiwan, Kaohsiung City, Cianjhen District, Chenggong 2nd Rd, 25號2 樓",Foxconn,GB200,"will be built around NVIDIA’s groundbreaking Blackwell architecture and feature the GB200 NVL72 platform, which includes a total of 64 racks and 4,608 Tensor Core GPUs",Planned 2026,,12.01729469,,,,,,,4608,NVIDIA,,,,,19.06145248,2.30E+19,2.30E+19,1.15E+19,5.76E+18,11.0702592,,1.04063E+12,,,,,,,,,https://web.archive.org/web/20241009234027/https://blogs.nvidia.com/blog/foxconn-taiwan-blackwell/,https://web.archive.org/web/20241008143947/https://www.datacenterdynamics.com/en/news/nvidia-and-foxconn-to-build-taiwans-fastest-ai-supercomputer/,,,
NVIDIA MLPerf v4.0 Submission 2024,Existing,Confirmed,Yes,19.3615024,11616,NVIDIA H100 SXM5 80GB,11616,,NVIDIA,6/12/2024,It seems likely that this could be an expansion on the NVIDIA Eos-DFW supercomputer,Private,16.57463808,414709261.3,,NVIDIA,H100,"In this round of MLPerf Training, NVIDIA has more than tripled its submission scale to 11,616 H100 GPUs and more than tripled performance to 3.4 minutes to train,",6/12/2024,,11.84098446,,,TRUE,NVIDIA CoreWeave Eos-DFW Phase 1,,,11616,NVIDIA,,,,6,19.06042851,2.30E+19,2.30E+19,1.15E+19,5.74644E+18,16.57463808,,6.93401E+11,414709261.3,,,,,,Meta GenAI 2024a,0.47265625,https://web.archive.org/web/20240915170526/https://developer.nvidia.com/blog/nvidia-sets-new-generative-ai-performance-and-scale-records-in-mlperf-training-v4-0/,,,,
Microsoft Azure MLPerf 3.1 Submission,Existing,Confirmed,Yes,19.32793505,10752,NVIDIA H100 SXM5 80GB,10752,United States of America,Microsoft,11/8/2023,Likely is CoreWeave Eos-DFW Phase 1,Private,15.61577472,355871718.4,,Microsoft,H100,"The GPT-3 LLM model and its 175 billion parameters were trained to completion in four minutes on 1,344 ND H100 v5 virtual machines (VMs), which represent 10,752 NVIDIA H100 Tensor Core GPUs",11/8/2023,,11.83329763,,,TRUE,Microsoft Azure Eagle,,,10752,NVIDIA,,,,1,19.02686116,2.13E+19,2.13E+19,1.06E+19,5.31901E+18,15.61577472,,6.81236E+11,355871718.4,,,,,,NVIDIA CoreWeave Eos-DFW Phase 1,1,https://web.archive.org/web/20240415030521/https://azure.microsoft.com/en-us/blog/azure-sets-a-scale-record-in-large-language-model-training/,,,,
NVIDIA CoreWeave Eos-DFW Phase 1,Existing,Confirmed,Yes,19.32793505,10752,NVIDIA H100 SXM5 80GB,10752,United States of America,"NVIDIA,CoreWeave",11/8/2023,There's also a 4.6k cluster with the same name,Private,15.61577472,355871718.4,"1000 Coit Rd, Plano, TX 75075","NVIDIA,CoreWeave",H100,"NVIDIA’s AI platform raised the bar for AI training and high performance computing in the latest MLPerf industry benchmarks. Among many new records and milestones, one in generative AI stands out: NVIDIA Eos — an AI supercomputer powered by a whopping 10,752 NVIDIA H100 Tensor Core GPUs and NVIDIA Quantum-2 InfiniBand networking — completed a training benchmark based on a GPT-3 model with 175 billion parameters trained on one billion tokens in just 3.9 minutes.",11/8/2023,Stated in NVIDIA blog post,11.83329763,,,,,,,10752,NVIDIA,,TRUE,,1,19.02686116,2.13E+19,2.13E+19,1.06E+19,5.31901E+18,15.61577472,,6.81236E+11,355871718.4,,,,,,NVIDIA CoreWeave Eos-DFW Phase 1,1,https://web.archive.org/web/20240430123932/https://blogs.nvidia.com/blog/scaling-ai-training-mlperf/,https://web.archive.org/web/20240901052405/https://www.nextplatform.com/2024/03/06/a-tale-of-two-nvidia-eos-supercomputers/,,,
Alps Supercomputer Phase 2,Existing,Confirmed,Yes,19.32793505,10752,NVIDIA GH200,10752,Switzerland,ETH Domain,9/17/2024,"Total number of sockets = 10,752
There are other portions of this supercomputer, but it is unclear which are in which location, as they are distributed. The 10k Grace Hopper GPUs is by far the largest and most significant portion, though",Public,11.9088,119153004.5,"Swiss National Supercomputing Centre
Via Trevano 131, 6900 Lugano, Switzerland",Swiss Universities,GH200,"has 10,752 of the world’s coveted NVIDIA Grace Hopper superchips",9/17/2024,Specs confirmed by the organization building this,11.95103705,Alps Supercomputer Phase 1,,,,,,10752,NVIDIA,,TRUE,,10,19.02690505,2.13E+19,2.13E+19,1.06E+19,5.31686E+18,15.34181376,11.9088,8.93382E+11,492647200.8,118000000,119153004.5,"""the CHF100 million ($118 million) supercomputer""",,,xAI Colossus Memphis Phase 1,0.10752,https://web.archive.org/web/20240519135849/https://www.cscs.ch/computers/alps,https://web.archive.org/web/20240415160748/https://www.hpcwire.com/2023/04/05/into-the-alps-what-exactly-is-the-new-swiss-supercomputer-infrastructure/,https://web.archive.org/web/20241004224121/https://www.hpcwire.com/off-the-wire/cscs-debuts-alps-supercomputer-enhancing-switzerlands-research-infrastructure/,https://web.archive.org/web/20250215163401/https://www.top500.org/system/180259/,https://www.swissinfo.ch/eng/science/how-switzerlands-alps-supercomputer-aims-to-advance-ai/87659724
Oracle OCI Supercluster A100s,Existing,Likely,Yes,19.31063452,10332.10308,NVIDIA A100,32768,United States of America,Oracle,11/15/2023,"Unclear exactly when this first became operational, if it was ever built. The announcement March 2023 states Oracle OCI can now scale to 32k A100s, but it's unclear whether this implies that a 32k A100 cluster has already been built, or just that they have the capacity to build one. It seems likely that a cluster of this size was built, but there's little direct evidence for this. 
I estimate Q4 2023 to be slightly conservative on the timing for when it was first operational. ",Private,27.19481856,646451863.9,,Cloud,A100,"OCI Supercluster networking can now scale up to 4,096 OCI Compute Bare Metal instances with 32,768 A100 GPUs",Q4 2023,,11.57511836,,,,,,,32768,NVIDIA,,TRUE,,4,19.00960453,2.04E+19,2.04E+19,1.02E+19,5.11181E+18,27.19481856,,3.7594E+11,646451863.9,,,,,,Microsoft Azure Eagle,0.717507158,https://web.archive.org/web/20240910231627/https://www.oracle.com/news/announcement/nvidia-chooses-oracle-cloud-infrastructure-for-ai-services-2023-03-21/,https://archive.ph/B3HJm,,,
Google TPUv5e,Existing,Confirmed,Unclear,19.30148559,10116.72158,Google TPU v5e,50944,,Google,11/8/2023,,Private,14.79780557,158260458.1,,Google,TPUv5e,"we used Multislice Training to run what we believe to be the world’s largest publicly disclosed LLM distributed training job (in terms of the number of chips used for training) on a compute cluster of 50,944 Cloud TPU v5e chips (spanning 199 Cloud TPU v5e pods) that is capable of achieving 10 exa-FLOPs (16-bit), or 20 exa-OPs (8-bit), of total peak performance",11/8/2023,Unclear whether it is a single cluster,11.82980268,,,,,,,50944,Google,,,,,19,2.00E+19,2.00E+19,1.00E+19,,14.79780557,,6.75776E+11,158260458.1,,,,,,,,https://web.archive.org/web/20240822230040/https://cloud.google.com/blog/products/compute/the-worlds-largest-distributed-llm-training-job-on-tpu-v5e,,,,
Imbue 10k Cluster,Existing,Likely,Yes,19.29644579,10000,NVIDIA H100 SXM5 80GB,10000,United States of America,Imbue,9/7/2023,,Private,14.5236,332254920.4,,Imbue,H100,"Dell PowerEdge server cluster with nearly 10,000 NVIDIA H100 Tensor Core GPUs",9/7/2023,,11.83329763,,,,,,,10000,NVIDIA,,TRUE,,1,18.99537191,1.98E+19,1.98E+19,9.89E+18,4.947E+18,14.5236,,6.81236E+11,332254920.4,,,,,,Tesla 10k H100 Cluster,1,https://web.archive.org/web/20240423234157/https://investors.delltechnologies.com/news-releases/news-release-details/imbue-develop-next-generation-ai-models-150-million-dell-high,https://web.archive.org/web/20240830093115/https://imbue.com/company/introducing-imbue/,,,
Poolside 10k Cluster,Planned,Likely,Yes,19.29644579,10000,NVIDIA H100 SXM5 80GB,10000,,Poolside,,Unclear if they own this or if they're just renting it.,Private,14.014,470004915.3,,Poolside,H100,"With this new funding round we have been able to scale our training cluster to 10,000 GPUs and start the work of scaling up both RLCEF and our model training.",Planned,"Unclear if they own this or if they're just renting it.
Type of GPU unspecified, I'm assuming they're H100s",11.84880979,,,,,,,10000,NVIDIA,,,,,18.99537191,1.98E+19,1.98E+19,9.89E+18,4.947E+18,14.014,,7.06008E+11,470004915.3,,,,,,,,https://web.archive.org/web/20241007114224/https://poolside.ai/checkpoint/announcing-our-500-million-fundraise-to-make-progress-towards-agi,,,,
Tesla 10k H100 Cluster,Existing,Confirmed,Yes,19.29644579,10000,NVIDIA H100 SXM5 80GB,10000,United States of America,Tesla,8/28/2023,"There are claims of the cluster costing $300 million, however, that was not an official anouncement and might just be a BOTEC ""30k per H100 times 10,000",Private,14.5236,360340094.3,"""on prem"" for Tesla",Tesla,H100,"Tomorrow, @Tesla will turn on a massive and very expensive 10,000 unit NVIDIA H100 GPU cluster to help it train FSD",8/28/2023,"Said it was ""going live tomorrow"" in Aug 2023",11.83329763,,,,,,,10000,NVIDIA,,TRUE,,1,18.99537191,1.98E+19,1.98E+19,9.89E+18,4.947E+18,14.5236,,6.81236E+11,360340094.3,,,,,,Tesla 10k H100 Cluster,1,https://web.archive.org/web/20240802004227/https://www.theregister.com/2023/08/30/tesla_nvidia_supercomputer/,https://archive.ph/PwU9T,https://web.archive.org/web/20240524114346/https://www.tomshardware.com/news/teslas-dollar300-million-ai-cluster-is-going-live-today,,
Anonymized Chinese System,Existing,Likely,Yes,19.47712125,10000,,,China,,10/15/2024,,Public/Private,,,China,,,,,,,,,,,,,20000,Anonymized,Anonymized,TRUE,,9,19.47712125,3.00E+19,,3.00E+19,,,,,,,,,,,xAI Colossus Memphis Phase 1,0.1,,,,,
Anonymized Chinese System,Existing,Likely,Yes,19.30103,10000,,,China,,2/15/2025,,Private,,,China,,,,,,,,,,,,,10000,Anonymized,Anonymized,TRUE,,17,,2.00E+19,2.00E+19,,,,,,,,,,,,xAI Colossus Memphis Phase 2,0.06,,,,,
Anonymized Chinese System,Existing,Confirmed,No,19.30103,10000,,,China,,6/15/2024,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,19.30103,2.00E+19,,2.00E+19,,,,,,,,,,,,,,,,,
SoftBank Planned B200 Superpod,Planned,Likely,Yes,19.25527251,9095.502779,NVIDIA B200,4000,Japan,Softbank,,"They had 6k GPUs before (2k A100s, 4k H100s), so bringing the total to 10k GPUs would be adding 4k Blackwell GPUs",Private,8.008,200337911.7,Japan,Softbank,B200,"The company has further plans to expand this with an Nvidia DGX SuperPOD featuring GGX B200 systems within the fiscal year (FY) 2025, ultimately growing the number of GPUs installed to around 10,000 with a compute capability of 25.7 exaflops.",Planned,,12.05071844,,,,,,,4000,NVIDIA,,,,,18.95424251,1.80E+19,1.80E+19,9.00E+18,4.5E+18,8.008,,1.12388E+12,200337911.7,,,,,,,,https://web.archive.org/web/20241116120654/https://nvidianews.nvidia.com/news/nvidia-and-softbank-accelerate-japans-journey-to-global-ai-powerhouse,https://web.archive.org/web/20241221053240/https://www.datacenterdynamics.com/en/news/softbank-installs-4000-nvidia-h100s-for-japanese-ai-computing-platform/,,,
S. Korea 6th national supercomputer,Planned,Likely,Yes,19.24092847,8800,NVIDIA H100 SXM5 80GB,8800,Korea (Republic of),Ministry of Science and ICT,,,Public,12.33232,413604325.5,Korea (Republic of),Researchers,H100,"A total of 8,800 GPUs will be used for No. 6",Planned 2026,Uncertain what type of GPUs will be used,11.84880979,,,,,,,8800,NVIDIA,,,,,18.93985458,1.74E+19,1.74E+19,8.71E+18,4.35336E+18,12.33232,,7.06008E+11,413604325.5,,,,,,,,https://web.archive.org/web/20250110174004/https://biz.chosun.com/en/en-science/2025/01/10/A4AEH4EX2JEA7CGFXUOADNCGD4/,,,,
Stargate UAE Phase 1,Planned,Likely,Unclear,19.21748394,8337.544214,NVIDIA GB300 (Blackwell Ultra),100000,United Arab Emirates,Stargate (OpenAI),,"Chip amount roughly estimated based on power capacity. Assuming for now that they're using GB300s, but this hasn't been officially confirmed
Part of the broader planned 5GW UAE/USA cluster",Public/Private,200,,"Abu Dhabi, United Arab Emirates",OpenAI,Uncertain,"""A 1GW Stargate UAE cluster in Abu Dhabi with 200MW expected to go live in 2026""
""Breaking ground on 1GW AI Datacenter, part of a planned 5GW UAE-US artificial intelligence campus; largest such deployment outside of the US""
""The campus, which will span 10 square miles within the Emirate, will be built by G42 and operated in partnership with several US companies.""
""Stargate UAE is a next-generation AI infrastructure cluster that will run in the newly established 5-gigawatt UAE–U.S. AI Campus in Abu Dhabi. Stargate UAE, a 1-gigawatt compute cluster""",Planned 2026,,12.09691001,,,,,,,100000,NVIDIA,,,,,20.39794001,1.65E+19,1.65E+19,2.50E+20,1.25E+20,280.28,200,1.25E+12,,,,,,,,,https://web.archive.org/web/20250515200336/https://www.commerce.gov/news/press-releases/2025/05/uae-and-us-presidents-attend-unveiling-phase-1-new-5gw-ai-campus-abu,https://archive.ph/hQWdM,https://archive.ph/PfVwI,,
Sesterce Nordics,Existing,Likely,Yes,19.20983574,8192,NVIDIA H100 SXM5 80GB,8192,,Sesterce,6/29/2024,"This cluster was originally planned for Spain, but was later moved to the Nordics",Private,11.68900096,291932774.9,The Nordics,Cloud,H100,"8192 H100s available from 06/29/2024 to 06/29/2026 (Spain)
Sesterce 1024 Servers Cluster available in Spain Europe End of June.",6/29/2024,,11.84098446,,,,,,,8192,NVIDIA,,TRUE,,12,18.90876185,1.62E+19,1.62E+19,8.11E+18,4.05258E+18,11.68900096,,6.93401E+11,291932774.9,,,,,,Meta GenAI 2024a,0.333333333,https://web.archive.org/web/20240523122734/https://gpulist.ai/detail/f6be42a,,,,
Nebius 8k Finland Phase 1,Existing,Likely,Yes,19.19953578,8000,NVIDIA H100 SXM5 80GB,8000,Finland,Nebius AI,5/10/2024,,Private,11.41504,284577775,"Moreenikatu 6, 04600 Mäntsälä, Finland",Cloud,H100,"we also constructed an 8,000-GPU supercluster",5/10/2024,,11.84098446,,,,,,,8000,NVIDIA,,TRUE,,12,18.89846189,1.58E+19,1.58E+19,7.92E+18,3.9576E+18,11.41504,,6.93401E+11,284577775,,,,,,Meta GenAI 2024a,0.325520833,https://web.archive.org/web/20241008144121/https://www.datacenterdynamics.com/en/news/nebius-deploys-ai-cluster-at-equinix-data-center-in-paris/,https://archive.ph/x0bXS,,,
Magic G4 Google Cloud Rental,Existing,Confirmed,Yes,19.19953578,8000,NVIDIA H100 SXM5 80GB,8000,United States of America,Google,,"Rented from Google Cloud. Not marked ""potential duplicate"" because we don't have other operational Google cloud clusters of this size",Private,11.2112,376003932.2,,Magic,H100,"Magic-G4 supercomputer, which will be based on Nvidia H100 Tensor Core graphics processor unit (GPU) chips. Magic says it has 8,000 of the hugely powerful chips",Planned,,11.84880979,,,,,,,8000,NVIDIA,,TRUE,,,18.89846189,1.58E+19,1.58E+19,7.92E+18,3.9576E+18,11.2112,,7.06008E+11,376003932.2,,,,,,,,https://archive.ph/DLF9r,https://archive.ph/PZl4u,,,
Tesla Dojo 1 Planned Phase 2,Planned,Likely,Yes,19.19953578,8000,Tesla D1 Dojo,,United States of America,Tesla,,"""roughly 8k H100-equivalent"" I interpreted this as ",Private,,,"San Jose, California",Tesla,Dojo,And Dojo 1 will have roughly 8k H100-equivalent of training online by end of year,Planned,"Source is one Musk Tweet. Uncertain on the exact number of chips, because Musk only specifies H100 equivalents",,Tesla Dojo 1 Phase 1,,,,,,,Tesla,,,,,18.9048237,1.58E+19,1.58E+19,8.03E+18,,,,,,,,,,,,,https://archive.ph/pM3CE,https://web.archive.org/web/20240903213050/https://techcrunch.com/2024/08/10/teslas-dojo-a-timeline/,,,
Anonymized Chinese System,Existing,Confirmed,Yes,19.30103,8000,,,China,,3/15/2024,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,TRUE,,7,19.30103,2.00E+19,,2.00E+19,,,,,,,,,,,Microsoft Azure Eagle,0.6,,,,,
Anonymized Chinese System,Planned,Likely,Yes,19.30103,8000,,30000,China,,,,Public,40,,China,,,,,,11.69897,,,,,,,30000,Anonymized,Anonymized,,,,19.30103,2.00E+19,,2.00E+19,,40,,5E+11,,,,,,,,,,,,,
Anonymized Chinese System,Existing,Likely,Yes,19.30103,8000,,8000,China,,8/15/2024,,Private,10,300000000,China,,,,,,11.90308999,,,,,,,8000,Anonymized,Anonymized,TRUE,,15,18.90308999,2.00E+19,2.00E+19,8.00E+18,4E+18,10,,8E+11,300000000,,,,,,Meta GenAI 2024a,0.3,,,,,
Anonymized Chinese System,Existing,Likely,No,19.30103,8000,,,China,,4/15/2023,,,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,,2.00E+19,2.00E+19,,,,,,,,,,,,,,,,,,
Microsoft GPT-4 cluster,Existing,Likely,Yes,19.1931246,7882.769075,NVIDIA A100,25000,United States of America,"Microsoft,OpenAI",4/30/2022,"Training finished August 2022. Training lasted ~3 months, meaning that it was operational late May at the latest",Private,21.294,562523106,"Des Moines, Iowa",OpenAI,A100,"OpenAI’s training FLOPS for GPT-4 is ~2.15e25, on ~25,000 A100s for 90 to 100 days at about 32% to 36% MFU....",4/30/2022,"Uncertain, but seems likely located in Des Moines, Iowa",11.56383735,,,,,,,25000,NVIDIA,,TRUE,,1,18.8920946,1.56E+19,1.56E+19,7.80E+18,3.9E+18,21.294,,3.663E+11,562523106,,,,TRUE,,Microsoft GPT-4 cluster,1,https://archive.ph/qTvzU,https://archive.ph/vqdRc#selection-4109.11-4109.239,https://web.archive.org/web/20241212034522/https://www.glennklockwood.com/garden/microsoft-supercomputers,https://web.archive.org/web/20250201064042/https://semianalysis.com/2023/07/10/gpt-4-architecture-infrastructure/,
Aramco Groq Inference Cluster,Existing,Likely,Yes,19.17007828,7475.366347,GroqChip LPU v1,19725,Saudi Arabia,Saudi Aramco,1/15/2025,"This cluster is designed for only inference, not training",Private,8.49023175,674658904.3,"Dammam, Saudi Arabia","Saudi Aramco,Cloud",GroqChip,"""19,725 LPUs deployed Dammam, January 2025""",2025-01,,11.64025532,,,,,,,19725,Unknown,,TRUE,,28,18.56917486,1.48E+19,1.48E+19,3.71E+18,,8.49023175,,4.36773E+11,674658904.3,,,,,,xAI Colossus Memphis Phase 1,0.074753663,https://web.archive.org/web/20250212161800/https://www.linkedin.com/posts/sundeepm_groq-aramco-digital-build-the-activity-7294312309013848064-T1fY/,,,,
Oak Ridge NL Frontier,Existing,Confirmed,Yes,19.15875607,7282.999495,AMD Radeon Instinct MI250X,37632,United States of America,US Department of Energy,5/30/2022,"(9408 nodes)x(4 GPUs/node) = 37,632 GPUs",Public,40,620445966.2,"Oak Ridge National Laboratory
5200, 1 Bethel Valley Rd, Oak Ridge, TN 37830","US Government,Academia,Industry",AMD MI250X,"The system has 74 Olympus rack HPE cabinets, each with 128 AMD compute nodes, and a total of 9,408 AMD compute nodes....
Each Frontier compute node consists of [1x] 64-core AMD “Optimized 3rd Gen EPYC” CPU (with 2 hardware threads per physical core) with access to 512 GB of DDR4 memory. Each node also contains [4x] AMD MI250X",5/30/2022,Details released on official government website,11.55669608,,,,,,,37632,AMD,,TRUE,,2,19.15875607,1.44E+19,1.44E+19,1.44E+19,3.60138E+18,40.0667904,40,3.60326E+11,967488702.6,600000000,620445966.2,$600M,TRUE,,Microsoft GPT-4 cluster,0.923913846,https://web.archive.org/web/20240720224959/https://docs.olcf.ornl.gov/systems/frontier_user_guide.html,https://web.archive.org/web/20240823003004/https://www.ornl.gov/news/frontier-supercomputer-debuts-worlds-fastest-breaking-exascale-barrier,,,
Microsoft Sweden Staffanstorp,Planned,Likely,Yes,19.12057393,6670.035371,,6666,Sweden,Microsoft,,"The ""Reported Manual Flop/s"" column is the number of INT8 FLOPs that this cluster could do assuming it was using (20k/3) GPUs, and they were all H100s. I assume that the true FLOP/s number will be higher as I assume they will likely use some Blackwell chips. Unclear how many GPUs will be in each of the three stated data centers, so this (20k/3) is a lower bound on how large the largest cluster must be. Also, it is unclear what type of GPUs will be used (though presumable some H100s and Blackwells). ",Private,,,"Västanvägen 86, 245 42 Staffanstorp, Sweden","Cloud,Microsoft",,"Microsoft said it will place more than 20,000 graphic processing units (GPUs) at its data centers in Sandviken, Gävle, and Staffanstorp",Planned 2027,"Unclear how many GPUs will be in which location, and which type. FLOP/s count is lower bound on FLOP/s assuming that they're all H100s (and there are 20k/3 GPUs in each cluster)",,,,,,,,6666,Unknown,,,,,18.81954394,1.32E+19,1.32E+19,6.60E+18,,,,,,,,,,,,,https://web.archive.org/web/20240915193858/https://www.datacenterdynamics.com/en/news/microsoft-to-deploy-20000-gpus-in-sweden/,,,,
Microsoft Sweden Sandviken,Planned,Likely,Yes,19.12057393,6670.035371,,6666,Sweden,Microsoft,,"The ""Reported Manual Flop/s"" column is the number of INT8 FLOPs that this cluster could do assuming it was using (20k/3) GPUs, and they were all H100s. I assume that the true FLOP/s number will be higher as I assume they will likely use some Blackwell chips. Unclear how many GPUs will be in each of the three stated data centers, so this (20k/3) is a lower bound on how large the largest cluster must be. Also, it is unclear what type of GPUs will be used (though presumable some H100s and Blackwells). ",Private,,,"Gamla Tunavägen, 811 42 Sandviken, Sweden","Cloud,Microsoft",,"Microsoft said it will place more than 20,000 graphic processing units (GPUs) at its data centers in Sandviken, Gävle, and Staffanstorp",Planned 2027,"Unclear how many GPUs will be in which location, and which type. FLOP/s count is lower bound on FLOP/s assuming that they're all H100s (and there are 20k/3 GPUs in each cluster)",,,,,,,,6666,Unknown,,,,,18.81954394,1.32E+19,1.32E+19,6.60E+18,,,,,,,,,,,,,https://web.archive.org/web/20240915193858/https://www.datacenterdynamics.com/en/news/microsoft-to-deploy-20000-gpus-in-sweden/,,,,
Microsoft Sweden Gävle,Planned,Likely,Yes,19.12057393,6670.035371,,6666,Sweden,Microsoft,,"The ""Reported Manual Flop/s"" column is the number of INT8 FLOPs that this cluster could do assuming it was using (20k/3) GPUs, and they were all H100s. I assume that the true FLOP/s number will be higher as I assume they will likely use some Blackwell chips. Unclear how many GPUs will be in each of the three stated data centers, so this (20k/3) is a lower bound on how large the largest cluster must be. Also, it is unclear what type of GPUs will be used (though presumable some H100s and Blackwells). ",Private,,,"805 91 Gävle, Sweden","Cloud,Microsoft",,"Microsoft said it will place more than 20,000 graphic processing units (GPUs) at its data centers in Sandviken, Gävle, and Staffanstorp",Planned 2027,"Unclear how many GPUs will be in which location, and which type. FLOP/s count is lower bound on FLOP/s assuming that they're all H100s (and there are 20k/3 GPUs in each cluster)",,,,,,,,6666,Unknown,,,,,18.81954394,1.32E+19,1.32E+19,6.60E+18,,,,,,,,,,,,,https://web.archive.org/web/20240915193858/https://www.datacenterdynamics.com/en/news/microsoft-to-deploy-20000-gpus-in-sweden/,,,,
Alps Supercomputer Phase 1,Existing,Confirmed,Yes,19.10262577,6400,NVIDIA GH200,6400,Switzerland,ETH Domain,6/1/2024,,Public,8.6825,293225224.7,"Swiss National Supercomputing Centre
Via Trevano 131, 6900 Lugano, Switzerland",Swiss Universities,GH200,Calculated from Top500,6/1/2024,,11.86295098,,Alps Supercomputer Phase 2,,,,,6400,NVIDIA,,TRUE,,15,18.80159577,1.27E+19,1.27E+19,6.33E+18,3.1648E+18,9.132032,8.6825,7.29375E+11,293225224.7,,,,,,Meta GenAI 2024a,0.260416667,https://web.archive.org/web/20250215163401/https://www.top500.org/system/180259/,,,,
AWS EC2 Trn1,Existing,Likely,Unclear,19.10037055,6366.851946,Amazon Trainium1,30000,United States of America,Amazon,10/11/2022,,Private,,,,Cloud,AWS Trainium,"customers will be able to scale the training of machine learning models with up to 30,000 Trainium accelerators interconnected with EFA petabit-scale networking",10/11/2022,,,,,,,,,30000,Amazon AWS,,,,,18.75587486,1.26E+19,1.26E+19,5.70E+18,5.7E+18,,,,,,,,,,,,https://web.archive.org/web/20240531005626/https://press.aboutamazon.com/2022/10/aws-announces-general-availability-of-amazon-ec2-trn1-instances-powered-by-aws-designed-trainium-chips,,,,
Paper on Movie Gen,Existing,Confirmed,Yes,19.084897,6144,NVIDIA H100 SXM5 80GB,6144,,Meta AI,10/4/2024,,Private,8.76675072,233622579,,Meta,H100,"""We trained the media generation models using up to 6,144 H100 GPUs, each running at 700W TDP and with 80GB HBM3,""",10/4/2024,,11.84098446,,,TRUE,Meta GenAI 2024a,,,6144,NVIDIA,,,,24,18.78382311,1.22E+19,1.22E+19,6.08E+18,3.03944E+18,8.76675072,,6.93401E+11,233622579,,,,,,xAI Colossus Memphis Phase 1,0.06144,https://web.archive.org/web/20250211214423/https://ai.meta.com/static-resource/movie-gen-research-paper,,,,
AIST ABCI 3.0,Existing,Confirmed,Yes,19.08376455,6128,NVIDIA H200 SXM,6128,Japan,National Institute of Advanced Industrial Science and Technology (AIST),1/20/2025,,Public,8.5877792,235596799.7,"Kashiwa Campus, The University of Tokyo
Japan, 〒277-0882 Chiba, Kashiwa, Kashiwanoha, 5 Chome−１−5","industry,academia,and government in Japan",H200,ABCI 3.0 consists of computing servers equipped with 6128 of the NVIDIA H200 GPUs,1/20/2025,,11.84885369,AIST ABCI 2.0,,,,,,6128,NVIDIA,,TRUE,,32,18.78273456,1.21E+19,1.21E+19,6.06E+18,3.0303E+18,8.5877792,,7.0608E+11,218948448.5,233159040,235596799.7,36B yen,,,xAI Colossus Memphis Phase 1,0.06128,https://web.archive.org/web/20241115024655/https://arxiv.org/html/2411.09134v1,https://web.archive.org/web/20250211075424/https://www.asahi.com/ajw/articles/15593572,,,
Anonymized Chinese System,Existing,Likely,Yes,19,6000,,,China,,8/15/2022,,Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,TRUE,,3,19,1.00E+19,,1.00E+19,,,,,,,,,,,Microsoft GPT-4 cluster,0.8,,,,,
Anonymized Chinese System,Planned,Likely,Yes,19,6000,,,China,,,,Public,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,18.77815125,1.00E+19,1.00E+19,6.00E+18,,,,,,,,,,,,,,,,,
University of Bristol Isambard-AI,Existing,Confirmed,Yes,19.03268289,5448,NVIDIA GH200,5448,United Kingdom of Great Britain and Northern Ireland,UK Research and Innovation,6/15/2025,,Public,7.6348272,275854311,"National Composites Centre
Bristol & Bath Science Park, Emersons Green, Bristol BS16 7FS, United Kingdom",U.K. government’s Frontier AI Taskforce,GH200,"When fully installed next year, it will pack 5,448 NVIDIA GH200 Grace Hopper Superchips",2025-06,In Top500,11.84885369,,,,,,,5448,NVIDIA,,TRUE,,,18.7316529,1.08E+19,1.08E+19,5.39E+18,2.69404E+18,7.6348272,,7.0608E+11,249791698.2,273000000,275854311,The UK government says it will cough up £225 million ($273 million),,,,,https://web.archive.org/web/20240526090503/https://blogs.nvidia.com/blog/uk-largest-ai-supercomputer/,https://web.archive.org/web/20240525234841/https://www.hpcwire.com/off-the-wire/uks-isambard-ai-supercomputer-goes-online-ranking-2nd-greenest-globally/,https://www.theregister.com/2023/11/01/uk_isambard_supercomputer/,,
KDDI Sharp Sakai,Planned,Likely,Yes,19,5053.0571,NVIDIA GB200,2000,Japan,KDDI,,"Unclear whether ""1,000 units"" means 1k GPUs, 1k chips, or 1k racks of 72 GPUs",Private,4.8048,,"１番地 Takumicho, Sakai Ward, Sakai, Osaka 590-0908, Japan",Cloud,GB200,"It is assumed that NVIDIA shall supply an estimated quantity of 1,000 units of GB200 NVL72",Planned Q1 2026,Very uncertain about GPU number,12.01729469,,,,,,,2000,NVIDIA,,,,,18.69897,1.00E+19,1.00E+19,5.00E+18,2.5E+18,4.8048,,1.04063E+12,,,,,,,,,https://web.archive.org/web/20240917144105/https://newsroom.kddi.com/english/news/detail/kddi_nr_s-9_3387.html,,,,
Meta Research SuperCluster (RSC-1) Phase 2,Existing,Confirmed,Yes,18.99930457,5044.972208,NVIDIA A100,16000,United States of America,Meta AI,5/18/2023,"The location is found from triangulating from the promo video. I'm fairly confident in it, but it isn't guaranteed to be accurate",Private,13.27872,357306992.7,"6200 Technology Blvd, 23150 Sandston, Virginia, USA",Meta,A100,"The full system includes 2,000 DGX A100 systems, totaling a staggering 16,000 A100 GPUs",5/18/2023,,11.57511836,Meta Research SuperCluster (RSC-1) Phase 1,,,,,,16000,NVIDIA,,TRUE,,4,18.69827458,9.98E+18,9.98E+18,4.99E+18,2.496E+18,13.27872,,3.7594E+11,357306992.7,,,,,,Microsoft GPT-4 cluster,0.64,https://web.archive.org/web/20231215015022/http://www.hpcwire.com/2023/05/18/meta-completes-research-supercluster-announces-next-gen-datacenter/,https://web.archive.org/web/20240828114058/https://ai.meta.com/blog/ai-rsc/,,,
Inflection-2 training cluster,Existing,Likely,Yes,18.9954158,5000,NVIDIA H100 SXM5 80GB,5000,United States of America,"Inflection AI,CoreWeave",7/30/2023,"Inflection lent 3,500 H100 GPUs to Corweave (who appears to be the party that built that cluster) for the June MLPerf submission. Given that this is just slightly larger and operational just after, it appears very likely that this cluster is an extension of the MLPerf cluster (or the MLPerf cluster was just a subset of this cluster).
It's possible that both of these subsets of the Eos-DFW cluster, but there isn't much evidence for this besides that Eos-DFW was submitted to MLPerf by CoreWeave later that year (I give it a ~50% chance this is Eos-DFW)",Private,7.2618,179900873.3,,Inflection,H100,"Inflection-2 was trained on 5,000 NVIDIA H100 GPUs",7/30/2023,,11.83329763,NVIDIA Coreweave MLPerf v3.0 Submission 2023,,TRUE,NVIDIA CoreWeave Eos-DFW Phase 1,,,5000,NVIDIA,,,,5,18.69434191,9.90E+18,9.90E+18,4.95E+18,2.4735E+18,7.2618,,6.81236E+11,179900873.3,,,,,,Microsoft GPT-4 cluster,0.634294872,https://web.archive.org/web/20241203070436/https://inflection.ai/blog/inflection-2,https://archive.ph/fB8Io,,,
Sustainable Metal Cloud Singapore Phase 2,Planned,Likely,Unclear,18.9954158,5000,NVIDIA H100 SXM5 80GB,5000,Singapore,SMC - Sustainable Metal Cloud,,,Private,7.007,235002457.7,Singapore,Cloud,H100,"Their ambitious projection is to have 5,000 GPUs operational in Singapore by the end of the year",Planned H2 2024,,11.84880979,Sustainable Metal Cloud Singapore Phase 1,,,,,,5000,NVIDIA,,,,,18.69434191,9.90E+18,9.90E+18,4.95E+18,2.4735E+18,7.007,,7.06008E+11,235002457.7,,,,,,,,https://web.archive.org/web/20250116210511/https://itbrief.asia/story/firmus-sustainable-metal-cloud-powerfully-cuts-ai-energy-usage,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,18.95424251,5000,,40000,China,,6/15/2024,,Public/Private,,,China,,,,,,,,,,,,,40000,Anonymized,Anonymized,TRUE,,20,18.95424251,9.00E+18,,9.00E+18,,,,,,,,,,,Meta GenAI 2024a,0.2,,,,,
Anonymized Chinese System,Existing,Likely,Yes,19,5000,,,China,,2/15/2025,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,TRUE,,36,19,1.00E+19,,1.00E+19,,,,,,,,,,,xAI Colossus Memphis Phase 2,0.03,,,,,
Nebius ISEG2,Existing,Confirmed,Yes,18.99472037,4992,NVIDIA H200 SXM,4992,Iceland,Nebius AI,6/15/2025,,Private,6.9957888,,Netherlands,Cloud,H200,Calculated from Top500,2025-06,,11.84885369,,,,,,,4992,NVIDIA,,TRUE,,,18.69369038,9.88E+18,9.88E+18,4.94E+18,2.46854E+18,6.9957888,,7.0608E+11,,,,,,,,,https://archive.ph/ZFjf3,,,,
NVIDIA Eos Phase 2,Existing,Confirmed,Yes,18.95995826,4608,NVIDIA H100 SXM5 80GB,4608,United States of America,NVIDIA,11/1/2023,There's also a 10.7k cluster with the same name,Private,6.69247488,152516450.7,United States of America,NVIDIA,H100,"Its “Eos” supercomputer is expected to be the world’s fastest AI system after it begins operations later this year, featuring a total of 576 DGX H100 systems with 4,608 DGX H100 GPUs.",11/1/2023,Stated in NVIDIA blog post,11.83329763,NVIDIA Eos Phase 1,,,,,,4608,NVIDIA,,TRUE,,8,18.65888438,9.12E+18,9.12E+18,4.56E+18,2.27958E+18,6.69247488,,6.81236E+11,152516450.7,,,,,,Tesla 10k H100 Cluster,0.4608,https://nvidianews.nvidia.com/news/nvidia-announces-dgx-h100-systems-worlds-most-advanced-enterprise-ai-infrastructure,https://web.archive.org/web/20240901052405/https://www.nextplatform.com/2024/03/06/a-tale-of-two-nvidia-eos-supercomputers/,https://web.archive.org/web/20240901052405/https://www.nextplatform.com/2024/03/06/a-tale-of-two-nvidia-eos-supercomputers/,,
Lawrence Livermore NL Tuolumne,Existing,Confirmed,Yes,18.95603435,4566.553613,AMD Instinct MI300A,4608,United States of America,US Department of Energy,11/18/2024,,Public,5.6612,188962954.4,"Lawrence Livermore National Laboratory
7000 East Ave, Livermore, CA 94550","Lawrence Livermore NL,Scientific Research",AMD MI300A,Calculated from Top500,11/18/2024,,11.90209586,,,,,,,4608,AMD,,TRUE,,35,18.65500436,9.04E+18,9.04E+18,4.52E+18,2.2593E+18,7.138639872,5.6612,7.98171E+11,188962954.4,,,,,,xAI Colossus Memphis Phase 1,0.045665536,https://web.archive.org/web/20241109063437/https://www.top500.org/system/180282/,,,,
Google Oklahoma TPU v4 Pods,Existing,Confirmed,Yes,18.95478263,4553.410814,Google TPU v4,32768,United States of America,Google,5/12/2022,"8 pods x (4096 chips/pod) = 32,768 chips",Private,11.86195046,299820914.1,"4581 Webb St, Pryor, OK 74361",Google,TPUv4,"Google’s fourth iteration of its Tensor Processing Units launched at last year’s I/O and a single TPU pod consists of 4,096 of these chips. Each chip has a peak performance of 275 teraflops and each pod promises a combined compute power of up to 1.1 exaflops of compute power. Google now operates a full cluster of eight of these pods in its Oklahoma data center with up to 9 exaflops of peak aggregate performance",5/12/2022,,11.88062652,,,,,,,32768,Google,,TRUE,,2,18.95478263,9.01E+18,9.01E+18,9.01E+18,,11.86195046,,7.59673E+11,299820914.1,,,,,,Microsoft GPT-4 cluster,0.577641026,https://web.archive.org/web/20240516184309/https://techcrunch.com/2022/05/11/google-launches-a-9-exaflop-cluster-of-cloud-tpu-v4-pods-into-public-preview/,,,,
Mare Nostrum 5,Existing,Confirmed,Yes,18.94772381,4480,NVIDIA H100 SXM5 80GB,4480,Spain,"Barcelona Supercomputing Center,EuroHPC JU",11/1/2023,,Public,6,162157070.2,"Barcelona Supercomputing Center
Plaça d'Eusebi Güell, 1-3, Les Corts, 08034 Barcelona, Spain",European science and industry communities,H100,1120 nodes with 4 Hopper GPUs each,11/1/2023,Specs on their official website,11.86849867,,,,,,,4480,NVIDIA,,TRUE,,10,18.64664992,8.87E+18,8.87E+18,4.43E+18,2.21626E+18,6.5065728,6,7.38752E+11,148279882.7,159400000,162157070.2,"The budget is set for €151.41 million, which is $159.4 million with current US dollar exchange rates to the euro.",,,Tesla 10k H100 Cluster,0.448,https://web.archive.org/web/20240710184832/https://www.bsc.es/es/marenostrum/marenostrum-5,https://web.archive.org/web/20240421214717/https://www.hpcwire.com/off-the-wire/bsc-inaugurates-marenostrum-5-marking-a-historic-investment-in-spanish-and-european-science/,https://www.nextplatform.com/2022/06/16/atos-wins-marenostrum-5-deal-at-barcelona-supercomputing-center/,,
Together AI H100 Cluster,Existing,Likely,Yes,18.94226091,4424,NVIDIA H100 SXM5 80GB,4424,United States of America,Together,10/16/2023,Unclear if all the GPUs are networked together,Private,6.42524064,146721675.8,,Cloud,H100,Our first large H100 cluster (4424 GPUs) starts coming online today!,10/16/2023,,11.83329763,,,,,,,4424,NVIDIA,,TRUE,,9,18.64118702,8.76E+18,8.76E+18,4.38E+18,2.18855E+18,6.42524064,,6.81236E+11,146721675.8,,,,,,Tesla 10k H100 Cluster,0.4424,https://archive.ph/58l2A#selection-489.0-489.68,,,,
EuroHPC Leonardo,Existing,Confirmed,Yes,18.93581831,4358.855988,NVIDIA A100,13824,Italy,EuroHPC JU,11/22/2022,,Public,12.5985,280971647.2,"Via Piero Gobetti, 101, 40129 Bologna BO, Italy","European researchers,academia,and industry",A100,"This result is achieved with 3456 computing nodes, each equipped with four NVIDIA  A100 SXM6 64GB  GPUs driven by a single 32-cores Intel Ice Lake CPU.",11/22/2022,,11.53446948,,,,,,,13824,NVIDIA,,TRUE,,5,18.63478832,8.63E+18,8.63E+18,4.31E+18,2.15654E+18,11.77473024,12.5985,3.42349E+11,311419540.4,272208000,280971647.2,Cost 240M Euros in 2022,,,Microsoft GPT-4 cluster,0.55296,https://web.archive.org/web/20240830104624/https://www.theregister.com/2022/11/22/leonardo_supercomputer_goes_live/,https://web.archive.org/web/20230720164245/https://en.wikipedia.org/wiki/Leonardo_(supercomputer),https://web.archive.org/web/20250115051101/https://leonardo-supercomputer.cineca.eu/hpc-system/,,
Amazon Titan training cluster,Existing,Likely,Yes,18.93380302,4338.676099,NVIDIA A100,13760,United States of America,Amazon,1/15/2023,"Model was released to large clients April 2023, took 48 days to train, so this cluster was operational at least three months beforehand",Private,11.4196992,309575729.9,,Amazon,A100,"Specifically, they trained a 200B dense model on 4T tokens of data across 13,760 NVIDIA A100 chips (using 1,720 P4d nodes)",2023-01,,11.57511836,,,,,,,13760,NVIDIA,,TRUE,,6,18.63277303,8.59E+18,8.59E+18,4.29E+18,2.14656E+18,11.4196992,,3.7594E+11,309575729.9,,,,,,Microsoft GPT-4 cluster,0.5504,https://web.archive.org/web/20240824020111/https://lifearchitect.ai/titan/,,,,
Google Hypercomputer TPU v5p pod,Existing,Confirmed,Yes,18.91515069,4156.28095,Google TPU v5p,8960,,Google,12/6/2023,"The ""Hypercomputer"" is a cloud network, a TPU v5p pod seems to just be one component. Versions of this pod likely exist in multiple locations, possibly around the world",Private,,57598146.18,,"Google,Cloud",TPUv5p,"Each TPU v5p pod composes together 8,960 chips over our highest-bandwidth inter-chip interconnect (ICI) at 4,800 Gbps/chip in a 3D torus topology.",12/6/2023,,,,,,,,,8960,Google,,TRUE,,18,18.6141207,8.23E+18,8.23E+18,4.11E+18,,,,,57598146.18,,,,,,Microsoft Azure Eagle,0.288630622,https://web.archive.org/web/20240917131707/https://cloud.google.com/blog/products/ai-machine-learning/introducing-cloud-tpu-v5p-and-ai-hypercomputer,https://web.archive.org/web/20240528131246/https://www.hpcwire.com/2023/12/28/google-addresses-the-mysteries-of-its-hypercomputer/,,,
Yotta Shakti Cloud NM1 Phase 1,Existing,Confirmed,Yes,18.90880574,4096,NVIDIA H100 SXM5 80GB,4096,India,Yotta Data Services,5/15/2024,There is evidence on GPU listings of this portion being operational. The date comes from an article a month earlier saying when they expect to officially go live,Private,5.84450048,145703820.8,"Yotta Datacenter Park - Panvel Hiranandani Fortune City. Survey No. 30, MH SH 76, Panvel, Navi Mumbai, Maharashtra 410206, India",Cloud,H100,"""Yotta’s first slot of more than 4,000 GPUs was already booked by enterprises and is expected to go live by May 15, 2024""
""Yotta will deploy the first cluster of 16,384 GPUs at NM1""",5/15/2024,,11.84098446,,,,,,,4096,NVIDIA,,TRUE,,25,18.60773185,8.11E+18,8.11E+18,4.05E+18,2.02629E+18,5.84450048,,6.93401E+11,145703820.8,,,,,,Meta GenAI 2024a,0.166666667,https://web.archive.org/web/20240527112515/https://shakticloud.ai/,https://web.archive.org/web/20240423043934/https://yotta.com/media/press-release-yotta-receives-indias-first-ever-consignment-of-nvidia-h100-the-fastest-gpus-in-the-world-at-its-nm1-data-center-park/,https://web.archive.org/web/20240521205830/https://yotta.com/media/press-release-yotta-data-services-collaborates-with-nvidia-to-catalyze-indias-ai-transformation/,https://web.archive.org/web/20240407130017/https://www.business-standard.com/companies/news/data-centre-operator-yotta-looks-to-expand-its-gpu-capacity-march-2025-124040700389_1.html,
Sesterce Synapse Phase 2,Existing,Likely,Yes,18.90880574,4096,NVIDIA H200 SXM,4096,France,Sesterce,1/15/2025,,Private,5.7401344,144832469.3,"Paris, France",Cloud,H200,Private Correspondence,1/15/2025,,11.84885369,Sesterce Synapse Phase 1,,,,,,4096,NVIDIA,,TRUE,,44,18.60777575,8.11E+18,8.11E+18,4.05E+18,2.02547E+18,5.7401344,,7.0608E+11,144832469.3,,,,,,xAI Colossus Memphis Phase 1,0.04096,https://web.archive.org/web/20240823055517/https://www.sesterce.com/reserved-cloud,https://archive.ph/ZDjLF,,,
Sesterce H100s Phase 2,Existing,Likely,Yes,18.90880574,4096,NVIDIA H100 SXM5 80GB,4096,France,Sesterce,7/6/2024,,Private,5.84450048,145966387.4,"Paris, France",Cloud,H100,From a private correspondence,7/6/2024,,11.84098446,Sesterce H100s Phase 1,,,,,,4096,NVIDIA,,TRUE,,29,18.60773185,8.11E+18,8.11E+18,4.05E+18,2.02629E+18,5.84450048,,6.93401E+11,145966387.4,,,,,,Meta GenAI 2024a,0.166666667,,,,,
PanaAI AUS AISF,Planned,Likely,Yes,18.90795668,4088,NVIDIA H200 SXM,4088,Australia,PanaAI,,,Private,5.7289232,146060910.2,Australia,Cloud,H200,It will host up to 4088 NVIDIA H200 Tensor Core GPUs,Planned Q1 2025,,11.84885369,,,,,,,4088,NVIDIA,,,,,18.60692669,8.09E+18,8.09E+18,4.05E+18,2.02152E+18,5.7289232,,7.0608E+11,146060910.2,,,,,,,,https://web.archive.org/web/20250116143118/https://www.technologydecisions.com.au/content/it-management/news/panaai-to-build-australia-s-fastest-ai-supercomputer-731317630,,,,
Voltage Park Virginia,Planned,Likely,Yes,18.90795668,4088,NVIDIA H100 SXM5 80GB,4088,United States of America,Voltage Park,,"They offer configurations up to 4,088 GPUs, which suggests that their largest cluster is 4k, therefore all clusters would be roughly of size 4k",Private,5.7289232,192138009.4,Virginia,Cloud,H100,"With 24,000 H100 GPUs spread across six geographically distinct data centers",Planned,"Unclear if the 24k chips are divided equally among the six locations. They offer clusters up to size 4,088 GPUs, though",11.84880979,,,,,,,4088,NVIDIA,,,,,18.60688279,8.09E+18,8.09E+18,4.04E+18,2.02233E+18,5.7289232,,7.06008E+11,192138009.4,,,,,,,,https://web.archive.org/web/20240913183724/https://finance.yahoo.com/news/penguin-solutions-selected-managed-services-150000685.html?guccounter=1&guce_referrer=aHR0cHM6Ly93d3cucGVycGxleGl0eS5haS8&guce_referrer_sig=AQAAACbflXKvc2DyvuR2WPik8ksxhKTAz7auQoF9nqCdOM4yDPTL4jd-W2b-V8yRSH21fxe4Ccv4cv3WjdfSFteyrcb-e_fbqoJhuhCzDkUZMwLHbCvHLBwKEOw_g_JSilSt4zqyJ6cKRYvHZ-fOSzVztIaQMrRoiWkdsftOLBO6ZB7P,https://web.archive.org/web/20240420201628/https://www.datacenterdynamics.com/en/news/ai-cloud-computing-non-profit-buys-24000-nvidia-h100-chips/,,,
Voltage Park Location 6,Planned,Likely,Yes,18.90795668,4088,NVIDIA H100 SXM5 80GB,4088,United States of America,Voltage Park,,"They offer configurations up to 4,088 GPUs, which suggests that their largest cluster is 4k, therefore all clusters would be roughly of size 4k",Private,5.7289232,192138009.4,,Cloud,H100,"With 24,000 H100 GPUs spread across six geographically distinct data centers",Planned,"Unclear if the 24k chips are divided equally among the six locations. They offer clusters up to size 4,088 GPUs, though",11.84880979,,,,,,,4088,NVIDIA,,,,,18.60688279,8.09E+18,8.09E+18,4.04E+18,2.02233E+18,5.7289232,,7.06008E+11,192138009.4,,,,,,,,https://web.archive.org/web/20240913183724/https://finance.yahoo.com/news/penguin-solutions-selected-managed-services-150000685.html?guccounter=1&guce_referrer=aHR0cHM6Ly93d3cucGVycGxleGl0eS5haS8&guce_referrer_sig=AQAAACbflXKvc2DyvuR2WPik8ksxhKTAz7auQoF9nqCdOM4yDPTL4jd-W2b-V8yRSH21fxe4Ccv4cv3WjdfSFteyrcb-e_fbqoJhuhCzDkUZMwLHbCvHLBwKEOw_g_JSilSt4zqyJ6cKRYvHZ-fOSzVztIaQMrRoiWkdsftOLBO6ZB7P,https://web.archive.org/web/20240420201628/https://www.datacenterdynamics.com/en/news/ai-cloud-computing-non-profit-buys-24000-nvidia-h100-chips/,,,
Voltage Park Texas Phase 2,Planned,Likely,Yes,18.90795668,4088,NVIDIA H100 SXM5 80GB,4088,United States of America,Voltage Park,,"They offer configurations up to 4,088 GPUs, which suggests that their largest cluster is 4k, therefore all clusters would be roughly of size 4k",Private,5.7289232,192138009.4,Texas,Cloud,H100,"With 24,000 H100 GPUs spread across six geographically distinct data centers",Planned,"Unclear if the 24k chips are divided equally among the six locations. They offer clusters up to size 4,088 GPUs, though",11.84880979,Voltage Park Texas Phase 1,,,,,,4088,NVIDIA,,,,,18.60688279,8.09E+18,8.09E+18,4.04E+18,2.02233E+18,5.7289232,,7.06008E+11,192138009.4,,,,,,,,https://web.archive.org/web/20240913183724/https://finance.yahoo.com/news/penguin-solutions-selected-managed-services-150000685.html?guccounter=1&guce_referrer=aHR0cHM6Ly93d3cucGVycGxleGl0eS5haS8&guce_referrer_sig=AQAAACbflXKvc2DyvuR2WPik8ksxhKTAz7auQoF9nqCdOM4yDPTL4jd-W2b-V8yRSH21fxe4Ccv4cv3WjdfSFteyrcb-e_fbqoJhuhCzDkUZMwLHbCvHLBwKEOw_g_JSilSt4zqyJ6cKRYvHZ-fOSzVztIaQMrRoiWkdsftOLBO6ZB7P,https://web.archive.org/web/20240420201628/https://www.datacenterdynamics.com/en/news/ai-cloud-computing-non-profit-buys-24000-nvidia-h100-chips/,,,
Voltage Park Location 5,Planned,Likely,Yes,18.90795668,4088,NVIDIA H100 SXM5 80GB,4088,United States of America,Voltage Park,,"They offer configurations up to 4,088 GPUs, which suggests that their largest cluster is 4k, therefore all clusters would be roughly of size 4k",Private,5.7289232,192138009.4,,Cloud,H100,"With 24,000 H100 GPUs spread across six geographically distinct data centers",Planned,"Unclear if the 24k chips are divided equally among the six locations. They offer clusters up to size 4,088 GPUs, though",11.84880979,,,,,,,4088,NVIDIA,,,,,18.60688279,8.09E+18,8.09E+18,4.04E+18,2.02233E+18,5.7289232,,7.06008E+11,192138009.4,,,,,,,,https://web.archive.org/web/20240913183724/https://finance.yahoo.com/news/penguin-solutions-selected-managed-services-150000685.html?guccounter=1&guce_referrer=aHR0cHM6Ly93d3cucGVycGxleGl0eS5haS8&guce_referrer_sig=AQAAACbflXKvc2DyvuR2WPik8ksxhKTAz7auQoF9nqCdOM4yDPTL4jd-W2b-V8yRSH21fxe4Ccv4cv3WjdfSFteyrcb-e_fbqoJhuhCzDkUZMwLHbCvHLBwKEOw_g_JSilSt4zqyJ6cKRYvHZ-fOSzVztIaQMrRoiWkdsftOLBO6ZB7P,https://web.archive.org/web/20240420201628/https://www.datacenterdynamics.com/en/news/ai-cloud-computing-non-profit-buys-24000-nvidia-h100-chips/,,,
Voltage Park Utah,Planned,Likely,Yes,18.90795668,4088,NVIDIA H100 SXM5 80GB,4088,United States of America,Voltage Park,,"They offer configurations up to 4,088 GPUs, which suggests that their largest cluster is 4k, therefore all clusters would be roughly of size 4k",Private,5.7289232,192138009.4,Utah,Cloud,H100,"With 24,000 H100 GPUs spread across six geographically distinct data centers",Planned,"Unclear if the 24k chips are divided equally among the six locations. They offer clusters up to size 4,088 GPUs, though",11.84880979,,,,,,,4088,NVIDIA,,,,,18.60688279,8.09E+18,8.09E+18,4.04E+18,2.02233E+18,5.7289232,,7.06008E+11,192138009.4,,,,,,,,https://web.archive.org/web/20240913183724/https://finance.yahoo.com/news/penguin-solutions-selected-managed-services-150000685.html?guccounter=1&guce_referrer=aHR0cHM6Ly93d3cucGVycGxleGl0eS5haS8&guce_referrer_sig=AQAAACbflXKvc2DyvuR2WPik8ksxhKTAz7auQoF9nqCdOM4yDPTL4jd-W2b-V8yRSH21fxe4Ccv4cv3WjdfSFteyrcb-e_fbqoJhuhCzDkUZMwLHbCvHLBwKEOw_g_JSilSt4zqyJ6cKRYvHZ-fOSzVztIaQMrRoiWkdsftOLBO6ZB7P,https://web.archive.org/web/20240420201628/https://www.datacenterdynamics.com/en/news/ai-cloud-computing-non-profit-buys-24000-nvidia-h100-chips/,,,
Voltage Park Washington,Planned,Likely,Yes,18.90795668,4088,NVIDIA H100 SXM5 80GB,4088,United States of America,Voltage Park,,"They offer configurations up to 4,088 GPUs, which suggests that their largest cluster is 4k, therefore all clusters would be roughly of size 4k",Private,5.7289232,192138009.4,"1023 39th Ave SE, Puyallup, WA 98374",Cloud,H100,"With 24,000 H100 GPUs spread across six geographically distinct data centers",Planned,"Unclear if the 24k chips are divided equally among the six locations. They offer clusters up to size 4,088 GPUs, though",11.84880979,,,,,,,4088,NVIDIA,,,,,18.60688279,8.09E+18,8.09E+18,4.04E+18,2.02233E+18,5.7289232,,7.06008E+11,192138009.4,,,,,,,,https://web.archive.org/web/20240913183724/https://finance.yahoo.com/news/penguin-solutions-selected-managed-services-150000685.html?guccounter=1&guce_referrer=aHR0cHM6Ly93d3cucGVycGxleGl0eS5haS8&guce_referrer_sig=AQAAACbflXKvc2DyvuR2WPik8ksxhKTAz7auQoF9nqCdOM4yDPTL4jd-W2b-V8yRSH21fxe4Ccv4cv3WjdfSFteyrcb-e_fbqoJhuhCzDkUZMwLHbCvHLBwKEOw_g_JSilSt4zqyJ6cKRYvHZ-fOSzVztIaQMrRoiWkdsftOLBO6ZB7P,https://web.archive.org/web/20240420201628/https://www.datacenterdynamics.com/en/news/ai-cloud-computing-non-profit-buys-24000-nvidia-h100-chips/,https://web.archive.org/web/20240226140823/https://choosetacomapierce.org/news-events/voltage-park-selects-centeris-data-center/,,
ExxonMobil Discovery 6,Existing,Confirmed,Yes,18.90196632,4032,NVIDIA GH200,4032,United States of America,ExxonMobil,6/15/2025,,Private,5.6504448,,"22777 Springwoods Village Pkwy, Spring, TX 77389",ExxonMobil,GH200,"""machine consists of 4,032 Nvidia Grace Hopper Superchips""",2025-06,,11.84885369,,,,,,,4032,NVIDIA,,TRUE,,,18.60093632,7.98E+18,7.98E+18,3.99E+18,1.99382E+18,5.6504448,,7.0608E+11,,,,,,,,,http://web.archive.org/web/20250531143036/https://www.datacenterdynamics.com/en/news/exxonmobil-announces-discovery-6-supercomputer-to-power-oil-and-gas-deposit-mapping-technology/,,,,
Telangana Yotta H1 Hyderbad AI City Cluster Phase 1,Planned,Likely,Yes,18.89850579,4000,NVIDIA H100 SXM5 80GB,4000,India,Government of Telangana,,,Public,5.6056,188001966.1,"Hyderabad, India",Cloud,H100,"The first phase of the Supercomputer consisting of 4,000 high performance GPUs and the first Data Center within the campus, Yotta H1 will be operational within the next 24 months",Planned 2026,,11.84880979,,,,,,,4000,NVIDIA,,,,,18.5974319,7.92E+18,7.92E+18,3.96E+18,1.9788E+18,5.6056,,7.06008E+11,188001966.1,,,,,,,,https://web.archive.org/web/20240906230223/https://www.crn.in/news/government-of-telangana-partners-with-yotta-to-launch-indias-largest-ai-supercomputer-of-25000-high-performance-gpus-in-a-purpose-built-50-mw-ai-cloud-data-centre-campus-in-hyderabad/,,,,
OneAsia OBON Clusters,Planned,Likely,Yes,18.89850579,4000,NVIDIA H100 SXM5 80GB,4000,Thailand,OneAsia,,,Private,5.6056,188001966.1,"Bangkok, Thailand","Cloud,OBON,SIAM AI",H100,OBON will deploy multiple 4K supercomputing clusters at OneAsia's state-of-the-art data centre,Planned,,11.84880979,,,,,,,4000,NVIDIA,,,,,18.5974319,7.92E+18,7.92E+18,3.96E+18,1.9788E+18,5.6056,,7.06008E+11,188001966.1,,,,,,,,https://web.archive.org/web/20241211224237/https://www.prnewswire.com/apac/news-releases/oneasia-and-obon-unveil-thailands-first-4k-supercomputing-clusters-revolutionizing-ai-roadmap-in-thailand-302135527.html?tc=eml_cleartime,,,,
CoreWeave LiquidLab,Existing,Likely,Yes,18.89850579,4000,NVIDIA H100 SXM5 80GB,4000,United States of America,CoreWeave,,Unclear exactly what type of GPU they use. Likely operational by now,Private,5.6056,188001966.1,,CoreWeave,H100,"CoreWeave has a Liquid Lab where we conduct extensive liquid cooling testing, and we’re getting ready to deploy our first “small” liquid deployment of 4,000 GPUs",Planned,,11.84880979,,,,,,,4000,NVIDIA,,TRUE,,,18.5974319,7.92E+18,7.92E+18,3.96E+18,1.9788E+18,5.6056,,7.06008E+11,188001966.1,,,,,,,,https://web.archive.org/web/20241006003335/https://www.coreweave.com/blog/building-ai-clusters-for-enterprises-2025,,,,
Microsoft Azure ND H100 v5 VM,Existing,Confirmed,Yes,18.89850579,4000,NVIDIA H100 SXM5 80GB,4000,United States of America,Microsoft,3/13/2023,Planned to scale up,Private,5.80944,190859527,,"Cloud,Azure",H100,"Today marks the general availability of our Azure ND H100 v5 Virtual Machine (VM) series, featuring the latest NVIDIA H100 Tensor Core GPUs and NVIDIA Quantum-2 InfiniBand networking... we are leveraging an AI optimized 4K GPU cluster and will be ramping to hundreds of thousands of the latest GPUs in the next year.",3/13/2023,,11.83329763,,,,,,,4000,NVIDIA,,TRUE,,7,18.5974319,7.92E+18,7.92E+18,3.96E+18,1.9788E+18,5.80944,,6.81236E+11,190859527,,,,,,Microsoft GPT-4 cluster,0.507435897,https://web.archive.org/web/20240817024910/https://azure.microsoft.com/en-us/blog/scale-generative-ai-with-new-azure-ai-infrastructure-advancements-and-availability/,https://web.archive.org/web/20240819113927/https://azure.microsoft.com/en-us/blog/azure-previews-powerful-and-scalable-virtual-machine-series-to-accelerate-generative-ai/,,,
Sweden 4k H100 Cluster,Existing,Likely,Yes,18.89850579,4000,NVIDIA H100 SXM5 80GB,4000,Sweden,,1/14/2025,"Operational by 1/14/25 at the latest, but likely meaningfully earlier",Private,5.6056,150626230.5,Sweden,Cloud,H100,From a private correspondence,1/14/2025,From a private correspondence about GPU cluster availability,11.84880979,,,,,,,4000,NVIDIA,,TRUE,,46,18.5974319,7.92E+18,7.92E+18,3.96E+18,1.9788E+18,5.6056,,7.06008E+11,150626230.5,,,,,,xAI Colossus Memphis Phase 1,0.04,,,,,
Mistral Large Model Implied Cluster,Existing,Unlikely,Yes,18.89850579,4000,NVIDIA H100 SXM5 80GB,4000,,,11/26/2023,"Taking H100 price as $2/hr, and assuming they trained for 3 months max, this implies that the cluster they used was >4k GPUs",Private,5.80944,132084652.7,,"Cloud,Mistral",,"""Mensch said his new model costs less than 20 million [Euros] to train""",11/26/2023,GPU count implied from stated training cost,11.83329763,,,,,,,4000,NVIDIA,,,,,18.5974319,7.92E+18,7.92E+18,3.96E+18,1.9788E+18,5.80944,,6.81236E+11,132084652.7,,,,,,,,https://archive.ph/aAkAp,,,,
Nebius Kansas City Phase 1,Existing,Likely,Yes,18.89850579,4000,NVIDIA H200 SXM,4000,United States of America,Nebius AI,4/15/2025,"Current power is 5MW our of potential expansion to 40MW (which would host 35k GPUs), so extrapolating, that would put it at ~4k GPUs
They claimed that it was on track to go live in Q1 2025 in March 2025, implying that it likely came online soon after March",Private,5,142916741.9,"Kansas City, USA",Cloud,H200,"will house thousands of state-of-the-art NVIDIA GPUs, primarily NVIDIA Hopper GPUs in the initial phase, with the energy-efficient NVIDIA Blackwell platform expected to arrive in 2025. The colocation can be expanded from an initial 5 MW up to 40 MW, or about 35 thousand GPUs, at full potential capacity",2025-04,,11.89850579,,,,,,,4000,NVIDIA,,TRUE,,52,18.59747579,7.92E+18,7.92E+18,3.96E+18,1.978E+18,5.6056,5,7.916E+11,142916741.9,,,,,,xAI Colossus Memphis Phase 2,0.02,https://web.archive.org/web/20250109005023/https://group.nebius.com/newsroom/nebius-expands-in-us-with-first-gpu-cluster-in-kansas-city-offices-in-san-francisco-dallas-and-new-york,,,,
Anonymized Chinese System,Existing,Likely,Yes,18.90308999,4000,,10000,China,,9/15/2023,,Private,10,300000000,China,,,,,,11.60205999,,,,,,,10000,Anonymized,Anonymized,TRUE,,14,18.60205999,8.00E+18,8.00E+18,4.00E+18,2E+18,10,,4E+11,300000000,,,,,,Tesla 10k H100 Cluster,0.4,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,18.84509804,4000,,,China,,8/15/2024,,Public/Private,,,China,,,,,,,,,,,,,20000,Anonymized,Anonymized,TRUE,,41,18.84509804,7.00E+18,,7.00E+18,,,,,,,,,,,CoreWeave H200s,0.08,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,18.90308999,4000,,,China,,5/15/2024,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,TRUE,,26,18.90308999,8.00E+18,,8.00E+18,,,,,,,,,,,Meta GenAI 2024a,0.2,,,,,
Anonymized Chinese System,Existing,Likely,Unclear,18.95424251,4000,,30000,China,,11/15/2024,,Private,20,600000000,China,,,,,,11.30103,,,,,,,30000,Anonymized,Anonymized,,,,18.60205999,9.00E+18,9.00E+18,4.00E+18,2E+18,20,,2E+11,600000000,,,,,,,,,,,,
Anonymized Chinese System,Planned,Likely,Yes,18.90308999,4000,,,China,,,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,18.90308999,8.00E+18,,8.00E+18,,,,,,,,,,,,,,,,,
Gemini 1.0 Ultra training cluster A,Existing,Likely,Yes,18.89679068,3984.234462,Google TPU v4,28672,United States of America,Google,8/20/2023,Believed to be trained on two sets of 7 pods of 4096 TPUs,Private,10.11307315,259857295.6,,Google,TPUv4,"Gemini Ultra, the most compute-intensive model known to date, was trained across multiple data centers totaling 55,000 TPU v4s",8/20/2023,,11.89190753,,,,,,,28672,Google,,TRUE,,10,18.89679068,7.88E+18,7.88E+18,7.88E+18,,10.11307315,,7.79664E+11,259857295.6,,,,,,Microsoft GPT-4 cluster,0.505435897,https://arxiv.org/abs/2312.11805,https://web.archive.org/web/20250320051300/https://ifp.org/web/20250320051300/https://ifp.org/future-of-ai-compute/,,,
Gemini 1.0 Ultra training cluster B,Existing,Likely,Yes,18.89679068,3984.234462,Google TPU v4,28672,United States of America,Google,8/20/2023,Believed to be trained on two sets of 7 pods of 4096 TPUs,Private,10.11307315,259857295.6,,Google,TPUv4,"Gemini Ultra, the most compute-intensive model known to date, was trained across multiple data centers totaling 55,000 TPU v4s",8/20/2023,,11.89190753,,,TRUE,Google Oklahoma TPU v4 Pods,,,28672,Google,,,,10,18.89679068,7.88E+18,7.88E+18,7.88E+18,,10.11307315,,7.79664E+11,259857295.6,,,,,,Microsoft GPT-4 cluster,0.505435897,https://arxiv.org/abs/2312.11805,https://web.archive.org/web/20250320051300/https://ifp.org/web/20250320051300/https://ifp.org/future-of-ai-compute/,,,
Vultr Chicago Cluster,Existing,Likely,Yes,18.89457634,3963.971703,AMD Instinct MI300X,3000,United States of America,Vultr,12/11/2024,"Not totally certain that the ""thousands"" of MI300X's are all in this data center, but I'm pretty sure. I'm also estimating ""thousands"" as 3k",Private,4.5864,81699126.74,"4513 Western Ave, Lisle, IL 60532",Cloud,MI300X,"With thousands of MI300X GPUs available, clusters of any size can be deployed",12/11/2024,Uncertain about exact chip count,11.93205781,,,,,,,3000,AMD,,TRUE,,49,18.59352974,7.84E+18,7.84E+18,3.92E+18,1.9611E+18,4.5864,,8.55181E+11,81699126.74,,,,,,xAI Colossus Memphis Phase 1,0.039639717,https://web.archive.org/web/20241222233226/https://blogs.vultr.com/Lisle-data-center,,,,
NFDG Andromeda Phase 2,Existing,Confirmed,Yes,18.8846232,3874.158666,NVIDIA H100 SXM5 80GB,3632,United States of America,Nat Friedman and Daniel Gross,7/15/2024,,Private,5.80862464,146020983.4,"Ashburn, Virginia",Startups they invest in,"H100,A100","3,200 H100s on 400 nodes interlinked with 3.2Tbps infiniband
432 H100s on 54 nodes interlinked with 3.2Tbps infiniband
768 A100s for training and inference with 1.6Tbps infiniband",7/15/2024,Listed as available on their official website,11.81947874,NFDG Andromeda Phase 1,,,,NVIDIA A100 SXM4 40 GB,768,4400,NVIDIA,NVIDIA,TRUE,,36,18.58355205,7.67E+18,7.67E+18,3.83E+18,1.91656E+18,5.80862464,,6.59901E+11,146020983.4,,,,,,Meta GenAI 2024a,0.15763992,https://web.archive.org/web/20240814194924/https://andromeda.ai/,https://web.archive.org/web/20240706081837/https://www.businessinsider.com/nvidia-gpu-venture-capitalists-buying-for-startups-2023-6,,,
NVIDIA Coreweave MLPerf v3.0 Submission 2023,Existing,Confirmed,Yes,18.8508138,3584,NVIDIA H100 SXM5 80GB,3584,United States of America,"NVIDIA,CoreWeave,Inflection AI",6/1/2023,"Inflection lent 3,500 H100 GPUs to Corweave (who appears to be the party that built that cluster) for the June MLPerf submission. Given that this is just slightly larger and operational just after, it appears very likely that this cluster is an extension of the MLPerf cluster (or the MLPerf cluster was just a subset of this cluster).
It's possible that both of these subsets of the Eos-DFW cluster, but there isn't evidence for this besides that Eos-DFW was submitted to MLPerf later that year",Private,5.20525824,129400760,,NVIDIA,H100,"The 10,752 H100 GPUs far surpassed the scaling in AI training in June, when NVIDIA used 3,584 Hopper GPUs.",6/1/2023,,11.83329763,,Inflection-2 training cluster,TRUE,NVIDIA CoreWeave Eos-DFW Phase 1,,,3584,NVIDIA,,,,9,18.54973991,7.09E+18,7.09E+18,3.55E+18,1.773E+18,5.20525824,,6.81236E+11,129400760,,,,,,Microsoft GPT-4 cluster,0.454662564,https://web.archive.org/web/20240915170526/https://developer.nvidia.com/blog/nvidia-sets-new-generative-ai-performance-and-scale-records-in-mlperf-training-v4-0/,https://archive.ph/fB8Io,,,
XTX Markets Cluster,Existing,Likely,Yes,18.79518459,3153.10763,NVIDIA A100,10000,,XTX Markets,3/27/2023,"Unclear how many of the GPUs are A100s vs V100s
The State of AI report indicates that they have 10k A100s, but it's unclear if they're just estimating this or have an actual source
Also, the earliest point in the waybackmaching that the State of AI report mentions this cluster is March 2023, which is where the date comes from",Private,14.5236,409025143.8,,XTX Markets,"A100,V100,20,000 GPUs Maybe 10k A100, 10k V100?",Our dedicated research cluster contains a hundred thousand cores and twenty thousand A/V100 GPUs,3/27/2023,"Unclear how many of the GPUs are A100s vs V100s
The State of AI report indicates that they have 10k A100s, but it's unclear if they're just estimating this or have an actual source",11.47840716,,,,,NVIDIA V100,10000,20000,NVIDIA,NVIDIA,TRUE,,8,18.64048144,6.24E+18,6.24E+18,4.37E+18,1.7167E+18,14.5236,,3.0089E+11,409025143.8,,,,,,Microsoft GPT-4 cluster,0.4,https://web.archive.org/web/20240222140115/https://www.xtxmarkets.com/career/xty-labs-ai-residency/,https://web.archive.org/web/20230331093712/https://www.stateof.ai/compute,,,
Novo Nordisk Gefion,Existing,Confirmed,Yes,18.77815125,3031.83426,NVIDIA H100 SXM5 80GB,1528,Denmark,"Novo Nordisk Foundation,Danish Centre for AI Innovation,Export and Investment Fund of Denmark",10/23/2024,The Export and Investment Fund of Denmark (which is government-affiliated) owns 15% of the company created by Novo Nordisk that owns this supercomputer,Private,2.9307,99155664.25,Denmark,Public and private research; cloud,H100,"The supercomputer will feature 191 DGX H100 systems with a total of 1,528 Nvidia H100 Tensor Core GPUs and 382 Intel Xeon Platinum CPUs connected by Quantum 2 InfiniBand",10/23/2024,,11.7125239,,,,,,,1528,NVIDIA,,TRUE,,49,18.17949526,6.00E+18,6.00E+18,1.51E+18,7.55902E+17,2.18027264,2.9307,5.15851E+11,58217750.86,98000000,99155664.25,"In terms of price, Novo Nordisk Foundation is putting up 600 million Danish krone (about 14 cents to the US dollar) and the Export and Investment Fund of Denmark (EIFO) us putting up another 100 million Danish krone.",,,xAI Colossus Memphis Phase 1,0.030318343,https://web.archive.org/web/20240531144416/https://www.datacenterdynamics.com/en/news/novo-nordisk-foundation-to-build-danish-nvidia-dgx-superpod-supercomputer/,https://web.archive.org/web/20250201135159/https://novonordiskfonden.dk/en/news/denmarks-first-ai-supercomputer-is-now-operational/,https://www.nextplatform.com/2024/10/23/the-great-danes-get-a-supercomputer-for-ai-and-maybe-hpc/,,
FPT AI Factory Vietnam,Planned,Likely,Yes,18.77356705,3000,NVIDIA H100 SXM5 80GB,3000,Vietnam,FPT Corporation,,"Estimating ""thousands"" as 3k here. They were supposed to receive their first shipment of 1k H100s in December 2024. The facility is said to be $200 million, so assuming an H100 price of ~30k, this means there would be a max of 6k H100s here",Private,4.2042,141001474.6,Vietnam,Cloud,"H100,H200","FPT's AI Factory, the first of its kind in Vietnam, is equipped with thousands of NVIDIA GPU H100 graphics chips and will be ready to provide services starting January 2025",Planned Q1 2025,,11.84880979,,,,,NVIDIA H200 SXM,,3000,NVIDIA,NVIDIA,,,,18.47249316,5.94E+18,5.94E+18,2.97E+18,1.4841E+18,4.2042,,7.06008E+11,141001474.6,,,,,,,,https://web.archive.org/web/20250116004357/https://fpt.com/en/news/fpt-news/can-canh-sieu-chip-nvidia-tai-nha-may-ai-cua-fpt-tai-viet-nam,,,,
FPT AI Factory Japan,Planned,Likely,Yes,18.77356705,3000,NVIDIA H100 SXM5 80GB,3000,Japan,FPT Corporation,,"Same investment size as the Vietnam cluster, which said it would have ""thousands"" of GPUs. Estimating ""thousands"" as 3k here. They were supposed to receive their first shipment of 1k H100s in December 2024. The facility is said to be $200 million, so assuming an H100 price of ~30k, this means there would be a max of 6k H100s here",Private,4.2042,141001474.6,Japan,Cloud,"H100,H200","The center, like a similar $200 million Vietnam facility announced this year, will run on Nvidia Corp",Planned Q1 2025,,11.84880979,,,,,NVIDIA H200 SXM,,3000,NVIDIA,NVIDIA,,,,18.47249316,5.94E+18,5.94E+18,2.97E+18,1.4841E+18,4.2042,,7.06008E+11,141001474.6,,,,,,,,https://archive.ph/pqult,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,18.77815125,3000,,,China,,,,Public,30,,China,,,,,,11.30103,,,,,,,10000,Anonymized,Anonymized,TRUE,,,18.77815125,6.00E+18,,6.00E+18,,,30,2E+11,,,,,,,,,,,,,
Anonymized Chinese System,Existing,Likely,Yes,18.77815125,3000,,100000,China,,3/15/2021,,Public,,,China,,,,,,,,,,,,,100000,Anonymized,Anonymized,TRUE,,1,18.77815125,6.00E+18,,6.00E+18,3E+18,,,,,,,,,,Sunway OceanLight,1,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,18.69897,3000,,,China,,1/15/2024,,Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,TRUE,,27,18.69897,5.00E+18,,5.00E+18,,,,,,,,,,,Microsoft Azure Eagle,0.2,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,18.77815125,3000,,10000,China,,7/15/2021,,Private,6,200000000,China,,,,,,11.69897,,,,,,,10000,Anonymized,Anonymized,TRUE,,1,18.47712125,6.00E+18,6.00E+18,3.00E+18,2E+18,6,,5E+11,300000000,100000000,200000000,,,,DeepSeek Fire-Flyer 2,1,,,,,
Anonymized Chinese System,Planned,Confirmed,Yes,18.69897,3000,,3000,China,,,,Public/Private,8,100000000,China,,,,,,11.79588002,,,,,,,7000,Anonymized,Anonymized,,,,18.69897,5.00E+18,,5.00E+18,1E+18,2,8,6.25E+11,100000000,,,,,,,,,,,,
Anonymized Chinese System,Existing,Likely,Yes,18.69897,3000,,10000,China,,1/15/2025,,Private,,,China,,,,,,,,,,,,,10000,Anonymized,Anonymized,TRUE,,63,,5.00E+18,5.00E+18,,,,,,,,,,,,xAI Colossus Memphis Phase 1,0.03,,,,,
Anonymized Chinese System,Existing,Confirmed,No,18.84509804,3000,,,China,,6/15/2024,,Public/Private,,,China,,,,,,,,,,,,,20000,Anonymized,Anonymized,,,,18.84509804,7.00E+18,,7.00E+18,,,,,,,,,,,,,,,,,
Paper on Gemma 2 27B,Existing,Confirmed,Yes,18.75129389,2850.021223,Google TPU v5p,6144,,Google,6/27/2024,'Table 3 | Training infrastructure with sharding' in 27B row lists 6144 in #Chips column,Private,,39221061.61,,Google,TPUv5p,"we train on an 8x24x32 configuration of
TPUv5p, totaling 6144 chips",6/27/2024,,,,,TRUE,Google Hypercomputer TPU v5p pod,,,6144,Google,,,,38,18.45026389,5.64E+18,5.64E+18,2.82E+18,,,,,39221061.61,,,,,,Meta GenAI 2024a,0.11596766,https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf,,,,
KAUST Shaheen-III,Planned,Likely,Yes,18.74607844,2816,NVIDIA GH200,2816,Saudi Arabia,King Abdullah University of Science and Technology (KAUST),,"The CPU portion became operational in November 2023. It's unclear if the GPU portion is already operational, is still under construction, or got cancelled",Public,5.3,129114064.3,"King Abdullah University Of Science And Technology 23955, Saudi Arabia",,GH200,"Across these seven accelerated cabinets: 704 nodes, each with quadruple Nvidia “Grace Hopper” Superchips, each with a tightly coupled CPU and GPU. These 2,816 Superchips

The compute-only partition, spread across 18 liquid-cooled HPE Cray EX4000 cabinets and networked with HPE Slingshot, will comprise 4,608 nodes, each with two of AMD’s fourth-gen Epyc “Genoa” CPUs",Planned,,11.72077258,,,,,,,2816,NVIDIA,,,,,18.44504845,5.57E+18,5.57E+18,2.79E+18,1.39251E+18,3.9463424,5.3,5.25742E+11,129114064.3,,,,,,,,https://web.archive.org/web/20240707221559/https://www.kaust.edu.sa/en/news/kaust-s-shaheen-iii-confirmed-as-the-middle-east-s-most-powerful-supercomputer,https://web.archive.org/web/20221128092556/https://www.hpcwire.com/2022/09/27/hpe-to-build-100-petaflops-shaheen-iii-supercomputer/,,,
CEA EXA1-HE Phase 3,Existing,Confirmed,Yes,18.72587506,2688,NVIDIA GH200,2688,France,French Alternative Energies and Atomic Energy Commission (CEA),6/15/2025,,Public,3.7669632,,,Military Applications Division of the French Alternative Energies and Atomic Energy Commission,GH200,Calculated from Top500,2025-06,,11.84885369,,,,,,,2688,NVIDIA,,TRUE,,,18.42484506,5.32E+18,5.32E+18,2.66E+18,1.32922E+18,3.7669632,,7.0608E+11,,,,,,,,,https://web.archive.org/web/20240430193812/https://www.datacenterdynamics.com/en/news/eviden-delivers-second-exa1-supercomputer-to-frances-cea/,,,,
Eni HPC6,Existing,Confirmed,Yes,18.72583848,2687.773623,AMD Radeon Instinct MI250X,13888,Italy,Eni,11/18/2024,,Private,14.1546496,349498499.4,"Via per la Corradina, 35, 27032 Ferrera Erbognone PV, Italy",Eni,AMD MI250X,"the system includes 3472 computing nodes housing a total of 13,888 GPU",11/18/2024,,11.57493936,,,,,,,13888,AMD,,TRUE,,57,18.72583848,5.32E+18,5.32E+18,5.32E+18,1.32908E+18,14.1546496,,3.75785E+11,349498499.4,,,,,,xAI Colossus Memphis Phase 1,0.026877736,https://archive.ph/RIheG,,,,
Los Alamos NL Venado,Existing,Confirmed,Yes,18.70468576,2560,NVIDIA GH200,2560,United States of America,US Department of Energy,4/16/2024,,Public,2.7775,117272446.7,"Los Alamos, New Mexico, USA",Los Alamos National Laboratory,GH200,"The system consists of 2,560 direct, liquid-cooled Nvidia Grace Hopper Superchips; 920 Nvidia Grace CPU Superchips, each of which contains 144 Arm cores",4/16/2024,,11.9600017,,,,,,,2560,NVIDIA,,TRUE,,31,18.40365576,5.07E+18,5.07E+18,2.53E+18,1.26592E+18,3.6528128,2.7775,9.12014E+11,117272446.7,,,,,,Meta GenAI 2024a,0.104166667,https://web.archive.org/web/20240921074409/https://www.datacenterdynamics.com/en/news/los-alamos-national-laboratory-unveils-new-venado-supercomputer-reportedly-capable-of-10-exaflops-of-peak-ai-performance/,,,,
Meta Research SuperCluster 2 (RSC-2),Existing,Confirmed,Yes,18.69827458,2522.486104,NVIDIA A100,8000,United States of America,Meta AI,10/31/2023,,Private,6.63936,157825162.1,,"Meta,Primarily vision applications at Meta",A100,"RSC-1 is a general ML cluster (e.g., training some of the prominent LLMs) of 16k GPU size, while RSC-2 focuses on vision applications and is of 8k GPU size.",10/31/2023,Reported in publication,11.57511836,,,,,,,8000,NVIDIA,,TRUE,,20,18.39724458,4.99E+18,4.99E+18,2.50E+18,1.248E+18,6.63936,,3.7594E+11,157825162.1,,,,,,Tesla 10k H100 Cluster,0.25224861,https://web.archive.org/web/20241108001043/https://arxiv.org/abs/2410.21680,https://arxiv.org/pdf/2410.21680v1,,,
NFDG Andromeda Phase 1,Existing,Confirmed,Yes,18.69646543,2512,NVIDIA H100 SXM5 80GB,2512,United States of America,Nat Friedman and Daniel Gross,6/13/2023,They have since expanded this,Private,3.64832832,90696068.38,"Ashburn, Virginia",Startups they invest in,H100,"2,512 H100s on 314 nodes interlinked with 3.2Tbps infiniband",6/13/2023,Listed as available on their official website,11.83329763,,NFDG Andromeda Phase 2,,,,,2512,NVIDIA,,TRUE,,13,18.39539154,4.97E+18,4.97E+18,2.49E+18,1.24269E+18,3.64832832,,6.81236E+11,90696068.38,,,,,,Microsoft GPT-4 cluster,0.318669744,https://web.archive.org/web/20230907083238/https://andromeda.ai/,https://web.archive.org/web/20240706081837/https://www.businessinsider.com/nvidia-gpu-venture-capitalists-buying-for-startups-2023-6,,,
Samsung SSC4,Existing,Confirmed,Yes,18.69369038,2496,NVIDIA H100 SXM5 80GB,2496,Korea (Republic of),Samsung,6/15/2025,,Private,3.4978944,,Korea (Republic of),Samsung,H100,Calculated from Top500,2025-06,Listed in Top500,11.84880979,,,,,,,2496,NVIDIA,,TRUE,,,18.39261649,4.94E+18,4.94E+18,2.47E+18,1.23477E+18,3.4978944,,7.06008E+11,,,,,,,,,,,,,
Tesla A100 Cluster Phase 2,Existing,Confirmed,Yes,18.6620624,2320.687216,NVIDIA A100 SXM4 80 GB,7360,United States of America,Tesla,8/16/2022,,Private,6.2689536,166279435.5,,Tesla,A100,"now has a total of 7,360 A100 GPUs",8/16/2022,,11.56383735,Tesla A100 Cluster Phase 1,,,,,,7360,NVIDIA,,TRUE,,6,18.36103241,4.59E+18,4.59E+18,2.30E+18,1.14816E+18,6.2689536,,3.663E+11,166279435.5,,,,,,Microsoft GPT-4 cluster,0.2944,https://web.archive.org/web/20230521065409/https://www.hpcwire.com/2022/08/16/tesla-gooses-its-gpu-powered-ai-super-is-dojo-next/,,,,
EuroHPC LUMI,Existing,Confirmed,Yes,18.65918346,2305.354219,AMD Radeon Instinct MI250X,11912,Finland,EuroHPC JU,9/15/2022,"Came online in phases, so unclear exactly when it became operational. A portion of it first was innagurated as operational June 2022, and close to 10k GPUs were confirmed to operational by Nov 2022. Reports suggest that ""phase two"" began in August. Marking the operational date as September as an estimate",Public,12.0922,171001750.1,"Tehdaskatu 15, 87100 Kajaani, Finland","Researchers,Cloud",AMD MI250X,"The GPU partition consists of 2978 nodes, each node with one 64-core AMD Trento CPU and four AMD MI250X GPUs. A total of 11912 AMD GPUs.",2022-09,Confirmed by official sources,11.57667814,,,,,,,11912,AMD,,TRUE,,8,18.65918346,4.56E+18,4.56E+18,4.56E+18,1.13998E+18,12.6827064,12.0922,3.77292E+11,306928476.1,165000000,171001750.1,"""The total budget of the product, which will be co-funded by the EuroHPC JU and the LUMI Consortium, will total €144.5 million (over $165 million). The total cost of ownership of the system from 2020 to 2026 will be €200 million ($237.1 million), which includes the cost of hardware.""",,,Microsoft GPT-4 cluster,0.292454872,https://web.archive.org/web/20241003125301/https://www.lumi-supercomputer.eu/lumis-full-system-architecture-revealed/,https://web.archive.org/web/20240908234416/https://en.wikipedia.org/wiki/LUMI,https://lumi-supercomputer.github.io/LUMI-training-materials/2day-20240502/01_Architecture/,https://www.tomshardware.com/news/amd-based-finnish-supercomputer-will-be-one-of-worlds-most-powerful-in-mid-2021,
Lawrence Berkeley NL NERSC Perlmutter,Existing,Confirmed,Yes,18.65058259,2260.147549,NVIDIA A100 SXM4 40 GB,6144,United States of America,US Department of Energy,5/17/2021,,Public,6.9,153237845.4,"Lawrence Berkeley National Laboratory, Building 59, 1 Cyclotron Rd, Berkeley, CA 94720",Scientific research,A100,"1536 nodes: 1x AMD EPYC 7763	4x NVIDIA A100 (40GB)
256 nodes: 1x AMD EPYC 7763	4x NVIDIA A100 (80GB)	",5/17/2021,,11.5107035,,,,,NVIDIA A100 SXM4 80 GB,1024,7168,NVIDIA,NVIDIA,TRUE,,2,18.34955259,4.47E+18,4.47E+18,2.24E+18,1.11821E+18,6.15759872,6.9,3.24118E+11,198102286,146000000,153237845.4,$146 million for machine plus multiple years of service,,,Sunway OceanLight,0.752314815,https://web.archive.org/web/20240421214604/https://www.hpcwire.com/2021/05/27/nersc-debuts-perlmutter-worlds-fastest-ai-supercomputer/,https://web.archive.org/web/20240927184852/https://docs.nersc.gov/systems/perlmutter/architecture/,https://top500.org/news/crays-next-generation-supercomputer-headed-to-berkeley-lab-in-2020/,,
Sesterce H100s Phase 1,Existing,Likely,Yes,18.60777575,2048,NVIDIA H100 SXM5 80GB,2048,,Sesterce,7/6/2024,"Unclear exactly where this cluster is located. This is likely the early phases of the Paris 4k H100 cluster, but it's unclear",Private,2.92225024,72983193.71,,Cloud,H100,,7/6/2024,,11.84098446,,Sesterce H100s Phase 2,,,,,2048,NVIDIA,,TRUE,,48,18.30670186,4.05E+18,4.05E+18,2.03E+18,1.01315E+18,2.92225024,,6.93401E+11,72983193.71,,,,,,Meta GenAI 2024a,0.083333333,https://archive.ph/ZDjLF,https://archive.ph/gJllE,,,
Northern Data Group Taiga Cloud Island 4,Existing,Likely,Yes,18.60777575,2048,NVIDIA H100 SXM5 80GB,2048,,Northern Data Group,3/30/2024,,Private,2.92225024,73105785.87,Europe,Cloud,H100,"""Taiga Cloud is building out its NVIDIA H100 GPU infrastructure into pods of 512 GPUs, connected into islands of four pods each (2,048 GPUs) using NVIDIA BlueField DPUs and the NVIDIA Quantum-2 InfiniBand platform""
""the purchase of 20 NVIDIA H100 GPU Pods – each made up of 512 H100 GPUs""",3/30/2024,,11.84098446,,,,,,,2048,NVIDIA,,TRUE,,36,18.30670186,4.05E+18,4.05E+18,2.03E+18,1.01315E+18,2.92225024,,6.93401E+11,73105785.87,,,,,,Meta GenAI 2024a,0.083333333,https://web.archive.org/web/20241122185558/https://www.eqs-news.com/news/corporate/northern-data-group-europes-largest-deployment-of-nvidia-h100-tensor-core-gpus-to-expand-taiga-clouds-generative-ai-csp-offering/1903591,,,,
AIST ABCI-Q,Existing,Confirmed,Yes,18.60777575,2048,NVIDIA H100 SXM5 80GB,2048,Japan,National Institute of Advanced Industrial Science and Technology (AIST),6/15/2025,"""more than 2,000"": estimated as 2048",Public,2.8700672,96257006.65,"AIST Tsukuba
1 Chome Higashi, Tsukuba, Ibaraki 305-0046, Japan",,H100,"The supercomputer is powered by more than 2,000 NVIDIA H100 Tensor Core GPUs in 500+ nodes",2025-06,"""more than 2,000"": estimated as 2,200",11.84880979,,,,,,,2048,NVIDIA,,TRUE,,,18.30670186,4.05E+18,4.05E+18,2.03E+18,1.01315E+18,2.8700672,,7.06008E+11,96257006.65,,,,,,,,https://web.archive.org/web/20240729195553/https://nvidianews.nvidia.com/news/nvidia-powers-japans-abci-q-supercomputer-for-quantum-research,,,,
Northern Data Group Taiga Cloud NO1 Island 2,Existing,Likely,Yes,18.60777575,2048,NVIDIA H100 SXM5 80GB,2048,Norway,Northern Data Group,3/30/2024,"There are two 2,048 H100 islands at this location",Private,2.92225024,73105785.87,"Stølevegen 39, 4715 Øvrebø, Norway",Cloud,H100,"""Taiga Cloud is building out its NVIDIA H100 GPU infrastructure into pods of 512 GPUs, connected into islands of four pods each (2,048 GPUs) using NVIDIA BlueField DPUs and the NVIDIA Quantum-2 InfiniBand platform""
""The partnership provides Taiga Cloud with additional access to 100% hydro-powered data centers, with Bulk housing 2 of Taiga Cloud’s islands of NVIDIA H100 GPUs and customer availability starting from Q1 2024.""",3/30/2024,,11.84098446,,,,,,,2048,NVIDIA,,TRUE,,36,18.30670186,4.05E+18,4.05E+18,2.03E+18,1.01315E+18,2.92225024,,6.93401E+11,73105785.87,,,,,,Meta GenAI 2024a,0.083333333,https://web.archive.org/web/20241122185558/https://www.eqs-news.com/news/corporate/northern-data-group-europes-largest-deployment-of-nvidia-h100-tensor-core-gpus-to-expand-taiga-clouds-generative-ai-csp-offering/1903591,https://web.archive.org/web/20240621132432/https://bulkinfrastructure.com/newsroom/taiga-cloud-continues-rollout-of-nvidia-h100-gpus,,,
Sesterce Synapse Phase 1,Existing,Likely,Yes,18.60777575,2048,NVIDIA H200 SXM,2048,France,Sesterce,9/30/2024,,Private,2.92225024,73123829.94,"Paris, France",Cloud,H200,2048 x H200 SXM IB 3.2 available in early-september,9/30/2024,,11.84102835,,Sesterce Synapse Phase 2,,,,,2048,NVIDIA,,TRUE,,55,18.30674575,4.05E+18,4.05E+18,2.03E+18,1.01274E+18,2.92225024,,6.93471E+11,73123829.94,,,,,,xAI Colossus Memphis Phase 1,0.02048,https://web.archive.org/web/20240823055517/https://www.sesterce.com/reserved-cloud,https://archive.ph/ZDjLF,,,
NVIDIA Israel-1 Phase 2,Existing,Likely,Yes,18.60777575,2048,NVIDIA H100 SXM5 80GB,2048,Israel,NVIDIA,11/20/2024,"Not totally sure when this became operational, but it seems like it was at least operational by November 2024",Private,2.92225024,78062110.53,Israel,NVIDIA,H100,"will feature 256 Nvidia HGX H100 systems, combining 2,048 Nvidia H100 80GB GPUs with more than 34 million CUDA cores and 1 million fourth-generation Tensor Cores, 2,560 BlueField-3 DPUs, and 80 Spectrum-4 switches",11/20/2024,"The first half of the GPUs were confirmed to be operational Nov 2023, with the second half expected to be operational in the first half of 2024",11.84098446,NVIDIA Israel-1 Phase 1,,,,,,2048,NVIDIA,,TRUE,,65,18.30670186,4.05E+18,4.05E+18,2.03E+18,1.01315E+18,2.92225024,,6.93401E+11,78062110.53,,,,,,xAI Colossus Memphis Phase 1,0.02048,https://web.archive.org/web/20240628053616/https://www.datacenterdynamics.com/en/news/nvidias-israel-1-supercomputer-starts-operations/,,,,
Northern Data Group Taiga Cloud Island 3,Existing,Likely,Yes,18.60777575,2048,NVIDIA H100 SXM5 80GB,2048,,Northern Data Group,3/30/2024,,Private,2.92225024,73105785.87,Europe,Cloud,H100,"""Taiga Cloud is building out its NVIDIA H100 GPU infrastructure into pods of 512 GPUs, connected into islands of four pods each (2,048 GPUs) using NVIDIA BlueField DPUs and the NVIDIA Quantum-2 InfiniBand platform""
""the purchase of 20 NVIDIA H100 GPU Pods – each made up of 512 H100 GPUs""",3/30/2024,,11.84098446,,,,,,,2048,NVIDIA,,TRUE,,36,18.30670186,4.05E+18,4.05E+18,2.03E+18,1.01315E+18,2.92225024,,6.93401E+11,73105785.87,,,,,,Meta GenAI 2024a,0.083333333,https://web.archive.org/web/20241122185558/https://www.eqs-news.com/news/corporate/northern-data-group-europes-largest-deployment-of-nvidia-h100-tensor-core-gpus-to-expand-taiga-clouds-generative-ai-csp-offering/1903591,,,,
Northern Data Group Taiga Cloud NO1 Island 1,Existing,Likely,Yes,18.60777575,2048,NVIDIA H100 SXM5 80GB,2048,Norway,Northern Data Group,3/30/2024,"There are two 2,048 H100 islands at this location",Private,2.92225024,73105785.87,"Stølevegen 39, 4715 Øvrebø, Norway",Cloud,H100,"""Taiga Cloud is building out its NVIDIA H100 GPU infrastructure into pods of 512 GPUs, connected into islands of four pods each (2,048 GPUs) using NVIDIA BlueField DPUs and the NVIDIA Quantum-2 InfiniBand platform""
""The partnership provides Taiga Cloud with additional access to 100% hydro-powered data centers, with Bulk housing 2 of Taiga Cloud’s islands of NVIDIA H100 GPUs and customer availability starting from Q1 2024.""",3/30/2024,,11.84098446,,,,,,,2048,NVIDIA,,TRUE,,36,18.30670186,4.05E+18,4.05E+18,2.03E+18,1.01315E+18,2.92225024,,6.93401E+11,73105785.87,,,,,,Meta GenAI 2024a,0.083333333,https://web.archive.org/web/20241122185558/https://www.eqs-news.com/news/corporate/northern-data-group-europes-largest-deployment-of-nvidia-h100-tensor-core-gpus-to-expand-taiga-clouds-generative-ai-csp-offering/1903591,https://web.archive.org/web/20240621132432/https://bulkinfrastructure.com/newsroom/taiga-cloud-continues-rollout-of-nvidia-h100-gpus,,,
Northern Data Group Njored Taiga Cloud Island 5,Existing,Likely,Yes,18.60777575,2048,NVIDIA H100 SXM5 80GB,2048,United Kingdom of Great Britain and Northern Ireland,Northern Data Group,3/30/2024,"Mentioned in 2025-06 Top500 as ""Njored"" with at least 1,952 H100s",Private,2.92225024,73105785.87,United Kingdom,Cloud,H100,"""Taiga Cloud is building out its NVIDIA H100 GPU infrastructure into pods of 512 GPUs, connected into islands of four pods each (2,048 GPUs) using NVIDIA BlueField DPUs and the NVIDIA Quantum-2 InfiniBand platform""
""the purchase of 20 NVIDIA H100 GPU Pods – each made up of 512 H100 GPUs""",3/30/2024,,11.84098446,,,,,,,2048,NVIDIA,,TRUE,,36,18.30670186,4.05E+18,4.05E+18,2.03E+18,1.01315E+18,2.92225024,,6.93401E+11,73105785.87,,,,,,Meta GenAI 2024a,0.083333333,https://web.archive.org/web/20241122185558/https://www.eqs-news.com/news/corporate/northern-data-group-europes-largest-deployment-of-nvidia-h100-tensor-core-gpus-to-expand-taiga-clouds-generative-ai-csp-offering/1903591,,,,
Horizon Compute Baobab Phase 2,Existing,Likely,Yes,18.60777575,2048,NVIDIA H100 SXM5 80GB,2048,United States of America,Horizon Compute,3/1/2025,"Unclear exactly when this was expanded to 2k H100s, but at the latest by March 1st, 2025",Private,2.8700672,69027926.85,,Cloud,H100,"256 HGX H100 nodes

(2048 GPUs)",3/1/2025,,11.84880979,Horizon Compute Baobab Phase 1,,,,,,2048,NVIDIA,,TRUE,,75,18.30670186,4.05E+18,4.05E+18,2.03E+18,1.01315E+18,2.8700672,,7.06008E+11,69027926.85,,,,,,xAI Colossus Memphis Phase 2,0.01024,https://web.archive.org/web/20241230203302/https://horizoncompute.com/,https://archive.ph/VaYyj#selection-320.0-330.0,,,
SoftBank CHIE-2,Existing,Confirmed,Yes,18.60607596,2040,NVIDIA H100 SXM5 80GB,2040,Japan,Softbank,10/31/2024,,Private,2.9108352,77725269.48,Japan,Softbank,H100,Calculated from Top500,10/31/2024,,11.84098446,,,,,,,2040,NVIDIA,,TRUE,,68,18.30500207,4.04E+18,4.04E+18,2.02E+18,1.00919E+18,2.9108352,,6.93401E+11,77725269.48,,,,,,xAI Colossus Memphis Phase 1,0.0204,https://web.archive.org/web/20241119082609/https://top500.org/system/180329/,,,,
SoftBank CHIE-3,Existing,Confirmed,Yes,18.60607596,2040,NVIDIA H100 SXM5 80GB,2040,Japan,Softbank,10/31/2024,,Private,2.9108352,77725269.48,Japan,Softbank,H100,Calculated from Top500,10/31/2024,,11.84098446,,,,,,,2040,NVIDIA,,TRUE,,68,18.30500207,4.04E+18,4.04E+18,2.02E+18,1.00919E+18,2.9108352,,6.93401E+11,77725269.48,,,,,,xAI Colossus Memphis Phase 1,0.0204,https://web.archive.org/web/20241119062610/https://top500.org/system/180330/,,,,
Sakura's H100s Phase 1,Existing,Confirmed,Yes,18.60093632,2016,NVIDIA H100 SXM5 80GB,2016,Japan,Sakura Internet,7/31/2024,"The Ministry of Economy, Trade and Industry (METI) will subsidize 6.8 billion yen ($48.2 million) of the 13.5 billion yen it will cost to build the machine",Public/Private,2.87659008,95993910.04,"Ishikari City, Hokkaido Japan",Cloud,H100,"we have completed the installation of 2,016 'NVIDIA H100 Tensor Core GPUs'.",7/31/2024,,11.84098446,,,,,,,2016,NVIDIA,,TRUE,,57,18.29986243,3.99E+18,3.99E+18,1.99E+18,9.97315E+17,2.87659008,,6.93401E+11,76577398.59,95164335,95993910.04,"The Ministry of Economy, Trade and Industry (METI) will subsidize 6.8 billion yen ($48.2 million) of the 13.5 billion yen it will cost to build the machine.",,,Meta GenAI 2024a,0.08203125,https://archive.ph/g8HTU,https://archive.ph/m4ZML#,,,
Reka H100 Rental,Existing,Confirmed,Yes,18.59747579,2000,NVIDIA H100 SXM5 80GB,2000,,Reka AI,12/15/2023,,Private,2.90472,87430027.49,,Reka,H100,"""Our setup comprises of clusters from a mixture of vendors with our peak compute being approximately
2.5K H100s and 2.5K A100s""
""Provider A: 2000 chips""",12/15/2023,,11.83329763,,,TRUE,,,,2000,NVIDIA,,,,33,18.2964019,3.96E+18,3.96E+18,1.98E+18,9.894E+17,2.90472,,6.81236E+11,87430027.49,,,,,,Microsoft Azure Eagle,0.138888889,https://web.archive.org/web/20250108153733/https://publications.reka.ai/reka-core-tech-report.pdf,,,,
Quebec 2k H100 Cluster,Existing,Likely,Yes,18.59747579,2000,NVIDIA H100 SXM5 80GB,2000,Canada,,1/14/2025,"Operational by 1/14/25 at the latest, but likely meaningfully earlier",Private,2.8028,75313115.25,"Quebec, Canada",Cloud,H100,From a private correspondence,1/14/2025,From a private correspondence about GPU cluster availability,11.84880979,,,,,,,2000,NVIDIA,,TRUE,,82,18.2964019,3.96E+18,3.96E+18,1.98E+18,9.894E+17,2.8028,,7.06008E+11,75313115.25,,,,,,xAI Colossus Memphis Phase 1,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,18.60205999,2000,,,China,,9/15/2024,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,TRUE,,61,18.60205999,4.00E+18,,4.00E+18,,,,,,,,,,,xAI Colossus Memphis Phase 1,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,18.60205999,2000,,2000,China,,9/15/2024,,Private,3,70000000,China,,,,,,11.82390874,,,TRUE,,,,2000,Anonymized,Anonymized,,,54,18.30103,4.00E+18,4.00E+18,2.00E+18,9E+17,3,,6.66667E+11,70000000,,,,,,xAI Colossus Memphis Phase 1,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,18.47712125,2000,,,China,,3/15/2024,,Public,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,TRUE,,43,18.47712125,3.00E+18,,3.00E+18,,,,,,,,,,,Meta GenAI 2024a,0.06,,,,,
Anonymized Chinese System,Existing,Likely,Yes,18.60205999,2000,,2000,China,,10/15/2024,,Private,3,70000000,China,,,,,,11.82390874,,,,,,,2000,Anonymized,Anonymized,TRUE,,69,18.30103,4.00E+18,4.00E+18,2.00E+18,9E+17,3,,6.66667E+11,70000000,,,,,,xAI Colossus Memphis Phase 1,0.02,,,,,
Anonymized Chinese System,Existing,Likely,Yes,18.60205999,2000,,,China,,4/15/2021,,Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,TRUE,,2,18.60205999,4.00E+18,,4.00E+18,,,,,,,,,,,Sunway OceanLight,0.7,,,,,
Anonymized Chinese System,Existing,Likely,No,18.47712125,2000,,,China,,8/15/2022,,Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,18.47712125,3.00E+18,,3.00E+18,,,,,,,,,,,,,,,,,
Anonymized Chinese System,Planned,Likely,Yes,18.47712125,2000,,,Hong Kong,,,,Public,,,Hong Kong,,,,,,,,,,,,,,Anonymized,Anonymized,,,,18.30103,3.00E+18,3.00E+18,2.00E+18,,,,,,,,,,,,,,,,,
Anonymized Chinese System,Existing,Likely,Yes,18.47712125,2000,,,China,,6/15/2024,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,TRUE,,59,18.47712125,3.00E+18,,3.00E+18,,,,,,,,,,,Meta GenAI 2024a,0.06,,,,,
Meta Research SuperCluster (RSC-1) Phase 1,Existing,Confirmed,Yes,18.57908817,1917.089439,NVIDIA A100,6080,United States of America,Meta AI,1/24/2022,"The location is found from triangulating from the promo video. I'm fairly confident in it, but it isn't guaranteed to be accurate",Private,5.1787008,139229565.5,"6200 Technology Blvd, 23150 Sandston, Virginia, USA",Meta,A100,"At the time, “only” the first phase of the system was operational: 760 Nvidia DGX A100 nodes, totaling 6,080 Nvidia A100 GPUs",1/24/2022,,11.56383735,,Meta Research SuperCluster (RSC-1) Phase 2,,,,,6080,NVIDIA,,TRUE,,5,18.27805817,3.79E+18,3.79E+18,1.90E+18,9.4848E+17,5.1787008,,3.663E+11,139229565.5,,,,,,DeepSeek Fire-Flyer 2,0.608,https://web.archive.org/web/20231215015022/http://www.hpcwire.com/2023/05/18/meta-completes-research-supercluster-announces-next-gen-datacenter/,https://web.archive.org/web/20240828114058/https://ai.meta.com/blog/ai-rsc/,,,
CEA EXA1-HE Phase 2,Existing,Confirmed,Yes,18.57702416,1908,NVIDIA GH200,1908,France,French Alternative Energies and Atomic Energy Commission (CEA),4/17/2024,Phase 1 was from 2021 and didn't contain any GPUs,Public,2.0177,87101089.19,,Military Applications Division of the French Alternative Energies and Atomic Energy Commission,GH200,Calculated from Top500,4/17/2024,,11.97113757,,,,,,,1908,NVIDIA,,TRUE,,46,18.27599417,3.78E+18,3.78E+18,1.89E+18,9.43506E+17,2.72248704,2.0177,9.35702E+11,87101089.19,,,,,,Meta GenAI 2024a,0.077636719,https://web.archive.org/web/20240430193812/https://www.datacenterdynamics.com/en/news/eviden-delivers-second-exa1-supercomputer-to-frances-cea/,,,,
Tesla A100 Cluster Phase 1,Existing,Confirmed,Yes,18.55560707,1816.189995,NVIDIA A100 SXM4 80 GB,5760,United States of America,Tesla,6/20/2021,,Private,4.9480704,180974131.3,,Tesla,A100,"for a whopping 5,760 A100s throughout the system",6/20/2021,,11.56014121,,Tesla A100 Cluster Phase 2,,,,,5760,NVIDIA,,TRUE,,4,18.25457708,3.59E+18,3.59E+18,1.80E+18,8.9856E+17,4.9480704,,3.63196E+11,180974131.3,,,,,,Sunway OceanLight,0.60453869,https://web.archive.org/web/20230322111214/https://www.hpcwire.com/2021/06/22/ahead-of-dojo-tesla-reveals-its-massive-precursor-supercomputer/,,,,
Gcore data center Phase 2,Planned,Likely,Yes,18.5427632,1763.264275,NVIDIA H100 SXM5 80GB,500,Korea (Republic of),"Gcore,NHN Corporation",,,Private,1.9019,23500245.77,"Incheon, South Korea",Cloud,"H100,H200,GB200,>1,000 GPUs, including H200s and GB200s","Gcore plans to expand this to more than 1,000 GPUs by the end of the year and will include H200s and GB200s.",Planned Q4 2024,,11.96253308,Gcore data center Phase 1,,,,NVIDIA GB200,500,1000,NVIDIA,NVIDIA,,,,18.24172076,3.49E+18,3.49E+18,1.74E+18,8.7235E+17,1.9019,,9.17346E+11,23500245.77,,,,,,,,https://web.archive.org/web/20240723133814/https://www.datacenterdynamics.com/en/news/gcore-expands-into-south-korea-with-nhn-cloud/,,,,
Oak Ridge NL Summit,Decommissioned,Confirmed,Yes,18.53857373,1746.336534,NVIDIA Tesla V100 SXM2,27648,United States of America,US Department of Energy,6/8/2018,,Public,13,218653393.7,"Oak Ridge National Laboratory
5200, 1 Bethel Valley Rd, Oak Ridge, TN 37830",US civilian research,V100,"""The IBM AC922 system consists of 4,608 compute servers, each containing two 22-core IBM Power9 processors and six NVIDIA Tesla V100 graphics processing unit accelerators""",6/8/2018,,11.42463038,,,,,,,27648,NVIDIA,,,,1,18.53857373,3.46E+18,,3.46E+18,4.33244E+17,19.02071808,13,2.65846E+11,596012918.3,200000000,218653393.7,we estimate that Sierra represented $125 million of the $325 million of the CORAL procurement based on a flat flops cost between the two machines,TRUE,11/15/2024,Oak Ridge NL Summit,1,https://web.archive.org/web/20240904145755/https://www.ornl.gov/news/ornl-launches-summit-supercomputer,https://web.archive.org/https://www.olcf.ornl.gov/olcf-resources/compute-systems/summit/summit-faqs,https://www.nextplatform.com/2017/10/05/clever-machinations-livermores-sierra-supercomputer/,,
Microsoft Azure Meta AI Rental,Existing,Confirmed,Yes,18.52757835,1702.67812,NVIDIA A100,5400,United States of America,Microsoft,5/27/2022,"Not marked ""possible duplicate"" since we don't have any other Microsoft supercomputers existing at this time that are this large and aren't known to be used by anyone else (eg, the 25k A100 supercomputer used by OpenAI)",Private,4.599504,121870051.9,,Meta,A100,"Meta AI did reveal that the company plans to utilize next-gen machine learning workloads on a reserved cluster on Microsoft Azure that would enlist 5,400 A100 GPUs from NVIDIA.",5/27/2022,,11.56383735,,,,,,,5400,NVIDIA,,TRUE,,10,18.22654835,3.37E+18,3.37E+18,1.68E+18,8.424E+17,4.599504,,3.663E+11,121870051.9,,,,,,Microsoft GPT-4 cluster,0.216,https://web.archive.org/web/20220630142849/https://wccftech.com/microsoft-azure-upgraded-amd-instinct-mi200-gpu-clusters-for-large-scale-ai-training/,,,,
Argonne NL Aurora,Existing,Confirmed,Yes,18.52401917,1688.781162,Intel Data Center GPU Max 1550,63744,United States of America,US Department of Energy,5/13/2024,,Public,60,503027173.8,"Theory and Computing Sciences Building
Building 240, Argonne National Laboratory, 9700 S. Cass Ave, Argonne Ct, Lemont, IL 60439",Argonne National Laboratory and general scientific community,Intel Data Center GPU Max Series 1550,"The Aurora supercomputer at Argonne National Laboratory is now fully equipped with all 10,624 compute blades, boasting 63,744 Intel® Data Center GPU Max Series and 21,248 Intel® Xeon® CPU Max Series processors.",5/13/2024,From official sources,10.74586792,,,,,,,63744,Intel,,TRUE,,52,18.52401917,3.34E+18,,3.34E+18,3.3421E+18,77.96146176,60,55701632000,940144813.5,500000000,503027173.8,"The Aurora system was budgeted for $500 million, and HPE is still getting paid to assemble the system and Intel is getting paid to help with the software stack, we suspect.",,,Meta GenAI 2024a,0.068716681,https://web.archive.org/web/20240824025729/https://www.intel.com/content/www/us/en/newsroom/news/intel-powered-aurora-supercomputer-breaks-exascale-barrier.html#gs.f9p1j0,https://web.archive.org/web/20240524235355/https://www.extremetech.com/computing/intel-aurora-supercomputer-breaks-exascale-barrier-but-fails-to-topple,https://web.archive.org/web/20240604155426/https://extremecomputingtraining.anl.gov/wp-content/uploads/sites/96/2023/08/ATPESC-2023-Track-1-Talk-3-Servesh-Mulalidharan-Aurora.pdf,,
Jean Zay Supercomputer Phase 4,Existing,Confirmed,Yes,18.49706904,1587.169277,NVIDIA H100 SXM5 80GB,1456,France,GENCI,9/15/2024,,Public,2.41672704,63517846.75,"Campus universitaire d'Orsay, Batiment 506, Rue John Von Neumann, 91403 Orsay, France",Researchers in academia and industry,"V100,A100,H100,1,832 V100s 416 A100s 1,456 H100s","In total after this extension, Jean Zay will be equipped with 1,456 NVIDIA H100 GPUs, in addition to the 416 NVIDIA A100 Tensor Core GPUs and 1,832 NVIDIA V100 Tensor Core GPUs remaining from the old configuration",2024-09,,11.81277118,Jean Zay Supercomputer Phase 3,,,,NVIDIA A100 SXM4 80 GB,416,3704,NVIDIA,NVIDIA,TRUE,,71,18.19599878,3.14E+18,3.14E+18,1.57E+18,7.85179E+17,2.41672704,,6.49787E+11,63517846.75,,,,,,xAI Colossus Memphis Phase 1,0.015871693,https://web.archive.org/web/20240528032205/http://www.idris.fr/eng/annonces/idris-extension-jean-zay-h100-eng.html,https://web.archive.org/web/20240523101724/https://www.cnrs.fr/en/press/genci-and-cnrs-choose-eviden-make-jean-zay-supercomputer-one-most-powerful-france,,,
Nebius ISEG,Existing,Confirmed,Yes,18.47828938,1520,NVIDIA H100 SXM5 80GB,1520,Finland,Nebius AI,11/1/2023,,Private,2.246,50309245.9,"Moreenikatu 6, 04600 Mäntsälä, Finland",Cloud,H100,"""The Mäntsälä data center is also home to ISEG, among the most powerful supercomputers worldwide in both performance and energy efficiency.""
Chip count calculated from Top500",11/1/2023,,11.82580574,,,,,,,1520,NVIDIA,,TRUE,,32,18.17721549,3.01E+18,3.01E+18,1.50E+18,7.51944E+17,2.2075872,2.246,6.69585E+11,50309245.9,,,,,,Tesla 10k H100 Cluster,0.152,https://web.archive.org/web/20241008144121/https://www.datacenterdynamics.com/en/news/nebius-deploys-ai-cluster-at-equinix-data-center-in-paris/,https://web.archive.org/web/20241109070701/https://www.top500.org/system/180234/,,,
Sandia NL El Dorado,Existing,Confirmed,Yes,18.47436547,1506.328449,AMD Instinct MI300A,1520,United States of America,US Department of Energy,11/18/2024,,Public,1.8556,62331530.09,"Albuquerque, New Mexico","Scientific Research,Sandia NL",AMD MI300A,Calculated from Top500,11/18/2024,,11.90485111,,,,,,,1520,AMD,,TRUE,,90,18.17333548,2.98E+18,2.98E+18,1.49E+18,7.45256E+17,2.35475968,1.8556,8.03251E+11,62331530.09,,,,,,xAI Colossus Memphis Phase 1,0.015063284,https://web.archive.org/web/20241125211138/https://www.top500.org/system/180309/,,,,
HUMAIN Saudi Arabia/NVIDIA Phase 1,Planned,Confirmed,Yes,18.47275645,1500.757959,NVIDIA GB300 (Blackwell Ultra),18000,Saudi Arabia,Humain,,,Public/Private,50.4504,,,,GB300,"""HUMAIN is making a major investment to build AI factories in the Kingdom of Saudi Arabia with a projected capacity of up to 500 megawatts powered by several hundred thousand of NVIDIA’s most advanced GPUs over the next five years. The first phase of deployment will be an 18,000 NVIDIA GB300 Grace Blackwell AI supercomputer with NVIDIA InfiniBand networking.""",Planned 2026,,11.9503479,,,,,,,18000,NVIDIA,,,,,19.65321251,2.97E+18,2.97E+18,4.50E+19,2.25E+19,50.4504,,8.91965E+11,,,,,,,,,https://archive.ph/vvAxQ,https://archive.ph/CFcnD,,,
NVIDIA Selene Phase 2,Existing,Confirmed,Yes,18.4464626,1412.592218,NVIDIA A100,4480,United States of America,NVIDIA,11/30/2020,,Private,4.7391,121250566.1,"Santa Clara, California",NVIDIA,A100,Configuration: 4480 NVIDIA A100 Tensor Core GPUs,11/30/2020,,11.46973674,NVIDIA Selene Phase 1,,,,,,4480,NVIDIA,,TRUE,,2,18.14543261,2.80E+18,2.80E+18,1.40E+18,6.9888E+17,3.913728,4.7391,2.94942E+11,121250566.1,,,,,,Oak Ridge NL Summit,0.808888889,https://web.archive.org/web/20240406153716/https://www.hpcwire.com/2020/06/22/nvidia-nabs-7-spot-on-top500-with-selene-launches-a100-pcie-cards/,https://web.archive.org/web/20240404062320/https://www.opensfs.org/wp-content/uploads/Accelerating-AI-at-Scale_Julie_Prethvi_updated051421.pdf,,,
Paper on Megatron-Turing,Existing,Confirmed,Yes,18.4464626,1412.592218,NVIDIA A100 SXM4 80 GB,4480,,"Microsoft,NVIDIA",1/28/2022,,Private,3.8158848,102590206.2,,"Microsoft,NVIDIA",A100,"""Model training is done with mixed precision using 16-bit bfloat on NVIDIA’s Selene [2] supercomputer with 560 DGX A100 nodes. Each cluster node has 8 NVIDIA 80-GB A100 GPUs""",1/28/2022,,11.56383735,,,TRUE,NVIDIA Selene Phase 2,,,4480,NVIDIA,,,,8,18.14543261,2.80E+18,2.80E+18,1.40E+18,6.9888E+17,3.8158848,,3.663E+11,102590206.2,,,,,,DeepSeek Fire-Flyer 2,0.448,https://arxiv.org/abs/2201.11990,,,,
Meta 2017 V100 Cluster,Existing,Confirmed,Yes,18.43933269,1389.590702,NVIDIA Tesla V100 SXM2,22000,United States of America,Meta AI,10/1/2017,"Unsure if this cluster has since been decommissioned, since it's relatively old
Our only information on the date is sources saying it was ""Built in 2017"". The V100 wasn't released until June 2017, so I assume this cluster was built in the second half of 2017. October 1st is the middle of the second half of 2017, so I put that as the date",Private,15.49548,475980972.6,,Meta,V100,"The first generation of this infrastructure, designed in 2017, has 22,000 NVIDIA V100 Tensor Core GPUs in a single cluster that performs 35,000 training jobs a day.",10/1/2017,Might be decommissioned by now,11.24912766,,,,,,,22000,NVIDIA,,TRUE,,1,18.43933269,2.75E+18,,2.75E+18,3.4474E+17,15.49548,,1.77471E+11,475980972.6,,,,TRUE,,Meta 2017 V100 Cluster,1,https://web.archive.org/web/20240828114058/https://ai.meta.com/blog/ai-rsc/,,,,
TensorWave MI300X Cluster 1 Phase 1,Existing,Likely,Yes,18.41745509,1321.323901,AMD Instinct MI300X,1000,United States of America,TensorWave,4/1/2024,,Private,1.5288,27161808.75,,Cloud,MI300X,1000 MI300Xs available from 04/01/2024 to 04/01/2025 (USA),4/1/2024,,11.93205781,,,,,,,1000,AMD,,TRUE,,52,18.11640848,2.61E+18,2.61E+18,1.31E+18,6.537E+17,1.5288,,8.55181E+11,27161808.75,,,,,,Meta GenAI 2024a,0.053764807,https://web.archive.org/web/20240420190200/https://gpulist.ai/detail/8f759e7,https://web.archive.org/web/20241009165502/https://www.theregister.com/2024/04/16/amd_tensorwave_mi300x/,,,
Paper on Falcon 180B,Existing,Confirmed,Yes,18.40754454,1291.512885,NVIDIA A100,4096,,Amazon,11/28/2023,,Private,3.39935232,80618433.05,,Cloud,A100,"pretrain these models on up to 4,096 A100s on cloud AWS infrastructure",11/28/2023,,11.57511836,,,TRUE,Amazon Titan training cluster,,,4096,NVIDIA,,,,40,18.10651454,2.56E+18,2.56E+18,1.28E+18,6.38976E+17,3.39935232,,3.7594E+11,80618433.05,,,,,,Microsoft Azure Eagle,0.089688395,https://arxiv.org/abs/2311.16867,https://web.archive.org/web/20250210123929/https://huggingface.co/tiiuae/falcon-180B,,,
Core42 SuperPOD,Existing,Confirmed,Yes,18.40093291,1272,NVIDIA H100 SXM5 80GB,1272,United Arab Emirates,G42,10/30/2024,,Public/Private,1.81499136,48463991.56,United Arab Emirates,"Cloud,G42",H100,Calculated from Top500,10/30/2024,,11.84098446,,,,,,,1272,NVIDIA,,TRUE,,88,18.09985902,2.52E+18,2.52E+18,1.26E+18,6.29258E+17,1.81499136,,6.93401E+11,48463991.56,,,,,,xAI Colossus Memphis Phase 1,0.01272,https://web.archive.org/web/20241125214402/https://www.middleeastainews.com/p/core42s-nvidia-dgx-superpod-ranked-first,,,,
Tesla Training Cluster,Existing,Likely,Yes,18.40070511,1271.332996,NVIDIA A100,4032,United States of America,Tesla,8/20/2021,I'm assuming these are A100s because H100s weren't out yet at the time and that is what the other cluster mentioned has,Private,3.46364928,137894792.6,,Tesla,A100,"as well as the previously mentioned 5670 system which is used for training, it has a second, 4032 GPU system for training and a 1752 GPU system for auto-labeling",8/20/2021,"Number of GPUs officially announced, but no other details about it released",11.56014121,,,,,,,4032,NVIDIA,,TRUE,,9,18.09967512,2.52E+18,2.52E+18,1.26E+18,6.28992E+17,3.46364928,,3.63196E+11,137894792.6,,,,,,DeepSeek Fire-Flyer 2,0.4032,https://web.archive.org/web/20240221025227/https://www.datacenterdynamics.com/en/news/tesla-details-dojo-supercomputer-reveals-dojo-d1-chip-and-training-tile-module/,,,,
AWS EC2 P4d,Existing,Likely,Unclear,18.39724458,1261.243052,NVIDIA A100 SXM4 40 GB,4000,United States of America,Amazon,11/2/2020,,Private,3.4944,108448368.4,,Cloud,A100,"EC2 UltraClusters are comprised of More than 4,000 latest NVIDIA A100 Tensor Core GPUs",11/2/2020,,11.55284197,,,,,,,4000,NVIDIA,,,,,18.09621459,2.50E+18,2.50E+18,1.25E+18,6.24E+17,3.4944,,3.57143E+11,108448368.4,,,,,,,,https://web.archive.org/web/20240522231925/https://pages.awscloud.com/amazon-ec2-p4d.html,,,,
Ezra-1 Stability AI AWS Cluster,Existing,Confirmed,Yes,18.39724458,1261.243052,NVIDIA A100,4000,,Amazon,7/20/2022,,Private,3.40704,90369258.41,,Stability AI,A100,"The model was trained on our 4,000 A100 Ezra-1 AI ultracluster over the last month",7/20/2022,,11.56383735,,,,,,,4000,NVIDIA,,TRUE,,16,18.09621459,2.50E+18,2.50E+18,1.25E+18,6.24E+17,3.40704,,3.663E+11,90369258.41,,,,,,Microsoft GPT-4 cluster,0.16,https://web.archive.org/web/20241207204816/https://stability.ai/news/stable-diffusion-announcement,,,,
JUWELS-Booster,Existing,Confirmed,Yes,18.36852043,1180.523497,NVIDIA A100,3744,Germany,Julich Supercomputing Center,11/16/2020,,Public,3.1598,101507672.9,"52428 Jülich, Germany",Academic research in Europe,A100,total of 1872 CPUs (AMD EPYC Rome) + total of 3744 GPUs (NVIDIA A100),11/16/2020,,11.56783084,,,,,,,3744,NVIDIA,,TRUE,,3,18.06749043,2.34E+18,2.34E+18,1.17E+18,5.84064E+17,3.2707584,3.1598,3.69684E+11,101507672.9,,,,,,Oak Ridge NL Summit,0.676,https://archive.ph/3RM36,,,,
Huawei Pangu Ultra MoE 910Bs,Existing,Confirmed,Yes,18.3533391,1139.969682,Huawei Ascend 910B,6000,,,,,,4.8048,,,Huawei,,"In the end, we achieve an MFU of 30.0% when training Pangu Ultra MoE, with performance comparable to that of DeepSeek R1, on 6K Ascend NPUs",,,11.67166378,,,,,,,6000,Huawei,,TRUE,,,18.3533391,2.26E+18,,2.26E+18,5.64E+17,4.8048,,4.6953E+11,,,,,,,,,,,,,
Paper on AFM-server,Existing,Confirmed,Yes,18.35272264,1138.352703,Google TPU v4,8192,United States of America,Google,6/10/2024,Cloud TPU clusters implies they trained on Google infrastructure,Private,2.838757376,73409062.37,,"Cloud,Google",TPUv4,"""We train AFM-server from scratch for 6.3T tokens on 8192
TPUv4 chips, using a sequence length of 4096 and a batch-size of 4096 sequences.""
""The AFM models are pre-trained on v4 and v5p Cloud TPU clusters""
""AFM-server was trained on 8192 TPUv4 chips provisioned as 8 × 1024 chip slices, where slices are connected together by the data-center network (DCN)""",6/10/2024,,11.89959436,,,TRUE,Google Oklahoma TPU v4 Pods,,,8192,Google,,,,66,18.35272264,2.25E+18,2.25E+18,2.25E+18,,2.838757376,,7.93587E+11,73409062.37,,,,,,Meta GenAI 2024a,0.04631969,https://machinelearning.apple.com/research/apple-intelligence-foundation-language-models,https://web.archive.org/web/20250114124728/https://arxiv.org/pdf/2407.21075,,,
JCAHPC Miyabi,Existing,Confirmed,Yes,18.34566382,1120,NVIDIA GH200,1120,Japan,Joint Center for Advanced High Performance Computing (JCAHPC),1/15/2025,,Public,1.569568,54654675.1,Japan,Scientific Research,GH200,"Miyabi-G (Accelerator-node group): 1,120 nodes, one NVIDIA GH200 Grace Hopper Superchip per node",2025-01,,11.84885369,,,,,,,1120,NVIDIA,,TRUE,,109,18.04463382,2.22E+18,2.22E+18,1.11E+18,5.5384E+17,1.569568,,7.0608E+11,54654675.1,,,,,,xAI Colossus Memphis Phase 1,0.0112,https://web.archive.org/web/20240417155526/https://www.jcahpc.jp/eng/supercomputer/miyabi-e.html,,,,
Lawrence Livermore NL Sierra,Existing,Confirmed,Yes,18.33445375,1091.460334,NVIDIA V100,17280,United States of America,US Department of Energy,6/1/2018,,Public,11.5,136658371,"Lawrence Livermore National Laboratory
7000 East Ave, Livermore, CA 94550","Lawrence Livermore NL,Scientific Research",V100,"Total GPUs: 17,280",6/1/2018,,11.27375591,,,,,,,17280,NVIDIA,,TRUE,,2,18.33445375,2.16E+18,,2.16E+18,2.70778E+17,11.8879488,11.5,1.87826E+11,340708604.2,125000000,136658371,we estimate that Sierra represented $125 million of the $325 million of the CORAL procurement based on a flat flops cost between the two machines,TRUE,,Meta 2017 V100 Cluster,0.785454545,https://web.archive.org/web/20240926221231/https://en.wikipedia.org/wiki/Sierra_(supercomputer),https://www.nextplatform.com/2017/10/05/clever-machinations-livermores-sierra-supercomputer/,https://web.archive.org/web/20240927001630/https://hpc.llnl.gov/hardware/compute-platforms/sierra,,
Ubilink.AI Supercomputer,Existing,Confirmed,Yes,18.30674575,1024,NVIDIA H100 SXM5 80GB,1024,Taiwan,Ubilink AI,11/18/2024,,Private,1.7823,39031055.27,"Tucheng, New Taipei, Taiwan",Cloud,H100,Calculated from Top500,11/18/2024,,11.75469106,,,,,,,1024,NVIDIA,,TRUE,,104,18.00567186,2.03E+18,2.03E+18,1.01E+18,5.06573E+17,1.46112512,1.7823,5.68448E+11,39031055.27,,,,,,xAI Colossus Memphis Phase 1,0.01024,https://web.archive.org/web/20241125215925/https://ubitus.net/ubilink-opening/,,,,
LeptonAI H100 Cluster,Existing,Likely,Yes,18.30674575,1024,NVIDIA H100 SXM5 80GB,1024,United States of America,Lepton AI,8/20/2024,This cluster is implied by their blog post. Unclear when it was first operational,Private,1.46112512,38941979.02,,Cloud,H100,"We observe similar statistics in our operation of different-sized clusters as well. For example, a 128 nodes (1024 GPU) cluster experiences one interruption every 30 hours",8/20/2024,,11.84098446,,,,,,,1024,NVIDIA,,TRUE,,81,18.00567186,2.03E+18,2.03E+18,1.01E+18,5.06573E+17,1.46112512,,6.93401E+11,38941979.02,,,,,,Meta GenAI 2024a,0.041666667,https://web.archive.org/web/20250128074456/https://blog.lepton.ai/introducing-gpud-the-missing-gpu-management-for-ai-0f0d026337e3,,,,
IBM Blue Vela,Existing,Likely,Yes,18.30674575,1024,NVIDIA H100 SXM5 80GB,1024,United States of America,IBM,7/7/2024,"There's 128x8=1,024 GPUs per pod, and it's implied that there's multiple pods, implying that there are thousands of GPUs, but this is all the information I have",Private,1.46112512,36491596.86,,IBM,"A100,H100","It uses 128-node Compute Pods. These contain 4 x Scalable Units, each of which contain 32 nodes. The nodes contain [8] Nvidia H100 GPUs.",7/7/2024,Very uncertain on number of chips. ,11.84098446,,,,,,,1024,NVIDIA,,TRUE,,77,18.00567186,2.03E+18,2.03E+18,1.01E+18,5.06573E+17,1.46112512,,6.93401E+11,36491596.86,,,,,,Meta GenAI 2024a,0.041666667,https://web.archive.org/web/20240804175848/https://blocksandfiles.com/2024/08/02/big-blues-storage-scale-using-blue-vela-ai-supercomputer/,https://web.archive.org/web/20240516182554/https://www.eetimes.com/ibm-refreshes-its-vela-research-ai-supercomputer/,,,
SIAM AI HGX,Existing,Likely,Yes,18.30674575,1024,NVIDIA H100 SXM5 80GB,1024,Thailand,SIAM AI,9/12/2024,,Private,1.46112512,38941979.02,Thailand,Cloud,H100,"currently operates a powerful cluster of 1,024 NVIDIA H100 GPUs",9/12/2024,,11.84098446,,,,,,,1024,NVIDIA,,TRUE,,84,18.00567186,2.03E+18,2.03E+18,1.01E+18,5.06573E+17,1.46112512,,6.93401E+11,38941979.02,,,,,,xAI Colossus Memphis Phase 1,0.01024,https://web.archive.org/web/20241204114134/https://www.cmkl.ac.th/news/the-first-hgx-h200-poc--asias-first-nvidia-h200--was-presented-by-siam-ai-nvidia-cloud-partner-to-cmkl-university-and-aiei,,,,
Horizon Compute Baobab Phase 1,Existing,Likely,Yes,18.30674575,1024,NVIDIA H100 SXM5 80GB,1024,United States of America,Horizon Compute,2/15/2024,"Judging by their current offerings (March 2025), they have a 2k H100 cluster and a 1k H100 cluster, so I'm interpreting this as them saying that they had two 1k H100 clusters in Q1 2024",Private,1.46112512,44580651.41,,Cloud,H100,"Since Q1 2024, we have already deployed two large-scale clusters (2000+ H100s) ",Q1 2024,,11.84098446,,Horizon Compute Baobab Phase 2,,,,,1024,NVIDIA,,TRUE,,50,18.00567186,2.03E+18,2.03E+18,1.01E+18,5.06573E+17,1.46112512,,6.93401E+11,44580651.41,,,,,,Microsoft Azure Eagle,0.071111111,https://web.archive.org/web/20241230203302/https://horizoncompute.com/,,,,
Paper on Mamba 2 Hybrid,Existing,Confirmed,Yes,18.30674575,1024,NVIDIA H100 SXM5 80GB,1024,,NVIDIA,6/12/2024,,Private,1.46112512,36558392.18,,NVIDIA,H100,"""When training on NVIDIA H100 GPUs (NVIDIA 2023), with a tensor-parallel
size of four and data-parallel size of 256 (1024 total GPUs)""",6/12/2024,,11.84098446,,,TRUE,NVIDIA CoreWeave Eos-DFW Phase 1,,,1024,NVIDIA,,,,71,18.00567186,2.03E+18,2.03E+18,1.01E+18,5.06573E+17,1.46112512,,6.93401E+11,36558392.18,,,,,,Meta GenAI 2024a,0.041666667,https://arxiv.org/abs/2406.07887,,,,
NVIDIA Helios,Existing,Confirmed,Yes,18.30674575,1024,NVIDIA GH200,1024,,NVIDIA,11/15/2023,"They announced in May 2023 that they were building it and that it would be operational by the end of the year. They haven't publicly confirmed that it is operational, but it seems very likely that it is",Private,1.48721664,47268443.53,,NVIDIA,GH200,"""Helios will include 1,024 Grace Hopper Superchips and is expected to come online by the end of the year.""",Q4 2023,Likely finished by now,11.83334152,,,,,,,1024,NVIDIA,,TRUE,,46,18.00571576,2.03E+18,2.03E+18,1.01E+18,5.06368E+17,1.48721664,,6.81305E+11,47268443.53,,,,,,Microsoft Azure Eagle,0.071111111,https://web.archive.org/web/20240825120701/https://nvidianews.nvidia.com/news/nvidia-announces-dgx-gh200-ai-supercomputer,,,,
Chan Zuckerberg Initiative GPU Cluster,Existing,Confirmed,Yes,18.30674575,1024,NVIDIA H100 SXM5 80GB,1024,United States of America,"CZ Biohub Network,CoreWeave",1/15/2025,CoreWeave cloud based cluster,Public,1.4350336,38560315.01,,"Chan Zuckerberg Initiative,Healthcare,Researchers",H100,"The high-performance computing cluster, which is planned to comprise 1,000+ GPUs, will enable AI and large language models for biomedicine at scale.",2025-01,,11.84880979,,,TRUE,,,,1024,NVIDIA,,,,113,18.00567186,2.03E+18,2.03E+18,1.01E+18,5.06573E+17,1.4350336,,7.06008E+11,38560315.01,,,,,,xAI Colossus Memphis Phase 1,0.01024,https://web.archive.org/web/20241201154848/https://chanzuckerberg.com/newsroom/czscience-builds-ai-gpu-cluster-predictive-cell-models/,https://web.archive.org/web/20250221155133/https://chanzuckerberg.com/rfa/ai-computing-gpu/,,,
GreenNode Bangkok Cluster,Existing,Confirmed,Yes,18.30674575,1024,NVIDIA H100 SXM5 80GB,1024,Thailand,VNG Corporation,6/25/2024,,Private,1.46112512,36491596.86,"1 Soi Ramkhamhaeng 28, Hua Mak, Khet Bang Kapi, Bangkok 10240, Thailand",Cloud,H100,GreenNode's GPU cluster includes 128 bare-metal servers equipped with 1024 NVIDIA H100 Tensor Core GPUs,6/25/2024,,11.84098446,,,,,,,1024,NVIDIA,,TRUE,,73,18.00567186,2.03E+18,2.03E+18,1.01E+18,5.06573E+17,1.46112512,,6.93401E+11,36491596.86,,,,,,Meta GenAI 2024a,0.041666667,https://web.archive.org/web/20250108210408/https://greennode.ai/blog/launch-hyper-scale-ai-gpu-cluster,https://archive.ph/pi5KL,,,
Neevcloud cluster 1,Existing,Unlikely,Yes,18.30674575,1024,NVIDIA H100 SXM5 80GB,1024,India,NeevCloud,4/29/2024,"Marked ""unlikely"" because an industry expert doubts it exists",Private,1.46112512,36425955.2,Central India,Cloud,H100,1024 H100s available from 04/29/2024,4/29/2024,,11.84098446,,,,,,,1024,NVIDIA,,,,,18.00567186,2.03E+18,2.03E+18,1.01E+18,5.06573E+17,1.46112512,,6.93401E+11,36425955.2,,,,,,,,https://web.archive.org/web/20240420194021/https://gpulist.ai/detail/ea604d5,https://web.archive.org/web/20250110180448/https://entrepreneurshipstudio.com/news/posts/neevcloud-launches-with-40-000-gpus-revolutionizing-ai-cloud-services-for-smes-in-india,,,
NVIDIA Eos Phase 1,Existing,Confirmed,Yes,18.30674575,1024,NVIDIA H100 SXM5 80GB,1024,United States of America,NVIDIA,5/22/2023,"Full system = 4608 GPUs
Pre system = (128/576)x4608 = 1024",Private,1.48721664,48736713.51,United States of America,NVIDIA,H100,"Nvidia’s new “Pre-Eos 128 Node DGX SuperPOD” is another one we’ve been waiting to hear word on.... In its full configuration, Eos will span 18 32-DGX H100 Pods, for a total of 576 DGX H100 systems, 4,608 H100 GPUs",5/22/2023,Ranked in Top500,11.83329763,,NVIDIA Eos Phase 2,,,,,1024,NVIDIA,,TRUE,,29,18.00567186,2.03E+18,2.03E+18,1.01E+18,5.06573E+17,1.48721664,,6.81236E+11,48736713.51,,,,,,Microsoft GPT-4 cluster,0.12990359,https://web.archive.org/web/20240406153948/https://www.hpcwire.com/2023/05/22/top500-frontier-gains-92-petaflops-henri-gets-a-little-greener/,,,,
Ori Global Cloud H100 Cluster,Existing,Likely,Yes,18.30674575,1024,NVIDIA H100 SXM5 80GB,1024,,Ori Industries,10/16/2024,,Private,1.46112512,38937096.51,,Cloud,H100,"The cluster features 1,024 Nvidia H100s",10/16/2024,Unclear where it is located,11.84098446,,,,,,,1024,NVIDIA,,TRUE,,92,18.00567186,2.03E+18,2.03E+18,1.01E+18,5.06573E+17,1.46112512,,6.93401E+11,38937096.51,,,,,,xAI Colossus Memphis Phase 1,0.01024,https://archive.ph/UwWFb,,,,
Sustainable Metal Cloud Singapore Phase 1,Existing,Confirmed,Yes,18.30674575,1024,NVIDIA H100 SXM5 80GB,1024,Singapore,SMC - Sustainable Metal Cloud,5/30/2024,,Private,1.46112512,36558392.18,Singapore,Cloud,H100,"H100 GPUs, up to 1,024 per cluster",5/30/2024,,11.84098446,,,,,,,1024,NVIDIA,,TRUE,,68,18.00567186,2.03E+18,2.03E+18,1.01E+18,5.06573E+17,1.46112512,,6.93401E+11,36558392.18,,,,,,Meta GenAI 2024a,0.041666667,https://archive.ph/ElLD9,,,,
Denvr Dataworks H100,Existing,Likely,Yes,18.30674575,1024,NVIDIA H100 SXM5 80GB,1024,United States of America,Denvr Dataworks,10/15/2024,"It appears to be in their central US availablity zone, suggesting that it's in the United States. Other location information is unknown",Private,1.46112512,38937096.51,,Cloud,H100,"The cluster features Nvidia HGX H100 servers with 1,024 Hopper architecture-based SXM5 GPUs",10/15/2024,,11.84098446,,,,,,,1024,NVIDIA,,TRUE,,92,18.00567186,2.03E+18,2.03E+18,1.01E+18,5.06573E+17,1.46112512,,6.93401E+11,38937096.51,,,,,,xAI Colossus Memphis Phase 1,0.01024,https://web.archive.org/web/20241214104602/https://www.datacenterdynamics.com/en/news/denvr-dataworks-expands-with-cloud-based-nvidia-h100-gpu-cluster/,,,,
NVIDIA Israel-1 Phase 1,Existing,Confirmed,Yes,18.30674575,1024,NVIDIA H100 SXM5 80GB,1024,Israel,NVIDIA,11/23/2023,,Private,1.48721664,33813671.1,Israel,NVIDIA,H100,"The first phase includes Dell PowerEdge XE9680 servers based on 128 Nvidia HGX H100 systems

will feature 256 Nvidia HGX H100 systems, combining 2,048 Nvidia H100 80GB GPUs with more than 34 million CUDA cores and 1 million fourth-generation Tensor Cores, 2,560 BlueField-3 DPUs, and 80 Spectrum-4 switches",11/23/2023,"The first half of the GPUs were confirmed to be operational Nov 2023, with the second half expected to be operational in the first half of 2024",11.83329763,,NVIDIA Israel-1 Phase 2,,,,,1024,NVIDIA,,TRUE,,46,18.00567186,2.03E+18,2.03E+18,1.01E+18,5.06573E+17,1.48721664,,6.81236E+11,33813671.1,,,,,,Microsoft Azure Eagle,0.071111111,https://web.archive.org/web/20240628053616/https://www.datacenterdynamics.com/en/news/nvidias-israel-1-supercomputer-starts-operations/,,,,
Scaleway Nabuchodonosor,Existing,Confirmed,Yes,18.3033395,1016,NVIDIA H100 SXM5 80GB,1016,France,Iliad SA,10/5/2023,,Private,1.47559776,33695574.74,"Paris, France","Scaleway,Cloud",H100,"Iliad has purchased an Nvidia DGX SuperPOD with 1,016 H100 GPUs",10/5/2023,,11.83329763,,,,,,,1016,NVIDIA,,TRUE,,38,18.00226561,2.01E+18,2.01E+18,1.01E+18,5.02615E+17,1.47559776,,6.81236E+11,33695574.74,,,,,,Tesla 10k H100 Cluster,0.1016,https://web.archive.org/web/20240225054506/https://www.datacenterdynamics.com/en/news/french-telco-iliad-acquires-nvidia-dgx-superpod-with-1016-h100-gpus/,https://web.archive.org/web/20240228100033/https://www.telecoms.com/digital-ecosystem/iliad-lays-claim-to-europe-s-most-powerful-ai-supercomputer,https://web.archive.org/web/20231005173700/https://www.digitaltveurope.com/2023/09/26/xavier-niels-iliad-sets-out-plan-to-create-european-ai-champion-with-supercomputer-investment/,,
Hut 8 H100 Cluster,Existing,Confirmed,Yes,18.29644579,1000,NVIDIA H100 SXM5 80GB,1000,United States of America,Hut 8,9/26/2024,,Private,1.42688,38024508.31,"Chicago, USA",Cloud,H100,"The cluster, hosted at a tier-three data center in Chicago, comprises multiple Hewlett Packard Enterprise (“HPE”) Cray supercomputers powered by 1,000 NVIDIA H100 GPUs.",9/26/2024,,11.84098446,,,,,,,1000,NVIDIA,,TRUE,,101,17.99537191,1.98E+18,1.98E+18,9.89E+17,4.947E+17,1.42688,,6.93401E+11,38024508.31,,,,,,xAI Colossus Memphis Phase 1,0.01,https://web.archive.org/web/20241217152625/https://hut8.com/2024/09/26/hut-8-gpu-as-a-service-vertical-goes-live-with-inaugural-deployment/,,,,
Voltage Park Texas Phase 1,Existing,Likely,Yes,18.29644579,1000,NVIDIA H100 SXM5 80GB,1000,United States of America,Voltage Park,2/27/2024,,Private,1.42688,35810188.63,Texas,Cloud,H100,1000 H100s available from 02/27/2024 to 02/27/2025 (Texas),2/27/2024,This is from a posting on gpulist.ai,11.84098446,,,,,,,1000,NVIDIA,,TRUE,,56,17.99537191,1.98E+18,1.98E+18,9.89E+17,4.947E+17,1.42688,,6.93401E+11,35810188.63,,,,,,Microsoft Azure Eagle,0.069444444,https://web.archive.org/web/20240226194913/https://gpulist.ai/detail/2104b9d,https://web.archive.org/web/20240420201628/https://www.datacenterdynamics.com/en/news/ai-cloud-computing-non-profit-buys-24000-nvidia-h100-chips/,https://web.archive.org/web/20240913183724/https://finance.yahoo.com/news/penguin-solutions-selected-managed-services-150000685.html?guccounter=1&guce_referrer=aHR0cHM6Ly93d3cucGVycGxleGl0eS5haS8&guce_referrer_sig=AQAAACbflXKvc2DyvuR2WPik8ksxhKTAz7auQoF9nqCdOM4yDPTL4jd-W2b-V8yRSH21fxe4Ccv4cv3WjdfSFteyrcb-e_fbqoJhuhCzDkUZMwLHbCvHLBwKEOw_g_JSilSt4zqyJ6cKRYvHZ-fOSzVztIaQMrRoiWkdsftOLBO6ZB7P,,
NHN Cloud's National AI Data Center,Existing,Confirmed,Yes,18.29644579,1000,NVIDIA H100 SXM5 80GB,1000,Korea (Republic of),NHN Corporation,11/1/2023,,Public/Private,1.45236,33098188.09,"Oryong-dong, Buk-gu, Gwangju, South Korea",Cloud,H100,"some 1,000 Nvidia H100 graphics processing units (GPUs) are working,",11/1/2023,,11.83329763,,,,,,,1000,NVIDIA,,TRUE,,44,17.99537191,1.98E+18,1.98E+18,9.89E+17,4.947E+17,1.45236,,6.81236E+11,33098188.09,,,,,,Tesla 10k H100 Cluster,0.1,https://web.archive.org/web/20240721155544/https://koreajoongangdaily.joins.com/news/2024-03-25/business/industry/NHN-Cloud-aims-sky-high-with-AI-data-center-in-Gwangju/2010013,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,18.30103,1000,,,China,,1/15/2024,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,TRUE,,54,18.30103,2.00E+18,,2.00E+18,,,,,,,,,,,Microsoft Azure Eagle,0.07,,,,,
Anonymized Chinese System,Planned,Likely,Unclear,18.30103,1000,,4000,China,,,,Public,3,,China,,,,,,11.52287875,,,,,,,4000,Anonymized,Anonymized,,,,18,2.00E+18,2.00E+18,1.00E+18,,3,,3.33333E+11,,,,,,,,,,,,,
Anonymized Chinese System,Planned,Likely,Yes,18.30103,1000,,,China,,,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,18.30103,2.00E+18,,2.00E+18,,,,,,,,,,,,,,,,,
Anonymized Chinese System,Planned,Likely,Yes,18.30103,1000,,,China,,,,Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,18.30103,2.00E+18,,2.00E+18,,,,,,,,,,,,,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,18.30103,1000,,,China,,11/15/2024,,Public,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,TRUE,,120,18.30103,2.00E+18,,2.00E+18,,,,,,,,,,,xAI Colossus Memphis Phase 1,0.01,,,,,
Anonymized Chinese System,Existing,Likely,Yes,18.30103,1000,,,China,,7/15/2022,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,TRUE,,17,18.30103,2.00E+18,,2.00E+18,,,,,,,,,,,Microsoft GPT-4 cluster,0.1,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,18.30103,1000,,4000,China,,9/15/2020,,Public,3,,China,,,,,,11.52287875,,,,,,,4000,Anonymized,Anonymized,TRUE,,4,18,2.00E+18,2.00E+18,1.00E+18,,3,,3.33333E+11,,,,,,,Oak Ridge NL Summit,0.6,,,,,
Anonymized Chinese System,Existing,Confirmed,No,18.30103,1000,,,China,,11/15/2022,,,,,China,,,,,,,,,TRUE,,,,,Anonymized,Anonymized,,,,18.30103,2.00E+18,,2.00E+18,,,,,,,,,,,,,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,18.30103,1000,,,China,,12/15/2024,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,TRUE,,121,18.30103,2.00E+18,,2.00E+18,,,,,,,,,,,xAI Colossus Memphis Phase 1,0.01,,,,,
Anonymized Chinese System,Planned,Confirmed,Yes,18.30103,1000,,,China,,,,Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,18.30103,2.00E+18,,2.00E+18,,,,,,,,,,,,,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,18.30103,1000,,1000,China,,9/15/2024,,Public/Private,0.7,,China,,,,,,12.45593196,,,,,,,3000,Anonymized,Anonymized,TRUE,,94,18.30103,2.00E+18,,2.00E+18,4E+17,0.7,,2.85714E+12,,,,,,,xAI Colossus Memphis Phase 1,0.01,,,,,
Anonymized Chinese System,Planned,Confirmed,Unclear,18.47712125,1000,,,China,,,,Public,,,China,,,,,,,,,,,,,20000,Anonymized,Anonymized,,,,18,3.00E+18,3.00E+18,1.00E+18,,,,,,,,,,,,,,,,,
Paper on Chameleon,Existing,Confirmed,Yes,18.2826058,968.634664,NVIDIA A100,3072,,Meta AI,5/16/2024,Table 2 Chemeleon Model Pre-Training Resource Usage lists 3072 Concurrent GPUs for the 34B model. ,Private,2.50478592,59935114.39,,,A100,"""NVIDIA A100 80GB GPUs power both environments.""",5/16/2024,,11.58280519,,,TRUE,Meta Research SuperCluster (RSC-1) Phase 2,,,3072,NVIDIA,,,,76,17.98157581,1.92E+18,1.92E+18,9.58E+17,4.79232E+17,2.50478592,,3.82653E+11,59935114.39,,,,,,Meta GenAI 2024a,0.039413845,https://arxiv.org/abs/2405.09818v1,,,,
TSUBAME4.0,Existing,Confirmed,Yes,18.27871703,960,NVIDIA H100 SXM5 80GB,960,Japan,Tokyo Institute of Technology,4/1/2024,,Public,1.3638,34268337.12,"4259 Nagatsutacho, Midori Ward, Yokohama, Kanagawa 226-0026, Japan",Cloud,H100,"The Japanese system will pack in 240 nodes of HPE's Cray XD6500 hardware...
Each node has a pair of AMD's 4th generation Epyc processors with 768GB of memory and four of Nvidia's H100 Tensor Core GPUs",4/1/2024,,11.84289245,,,,,,,960,NVIDIA,,TRUE,,68,17.97764314,1.90E+18,1.90E+18,9.50E+17,4.74912E+17,1.3698048,1.3638,6.96454E+11,34268337.12,,,,,,Meta GenAI 2024a,0.0390625,https://web.archive.org/web/20241002114126/https://www.theregister.com/2023/05/22/hpe_gets_to_build_new/,https://web.archive.org/web/20240627054219/https://www.hpcwire.com/off-the-wire/tokyo-techs-tsubame4-0-supercomputer-now-operational/,,,
Paper on AFM-on-device,Existing,Confirmed,Yes,18.27417263,950.0070743,Google TPU v5p,2048,,Google,6/10/2024,Cloud TPU clusters implies they trained on Google infrastructure,Private,,13097617.68,,Cloud,TPUv5p,"""AFM-on-device was trained on one slice of 2048 TPUv5p chips.""
""The AFM models are pre-trained on v4 and v5p Cloud TPU clusters""",6/10/2024,,,,,TRUE,Google Hypercomputer TPU v5p pod,,,2048,Google,,,,81,17.97314264,1.88E+18,1.88E+18,9.40E+17,,,,,13097617.68,,,,,,Meta GenAI 2024a,0.038655887,https://machinelearning.apple.com/research/apple-intelligence-foundation-language-models,https://web.archive.org/web/20250114124728/https://arxiv.org/pdf/2407.21075,,,
Intel Stability Gaudi 2,Existing,Confirmed,Yes,18.25527251,909.5502779,Intel Habana Gaudi2,4000,United States of America,Intel,3/11/2024,,Private,4.89216,34422689.46,Texas,Stability AI,Habana Gaudi2,"build a large AI supercomputer using Xeon processors and 4,000 Gaudi2 AI hardware accelerators",3/11/2024,,11.56577185,,,,,,,4000,Intel,,TRUE,,59,18.25527251,1.80E+18,,1.80E+18,,4.89216,,3.67936E+11,34422689.46,,,,,,Microsoft Azure Eagle,0.063163214,https://web.archive.org/web/20240806003107/https://www.datacenterdynamics.com/en/news/intel-and-dell-to-build-supercomputer-for-stability-ai-featuring-cpus-and-gaudi2-accelerators/,https://web.archive.org/web/20241201154514/https://stability.ai/news/putting-the-ai-supercomputer-to-work,https://web.archive.org/web/20241202232149/https://www.databricks.com/blog/llm-training-and-inference-intel-gaudi2-ai-accelerators,,
IBM Vela,Existing,Likely,Unclear,18.25457708,908.0949975,NVIDIA A100 SXM4 80 GB,2880,United States of America,IBM,5/30/2022,,Private,2.4530688,64997361.02,,IBM,A100,"Vela, which has been online since May of last year, consists of 60 racks (per Forbes) and an unspecified number of nodes; however, one might be inclined to trust the above diagram – accurate in all other respects – and guess six nodes per rack, for a total of 360 nodes and 2,880 A100 GPUs.",5/30/2022,Number of chips is an estimate,11.56383735,,,,,,,2880,NVIDIA,,,,,17.95354708,1.80E+18,1.80E+18,8.99E+17,4.4928E+17,2.4530688,,3.663E+11,64997361.02,,,,,,,,https://web.archive.org/web/20230320230907/https://www.hpcwire.com/2023/02/08/ibm-introduces-vela-cloud-ai-supercomputer-powered-by-intel-nvidia/,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,18.30103,900,,7000,China,,3/15/2024,,Public,4,,China,,,,,,11.35218252,,,,,,,7000,Anonymized,Anonymized,TRUE,,61,17.95424251,2.00E+18,2.00E+18,9.00E+17,2E+17,4,,2.25E+11,,,,,,,Meta GenAI 2024a,0.04,,,,,
Paper on PaLM,Existing,Confirmed,Yes,18.2277839,853.7645276,Google TPU v4,6144,United States of America,Google,4/5/2022,,Private,2.224115712,56257230.65,,Google,TPUv4,"""We trained PaLM on 6144 TPU v4 chips using Pathways,""",4/5/2022,,11.88062652,,,,,,,6144,Google,,TRUE,,15,18.2277839,1.69E+18,1.69E+18,1.69E+18,,2.224115712,,7.59673E+11,56257230.65,,,,,,DeepSeek Fire-Flyer 2,0.270769231,https://arxiv.org/abs/2204.02311,,,,
Microsoft Ares/Maia,Existing,Likely,Yes,18.21441994,827.8928752,Maia 100 (M100),2048,United States of America,Microsoft,11/15/2023,,Private,2.1245952,,,Cloud,Maia 100 (aka Athena aka M100),"The largest individual deployment for the backend network for Maia is 2,048, but there is nothing preventing them from scaling this up further.",11/15/2023,,11.88714374,,,,,,,2048,Microsoft,,TRUE,,51,18.21441994,1.64E+18,,1.64E+18,,2.1245952,,7.71159E+11,,,,,,,Microsoft Azure Eagle,0.057492561,https://archive.ph/Iigef,https://web.archive.org/web/20250211085121/https://semianalysis.com/2023/11/15/microsoft-infrastructure-ai-and-cpu/,,,
Iris Energy Prince George cluster,Existing,Confirmed,Yes,18.20813595,816,NVIDIA H100 SXM5 80GB,816,Canada,IREN,4/30/2024,,Private,1.16433408,29026933.05,"Prince George, Canada",Cloud,H100,"""AI Cloud Services offered by Iris Energy cater to AI customers, providing cloud compute capabilities with 816 NVIDIA H100 GPUs. This service has been operational since 2024 and is characterized by a performance-focused technology stack.""",4/30/2024,,11.84098446,,,,,,,816,NVIDIA,,TRUE,,77,17.90706206,1.61E+18,1.61E+18,8.07E+17,4.03675E+17,1.16433408,,6.93401E+11,29026933.05,,,,,,Meta GenAI 2024a,0.033203125,https://www.globenewswire.com/en/news-release/2023/12/21/2799869/0/en/10-EH-s-Equity-Raising-Program-Complete.html,https://irisenergy.gcs-web.com/news-releases/news-release-details/iris-energy-purchases-nvidia-h100-gpus-target-generative-ai,https://iris-energy.co/blog/iris-energy-expands-partnership-with-poolside-for-ai-cloud-services/428,,
Anonymized Chinese System,Existing,Confirmed,Yes,18.30103,800,,,China,,1/15/2024,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,TRUE,,56,18.30103,2.00E+18,,2.00E+18,,,,,,,,,,,Microsoft Azure Eagle,0.05,,,,,
Anonymized Chinese System,Existing,Confirmed,Unclear,18.30103,800,,,China,,,,Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,18.30103,2.00E+18,,2.00E+18,,,,,,,,,,,,,,,,,
NVIDIA Selene Phase 1,Existing,Confirmed,Yes,18.14543261,706.2961092,NVIDIA A100,2240,United States of America,NVIDIA,6/22/2020,,Private,2.4075,61625087.04,"Santa Clara, California",NVIDIA,A100,"Altogether, Selene comprises 280 DGX A100s, housing a total 2,240 A100 GPUs and 494 Mellanox Quantum 200G InfiniBand switches, providing 56 TB/s network fabric",6/22/2020,,11.46283632,,NVIDIA Selene Phase 2,,,,,2240,NVIDIA,,TRUE,,4,17.84440261,1.40E+18,1.40E+18,6.99E+17,3.4944E+17,1.956864,2.4075,2.90293E+11,61625087.04,,,,,,Oak Ridge NL Summit,0.404444444,https://web.archive.org/web/20240406153716/https://www.hpcwire.com/2020/06/22/nvidia-nabs-7-spot-on-top500-with-selene-launches-a100-pcie-cards/,https://web.archive.org/web/20241229072511/https://top500.org/system/179842/,,,
NAVER Corp Sejong,Existing,Confirmed,Yes,18.14543261,706.2961092,NVIDIA A100,2240,Korea (Republic of),NAVER,11/15/2023,,Private,1.8590208,44191045.38,"824, Haengbok-daero, Sejong-si, South Korea","NAVER,Cloud",A100,Calculated from Top500,11/15/2023,,11.57511836,,,,,,,2240,NVIDIA,,TRUE,,52,17.84440261,1.40E+18,1.40E+18,6.99E+17,3.4944E+17,1.8590208,,3.7594E+11,44191045.38,,,,,,Microsoft Azure Eagle,0.049048341,https://web.archive.org/web/20241109103411/https://www.top500.org/system/180222/,,,,
Argonne NL Polaris,Existing,Confirmed,Yes,18.14543261,706.2961092,NVIDIA A100,2240,United States of America,US Department of Energy,11/1/2021,,Public,1.9242496,76476747.44,"9700 S Cass Ave, Lemont, IL 60439",Argonne National Laboratory and general scientific community,A100,"That includes a total of 560 AMD Epyc Milan CPUs and 2,240 Nvidia 40GB A100 GPUs, connected by HPE's Slingshot networking",11/1/2021,From gov website,11.56014121,,,,,,,2240,NVIDIA,,TRUE,,13,17.84440261,1.40E+18,1.40E+18,6.99E+17,3.4944E+17,1.9242496,,3.63196E+11,76476747.44,,,,,,DeepSeek Fire-Flyer 2,0.224,https://web.archive.org/web/20230512044406/https://www.hpcwire.com/2022/08/09/argonne-deploys-polaris-supercomputer-for-science-in-advance-of-aurora/,https://web.archive.org/web/20240530041315/https://www.top500.org/system/180016/,https://web.archive.org/web/20240228132238/https://www.datacenterdynamics.com/en/news/argonnes-44-petaflops-polaris-supercomputer-goes-live/,,
Microsoft Azure Voyager-EUS2,Existing,Confirmed,Yes,18.1198785,665.9363315,NVIDIA A100,2112,United States of America,Microsoft,11/1/2021,,Private,1.81429248,72106647.59,"101 Herbert Dr, Boydton, VA 23917","Cloud,Azure",A100,Calculated from Top500,11/1/2021,Listed in Top500,11.56014121,,,,,,,2112,NVIDIA,,TRUE,,15,17.81884851,1.32E+18,1.32E+18,6.59E+17,3.29472E+17,1.81429248,,3.63196E+11,72106647.59,,,,,,DeepSeek Fire-Flyer 2,0.2112,https://web.archive.org/web/20240716100550/https://www.zdnet.com/article/microsoft-now-has-one-of-the-worlds-fastest-supercomputers-and-no-it-doesnt-run-on-windows/,https://web.archive.org/web/20240808114950/https://www.top500.org/system/180024/,,,
Paper on LLaMA,Existing,Confirmed,Yes,18.10651454,645.7564427,NVIDIA A100 SXM4 80 GB,2048,,Meta AI,2/27/2023,,Private,1.69967616,45851025.56,,Meta,A100,"""Finally, we estimate that we used 2048 A100-80GB for a period of approximately 5 months to develop our models.""",2/27/2023,,11.57511836,,,TRUE,Meta Research SuperCluster (RSC-1) Phase 1,,,2048,NVIDIA,,,,30,17.80548455,1.28E+18,1.28E+18,6.39E+17,3.19488E+17,1.69967616,,3.7594E+11,45851025.56,,,,,,Microsoft GPT-4 cluster,0.08192,https://arxiv.org/abs/2302.13971,,,,
Softbank SuperPOD,Existing,Confirmed,Yes,18.10651454,645.7564427,NVIDIA A100,2048,Japan,Softbank,9/30/2023,"Estimated ""over 2,000"" as 2,048",Private,1.69967616,40484720.95,Japan,Softbank,A100,"This Japan top-level*2 computing platform comprises an NVIDIA DGX SuperPOD(Open in a new window)™ AI supercomputer with over 2,000 NVIDIA Tensor Core GPUs(Open in a new window), NVIDIA Networking(Open in a new window) and NVIDIA AI Enterprise(Open in a new window) software",9/30/2023,,11.57511836,,,,,,,2048,NVIDIA,,TRUE,,42,17.80548455,1.28E+18,1.28E+18,6.39E+17,3.19488E+17,1.69967616,,3.7594E+11,40484720.95,,,,,,Tesla 10k H100 Cluster,0.064575644,https://web.archive.org/web/20240623042927/https://www.softbank.jp/en/corp/news/press/sbkk/2023/20231031_01/,,,,
Petrobras Pegasus (Pégaso),Existing,Confirmed,Yes,18.09967512,635.6664982,NVIDIA A100,2016,Brazil,Petrobras,11/1/2022,,Public/Private,1.8039,45553132.31,"Rio de Janeiro, Brazil",Petrobras,A100,"With 21 petaflops of power, 678TB of RAM memory, 2,016 GPUs, and a 400GB/s Internet link, Pegasus will be the most powerful supercomputer in Latin America.",11/1/2022,,11.54243266,,,,,,,2016,NVIDIA,,TRUE,,28,17.79864512,1.26E+18,1.26E+18,6.29E+17,3.14496E+17,1.71714816,1.8039,3.48685E+11,45553132.31,,,,,,Microsoft GPT-4 cluster,0.08064,https://web.archive.org/web/20240423211648/https://www.datacenterdynamics.com/en/news/petrobras-announces-new-pegasus-supercomputer-in-rio-de-janeiro/,,,,
Azure OpenAI GPT-3 Cluster,Existing,Likely,Yes,18.09691001,631.6321375,NVIDIA V100,10000,United States of America,"Microsoft,OpenAI",4/15/2020,"It has ""since been upgraded to include tens of thousands of A100 chips"". Rumors suggest that it is in Washington State",Private,6.552,191957807,"1515 Port Industrial Way, Quincy, WA 98848",OpenAI,V100,"""The supercomputer developed for OpenAI is a single system with more than 285,000 CPU cores, 10,000 GPUs and 400 gigabits per second of network connectivity for each GPU server. Compared with other machines listed on the TOP500 supercomputers in the world, it ranks in the top five, Microsoft says.""
""We understand the supercomputer uses AMD second-generation Rome Epyc server processors, and Nvidia V100 GPUs, from sources in the industry.""",2020-04,,11.28053612,,,,,,,10000,NVIDIA,,TRUE,,4,18.09691001,1.25E+18,,1.25E+18,1.567E+17,6.552,,1.90781E+11,191957807,,,,TRUE,,Oak Ridge NL Summit,0.361689815,https://web.archive.org/web/20240902024137/https://news.microsoft.com/source/features/ai/openai-azure-supercomputer/,https://web.archive.org/web/20240307163254/https://www.theregister.com/2020/05/20/microsoft_openai_supercomputer/,,,
ND A100 v4,Existing,Likely,Yes,18.09621459,630.621526,NVIDIA A100,2000,United States of America,Microsoft,6/1/2021,"They ran the high performance LINPACK benchmark on a pool of 164 8xA100 VMs, so the clusters can include at least 1312 A100s. This might be a slice of a larger cluster.
Estimating as 2k since it says ""thousands""",Private,1.71808,53988631.73,,Cloud,A100,"""The ND A100 v4 series starts with a single VM and eight NVIDIA Ampere A100 40GB Tensor Core GPUs. ND A100 v4-based deployments can scale up to thousands of GPUs""",6/1/2021,,11.56014121,,,,,,,2000,NVIDIA,,TRUE,,12,17.79518459,1.25E+18,1.25E+18,6.24E+17,3.12E+17,1.71808,,3.63196E+11,53988631.73,,,,,,Sunway OceanLight,0.209909268,https://web.archive.org/web/20241114112532/https://azure.microsoft.com/de-de/blog/azure-announces-general-availability-of-scaleup-scaleout-nvidia-a100-gpu-instances-claims-title-of-fastest-public-cloud-super/,https://learn.microsoft.com/en-us/azure/virtual-machines/sizes/gpu-accelerated/ndasra100v4-series?tabs=sizebasic,,,
ExxonMobil Discovery 5,Existing,Confirmed,Yes,18.09621459,630.621526,NVIDIA A100 SXM4 40 GB,2000,United States of America,ExxonMobil,11/1/2022,,Private,1.6834,44379774.37,"22777 Springwoods Village Pkwy, Spring, TX 77389",ExxonMobil,A100,Calculated from Top500,11/1/2022,,11.56899727,,,,,,,2000,NVIDIA,,TRUE,,30,17.79518459,1.25E+18,1.25E+18,6.24E+17,3.12E+17,1.70352,1.6834,3.70678E+11,44379774.37,,,,,,Microsoft GPT-4 cluster,0.08,https://web.archive.org/web/20241109060139/https://www.top500.org/system/180131/,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,18,600,,5000,China,,12/15/2019,,Public,3,,China,,,,,,11.36797679,,,,,,4000,9000,Anonymized,Anonymized,TRUE,,4,17.84509804,1.00E+18,1.00E+18,7.00E+17,,3,,2.33333E+11,,,,,,,Oak Ridge NL Summit,0.3,,,,,
Corvex B200s,Planned,Likely,Yes,18.06145248,582.1121779,NVIDIA B200,256,United States of America,Corvex.ai,,,,0.512512,,Delaware,,,,Planned Q2 2025,,12.05071844,,,,,,,256,NVIDIA,,,,,17.76042248,1.15E+18,1.15E+18,5.76E+17,2.88E+17,0.512512,,1.12388E+12,,,,,,,,,Private correspondence with Corvex.ai staff,,,,
Paper on Gemma 2 9B,Existing,Confirmed,Yes,18.05169264,569.1763517,Google TPU v4,4096,United States of America,Google,6/27/2024,,Private,1.419378688,36637468.85,,Google,TPUv4,"""For the 9B model, we train on an 8x16x32 configuration of TPUv4, totaling 4096 chips,""",6/27/2024,,11.89959436,,,TRUE,Google Oklahoma TPU v4 Pods,,,4096,Google,,,,105,18.05169264,1.13E+18,1.13E+18,1.13E+18,,1.419378688,,7.93587E+11,36637468.85,,,,,,Meta GenAI 2024a,0.023159845,https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf,,,,
Google TPU v4 Pod,Existing,Confirmed,Yes,18.05169264,569.1763517,Google TPU v4,4096,United States of America,Google,5/19/2021,"Used in July 2021 MLPerf benchmark, mentioned May 2021, likely operational beforehand",Private,1.495416832,38153578.88,,Google,TPUv4,"""Google deploys the chips in its own data centers, combining them in pods of 4,096 TPUs""
""TPU v3. 4,096 of these TPU v4 chips are networked together to create a TPU v4 Pod""",5/19/2021,,11.87693038,,,,,,,4096,Google,,TRUE,,13,18.05169264,1.13E+18,1.13E+18,1.13E+18,,1.495416832,,7.53235E+11,38153578.88,,,,,,Sunway OceanLight,0.18945657,https://web.archive.org/web/20250221082528/https://cloud.google.com/blog/products/ai-machine-learning/google-wins-mlperf-benchmarks-with-tpu-v4,https://web.archive.org/web/20241005134423/https://www.datacenterdynamics.com/en/news/google-unveils-fourth-generation-tpu-ai-chip/,,,
NVIDIA Taipei-1,Existing,Confirmed,Yes,18.04372371,558.8276908,NVIDIA H100 SXM5 80GB,512,Taiwan,NVIDIA,6/1/2024,,Private,0.88711168,18279196.09,"Kaohsiung Software Technology Park
No. 10號, Fusing 4th Rd, Cianjhen District, Kaohsiung City, Taiwan 806","Cloud,NVIDIA","H100,L40","It will include 64 DGX H100 systems – each of which includes eight H100 GPUs (for a total of 512) – and 64 Omniverse-focused OVX systems, each of which includes four L40 GPUs and a BlueField-3 DPU...
the system is aimed at wide-ranging use cases like large language models (LLMs), healthcare, robotics, industrial digital twins and – like Taiwania 4 – climate science",6/1/2024,"Listed as operational in Top500, details of system confirmed earlier",11.7946752,,,,,NVIDIA L40,256,768,NVIDIA,NVIDIA,TRUE,,98,17.7426535,1.11E+18,1.11E+18,5.53E+17,2.7646E+17,0.88711168,,6.23269E+11,18279196.09,,,,,,Meta GenAI 2024a,0.022738757,https://web.archive.org/web/20240423000708/https://www.hpcwire.com/2023/05/29/nvidia-announces-four-supercomputers-with-two-in-taiwan/,,,,
Samsung SSC-21,Existing,Confirmed,Yes,18.04069726,554.9469429,NVIDIA A100,1760,Korea (Republic of),Samsung,11/1/2021,,Private,1.5119104,60088872.99,Korea (Republic of),Samsung,A100,Calculated from Top500,11/1/2021,Listed in Top500,11.56014121,,,,,,,1760,NVIDIA,,TRUE,,20,17.73966726,1.10E+18,1.10E+18,5.49E+17,2.7456E+17,1.5119104,,3.63196E+11,60088872.99,,,,,,DeepSeek Fire-Flyer 2,0.176,"https://web.archive.org/web/20240117215326/https://www.kedglobal.com/artificial-intelligence/newsView/ked202305310010#:~:text=Samsung%20Electronics%20Co.%2C%20the%20world's,with%201.19%20exaflops%20of%20performance.",https://web.archive.org/web/20240717134459/https://www.top500.org/system/180041/,,,
Tesla Auto-Labeling Cluster,Existing,Likely,Yes,18.03871869,552.4244568,NVIDIA A100,1752,United States of America,Tesla,8/20/2021,I'm assuming these are A100s because H100s weren't out yet at the time and that is what the other cluster mentioned has,Private,1.50503808,59918570.58,,Tesla,A100,"as well as the previously mentioned 5670 system which is used for training, it has a second, 4032 GPU system for training and a 1752 GPU system for auto-labeling",8/20/2021,"Number of GPUs officially announced, but no other details about it released",11.56014121,,,,,,,1752,NVIDIA,,TRUE,,18,17.7376887,1.09E+18,1.09E+18,5.47E+17,2.73312E+17,1.50503808,,3.63196E+11,59918570.58,,,,,,DeepSeek Fire-Flyer 2,0.1752,https://web.archive.org/web/20240221025227/https://www.datacenterdynamics.com/en/news/tesla-details-dojo-supercomputer-reveals-dojo-d1-chip-and-training-tile-module/,,,,
Tesla Dojo 1 Phase 1,Planned,Confirmed,Yes,18.03582983,548.762001,Tesla D1 Dojo,3000,United States of America,Tesla,,,Private,2.4024,,"San Jose, California","Tesla,but ""“It's probably going to make more sense to have Dojo operate in like an Amazon Web Services manner than to try to sell it to someone else",Tesla Dojo D1,"Once complete, the entire system will house around 120 tiles, holding 3,000 custom D1 chips.",Planned,Musk plans for a scaled up version of this cluster to be operational by the end of 2024. Unclear if the smaller version ever ran,11.65518451,,,,,,,3000,Tesla,,,,,18.03582983,1.09E+18,,1.09E+18,6.78E+16,2.4024,,4.52048E+11,,,,$1B,,,,,https://web.archive.org/web/20241208081438/https://www.datacenterdynamics.com/en/news/tesla-begins-installing-dojo-supercomputer-cabinets-trips-local-substation/,https://web.archive.org/web/20240903213050/https://techcrunch.com/2024/08/10/teslas-dojo-a-timeline/,https://web.archive.org/web/20250125054824/https://dgtlinfra.com/elon-musk-data-centers/,,
DeepL Mercury,Existing,Confirmed,Yes,18.03204469,544,NVIDIA H100 SXM5 80GB,544,Sweden,DeepL,8/2/2023,,Private,0.79008384,19573215.02,"Slaggvarpsvägen 21, 791 77 Falun, Sweden",DeepL,H100,DeepL has added a DGX SuperPOD with 68 NVIDIA DGX H100 systems which are supplied by DELTA Computer Products,8/2/2023,,11.83329763,,,,,,,544,NVIDIA,,TRUE,,46,17.73097081,1.08E+18,1.08E+18,5.38E+17,2.69117E+17,0.79008384,,6.81236E+11,19573215.02,,,,,,Microsoft GPT-4 cluster,0.069011282,https://web.archive.org/web/20241120225551/https://digests.digitalisationworld.com/news/65928/deepl-deploys-largest-nvidia-dgx-h100-superpod-in-europe,,,,
Paper on AlphaCode,Existing,Confirmed,Yes,18.01336396,521.0965134,Google TPU v4,3750,United States of America,Google,2/8/2022,,Private,1.3574925,34919722.83,,Google,TPUv4,,2/8/2022,,11.88062652,,,TRUE,Google TPU v4 Pod,,,3750,Google,,,,24,18.01336396,1.03E+18,1.03E+18,1.03E+18,,1.3574925,,7.59673E+11,34919722.83,,,,,,DeepSeek Fire-Flyer 2,0.165264423,https://arxiv.org/abs/2203.07814,,,,
Paper on Phi-3.5-Mini,Existing,Confirmed,Yes,18.00571576,512,NVIDIA H100 SXM5 80GB,512,,Microsoft,4/22/2024,,Private,0.73056256,18212977.6,,Cloud,H100,,4/22/2024,,11.84098446,,,TRUE,Microsoft Azure Eagle,,,512,NVIDIA,,,,94,17.70464187,1.01E+18,1.01E+18,5.07E+17,2.53286E+17,0.73056256,,6.93401E+11,18212977.6,,,,,,Meta GenAI 2024a,0.020833333,https://arxiv.org/abs/2404.14219,,,,
Lawrence Livermore NL El Capitan Phase 1,Existing,Confirmed,Yes,18.00179185,507.3948459,AMD Instinct MI300A,512,United States of America,US Department of Energy,6/1/2024,,Public,0.793182208,20944115.36,"Lawrence Livermore National Laboratory
7000 East Ave, Livermore, CA 94550","US Government,Lawrence Livermore NL,Scientific Research",AMD MI300A,Calculated from Top500,6/1/2024,From Top500,11.80138889,,Lawrence Livermore NL El Capitan Phase 2,,,,,512,AMD,,TRUE,,104,17.70076185,1.00E+18,1.00E+18,5.02E+17,2.51034E+17,0.793182208,,6.32978E+11,20944115.36,,,,,,Meta GenAI 2024a,0.020645949,https://web.archive.org/web/20240406154137/https://www.hpcwire.com/2022/06/21/amds-mi300-apus-to-power-exascale-el-capitan-supercomputer/,https://web.archive.org/web/20240517002417/https://en.wikipedia.org/wiki/El_Capitan_(supercomputer),https://web.archive.org/web/20240806222453/https://www.nextplatform.com/2023/07/10/lining-up-the-el-capitan-supercomputer-against-the-ai-upstarts/,https://web.archive.org/web/20240611014231/https://top500.org/system/180283/,
Lawrence Livermore NL RZAdams,Existing,Confirmed,Yes,18.00179185,507.3948459,AMD Instinct MI300A,512,United States of America,US Department of Energy,6/1/2024,,Public,0.793182208,20944115.36,"7000 East Ave, Livermore, CA 94550","Lawrence Livermore NL,Researchers",AMD MI300A,Calculated from Top500,6/1/2024,,11.80138889,,,,,,,512,AMD,,TRUE,,104,17.70076185,1.00E+18,1.00E+18,5.02E+17,2.51034E+17,0.793182208,,6.32978E+11,20944115.36,,,,,,Meta GenAI 2024a,0.020645949,https://web.archive.org/web/20250121085639/https://top500.org/system/180284/,,,,
Ahrefs Yep1,Existing,Confirmed,Yes,17.99887633,504,NVIDIA H100 SXM5 80GB,504,United States of America,Ahrefs,6/1/2024,,Private,0.71914752,17993583.65,"Ashburn, Virginia",Ahrefs,H100,Calculated from Top500,6/1/2024,,11.84098446,,,,,,,504,NVIDIA,,TRUE,,108,17.69780244,9.97E+17,9.97E+17,4.99E+17,2.49329E+17,0.71914752,,6.93401E+11,17993583.65,,,,,,Meta GenAI 2024a,0.020507812,https://web.archive.org/web/20250212094644/https://top500.org/system/180265/,,,,
Recursion BioHive-2,Existing,Confirmed,Yes,17.99887633,504,NVIDIA H100 SXM5 80GB,504,United States of America,Recursion Pharmaceuticals,5/13/2024,,Private,0.71914752,17928399.82,"41 400 W, Salt Lake City, UT 84101","Recursion Pharmaceuticals,Pharmaceutical Research",H100,BioHive-2 is equipped with 504 Nvidia H100 Tensor Core GPUs,5/13/2024,,11.84098446,,,,,,,504,NVIDIA,,TRUE,,100,17.69780244,9.97E+17,9.97E+17,4.99E+17,2.49329E+17,0.71914752,,6.93401E+11,17928399.82,,,,,,Meta GenAI 2024a,0.020507812,https://web.archive.org/web/20240621074430/https://the-decoder.com/biohive-2-to-speed-up-drug-development-by-several-years/,,,,
Yandex Chervonenkis,Existing,Confirmed,Yes,17.99712765,501.9747347,NVIDIA A100 SXM4 80 GB,1592,Russia,Yandex,11/1/2021,,Private,1.36759168,65354449.21,"ул. Пушкина, 21, Sasovo, Ryazan Oblast, Russia, 391431",Yandex,A100,"GPU
1,592
NVIDIA A100 80G",11/1/2021,,11.56014121,,,,,,,1592,NVIDIA,,TRUE,,23,17.69609766,9.93E+17,9.93E+17,4.97E+17,2.48352E+17,1.36759168,,3.63196E+11,65354449.21,,,,,,DeepSeek Fire-Flyer 2,0.1592,https://web.archive.org/web/20231115155013/https://yandex.com/supercomputers,,,,
BNY Mellon Supercomputer,Existing,Likely,Yes,17.9954158,500,NVIDIA H100 SXM5 80GB,500,,BNY Mellon,3/18/2024,"""Dozens"" probably means at least 36 and less than 100. Multiply this by 8 since it's DGXs. I estimated around 500 GPUs for this",Private,0.71344,17843147.26,,BNY Mellon,H100,"The system, equipped with dozens of NVIDIA DGX systems and NVIDIA InfiniBand networking",3/18/2024,Very uncertain about exact number,11.84098446,,,,,,,500,NVIDIA,,TRUE,,85,17.69434191,9.90E+17,9.90E+17,4.95E+17,2.4735E+17,0.71344,,6.93401E+11,17843147.26,,,,,,Meta GenAI 2024a,0.020345052,https://web.archive.org/web/20240725023803/https://blogs.nvidia.com/blog/bny-mellon-superpod/,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,18,500,,,China,,1/15/2023,,Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,TRUE,,39,18,1.00E+18,,1.00E+18,,,,,,,,,,,Microsoft GPT-4 cluster,0.06,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,18,500,,2000,China,,12/15/2021,,Private,1,,China,,,,,,11.69897,,,TRUE,,,,2000,Anonymized,Anonymized,,,25,17.69897,1.00E+18,1.00E+18,5.00E+17,,1,,5E+11,,,,,,,DeepSeek Fire-Flyer 2,0.2,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,18,500,,400,China,,8/15/2021,,Public,0.3,10000000,China,,,,,,11.52287875,,,,,,,2000,Anonymized,Anonymized,TRUE,,17,17,1.00E+18,1.00E+18,1.00E+17,6E+16,0.3,,3.33333E+11,10000000,,,,,,DeepSeek Fire-Flyer 2,0.2,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,18,500,,40000,China,,11/15/2020,,Public,30,,China,,,,,,10.52287875,,,,,,,40000,Anonymized,Anonymized,TRUE,,8,18,1.00E+18,,1.00E+18,,30,,33333333333,,,,,,,Oak Ridge NL Summit,0.3,,,,,
Anonymized Chinese System,Planned,Confirmed,Yes,18,500,,,Hong Kong,,,,Private,,,Hong Kong,,,,,,,,,,,,,,Anonymized,Anonymized,,,,18,1.00E+18,,1.00E+18,,,,,,,,,,,,,,,,,
Saudi Aramco Dammam-7,Existing,Confirmed,Yes,17.99519629,499.7473472,NVIDIA Tesla V100 SXM2,7912,Saudi Arabia,Saudi Aramco,1/21/2021,,Private,5.09754336,164171823,"Dhahran, Saudi Arabia",Saudi Aramco,V100,Calculated from Top500,1/21/2021,Listed in Top500,11.28783536,,,,,,,7912,NVIDIA,,TRUE,,10,17.99519629,9.89E+17,,9.89E+17,1.23981E+17,5.09754336,,1.94015E+11,164171823,,,,,,Oak Ridge NL Summit,0.286168981,https://web.archive.org/web/20240823132655/https://www.hpcwire.com/2021/01/21/saudi-aramco-unveils-dammam-7-its-new-top-ten-supercomputer/,https://web.archive.org/web/20240423152513/https://www.top500.org/system/179885/,,,
Condor Galaxy 3 (CG-3),Planned,Confirmed,Yes,17.98227123,485.0934816,Cerebras CS-3,64,United States of America,"G42,Cerebras Systems",,"Third of 9 planned supercomputers that will be interconnected
Unconventional hardware
Reported FLOP/s is with sparcity
A UAE government holds a significant but unknown stake in G42, possibly qualifying this as a government project",Public/Private,,258201600,"Dallas, Texas",Cloud,Cerebras CS3,"""• 8 ExaFLOPs
• 64 x CS-3s
• 108 TB of Memory""
""CG-1 links 64 Cerebras CS-2 systems together into a single, easy-to-use AI supercomputer, with an AI training capacity of 4 exaFLOPs""
""Upon completion in 2024, the nine inter-connected supercomputers will have 36 ExaFLOPS of AI compute, making it one of the most powerful cloud AI supercomputers in the world.""",Planned,Reported on their official website,,,,,,,,64,Cerebras,,,,,17.98227123,9.60E+17,,9.60E+17,,,,,258201600,,,,,,,,https://web.archive.org/web/20240831173119/https://cerebras.ai/blog/introducing-condor-galaxy-1-a-4-exaflop-supercomputer-for-generative-ai/,https://web.archive.org/web/20240603001926/https://www.hpcwire.com/off-the-wire/cerebras-and-g42-unveil-condor-galaxy-to-accelerate-global-ai-advancement/,https://web.archive.org/web/20240827205627/https://cerebras.ai/condor-galaxy,https://web.archive.org/web/20240519195141/https://www.servethehome.com/detail-of-the-giant-cerebras-wafer-scale-cluster-nvidia/,
AGH Cyfronet Helios,Existing,Confirmed,Yes,17.96296377,464,NVIDIA GH200,440,Poland,AGH University of Krakow,6/1/2024,,Public,0.5297,21016071.52,"Nawojki 11, 30-950 Kraków, Poland","AGH University of Krakow,Researchers in Poland,Cloud","GH200,H100",The GPU partition contains 440 Nvidia GH200 Grace Hopper Superchips and the final INT partition is equipped with 24 Nvidia H100 Tensor Core GPUs and high-speed NVMe local storage.,6/1/2024,,11.93790154,,,,,NVIDIA H100 SXM5 80GB,24,464,NVIDIA,NVIDIA,TRUE,,115,17.66193151,9.18E+17,9.18E+17,4.59E+17,2.29453E+17,0.66207232,0.5297,8.66765E+11,21016071.52,,,,,,Meta GenAI 2024a,0.018880208,https://web.archive.org/web/20240514095456/https://www.datacenterdynamics.com/en/news/hpe-to-build-new-35-petaflop-supercomputer-in-poland/,"https://web.archive.org/web/20240527154516/https://www.cyfronet.pl/en/news/19869,42,komunikat,helios__athena_and_ares_-_3_supercomputers_from_cyfronet_among_the_fastest_and_most_energy-efficient_computers_in_the_world.html",https://web.archive.org/web/20240806125635/https://top500.org/system/180244/,,
Eni HPC5,Existing,Confirmed,Yes,17.95904139,459.8281961,NVIDIA V100,7280,Italy,Eni,2/21/2020,,Private,4.0337,139622268.3,"Via per la Corradina, 35, 27032 Ferrera Erbognone PV, Italy",Eni,V100,"In total, the system comprises 7,280 NVIDIA V100 GPUs.",2/21/2020,,11.3533378,,,,,,,7280,NVIDIA,,TRUE,,5,17.95904139,9.10E+17,,9.10E+17,1.14078E+17,4.769856,4.0337,2.25599E+11,139622268.3,,,,,,Oak Ridge NL Summit,0.263310185,https://web.archive.org/web/20231120074252/https://energyindustryreview.com/tech/new-supercomputing-system-hpc5/,https://web.archive.org/web/20240906185041/https://en.wikipedia.org/wiki/HPC5,,,
CSIRO Virga,Existing,Confirmed,Yes,17.94772381,448,NVIDIA H100 SXM5 80GB,448,Australia,CSIRO,7/11/2024,,Public,0.63924224,10935360.37,"54 Sheppard St, Hume ACT 2620, Australia","Researchers,Academia,Healthcare",H100,There are 448 H100 GPUs in total in Virga,7/11/2024,,11.84098446,,,,,,,448,NVIDIA,,TRUE,,130,17.64664992,8.87E+17,8.87E+17,4.43E+17,2.21626E+17,0.63924224,,6.93401E+11,15965073.63,10850000,10935360.37,"""The system actually cost AU$16.3m (US$10.85m).""",,,Meta GenAI 2024a,0.018229167,https://web.archive.org/web/20240720195802/https://blocksandfiles.com/2024/07/03/australia-dell-based-virga-ai-workload-cluster/,https://www.datacenterdynamics.com/en/news/australias-csiro-unveils-virga-supercomputer/,,,
SURF Snellius Phase 3,Existing,Confirmed,Yes,17.94266272,442.8094998,NVIDIA H100 SXM5 80GB,352,Netherlands,SURF,7/11/2024,,Public,0.73708544,18173029,"Science Park 120, 1098 XG Amsterdam, Netherlands","Academia,Researchers","H100,A100",The expansion includes the addition of 352 NVIDIA H100 (Hopper) GPUs,7/11/2024,,11.77408,SURF Snellius Phase 1,,,,NVIDIA A100 SXM4 80 GB,288,640,NVIDIA,NVIDIA,TRUE,,131,17.64159784,8.76E+17,8.76E+17,4.38E+17,2.19062E+17,0.73708544,,5.94402E+11,18173029,,,,,,Meta GenAI 2024a,0.018017965,https://web.archive.org/web/20240712185633/https://www.surf.nl/en/news/gpu-expansion-of-supercomputer-snellius-enables-even-faster-data-processing-for-dutch,https://web.archive.org/web/20240725104223/https://visualization.surf.nl/snellius-virtual-tour/#/,,,
Saudi Aramco Tuwaiq-1,Existing,Confirmed,Yes,17.91553912,416,NVIDIA H100 SXM5 80GB,416,Saudi Arabia,Saudi Aramco,6/1/2024,,Private,0.59358208,14851846.82,Saudi Arabia,Saudi Aramco,H100,Calculated from Top500,6/1/2024,,11.84098446,,,,,,,416,NVIDIA,,TRUE,,119,17.61446524,8.23E+17,8.23E+17,4.12E+17,2.05795E+17,0.59358208,,6.93401E+11,14851846.82,,,,,,Meta GenAI 2024a,0.016927083,https://web.archive.org/web/20241205003221/https://top500.org/system/180260/,,,,
Microsoft Azure Pioneer-WUS2,Existing,Confirmed,Yes,17.91311842,413.6877211,NVIDIA A100,1312,United States of America,Microsoft,6/1/2021,,Private,1.12706048,35416542.41,Washington,"Cloud,Azure",A100,Calculated from Top500,6/1/2021,Listed in Top500,11.56014121,,,,,,,1312,NVIDIA,,TRUE,,18,17.61208843,8.19E+17,8.19E+17,4.09E+17,2.04672E+17,1.12706048,,3.63196E+11,35416542.41,,,,,,Sunway OceanLight,0.137700479,https://web.archive.org/web/20241109144557/https://www.top500.org/system/179967/,,,,
Microsoft Azure Pioneer-WEU,Existing,Confirmed,Yes,17.91311842,413.6877211,NVIDIA A100,1312,Netherlands,Microsoft,6/1/2021,,Private,1.12706048,35416542.41,Netherlands,"Cloud,Azure",A100,Calculated from Top500,6/1/2021,Listed in Top500,11.56014121,,,,,,,1312,NVIDIA,,TRUE,,18,17.61208843,8.19E+17,8.19E+17,4.09E+17,2.04672E+17,1.12706048,,3.63196E+11,35416542.41,,,,,,Sunway OceanLight,0.137700479,https://web.archive.org/web/20241109142236/https://www.top500.org/system/179968/,,,,
Microsoft Azure Pioneer-EUS,Existing,Confirmed,Yes,17.91311842,413.6877211,NVIDIA A100,1312,United States of America,Microsoft,6/1/2021,,Private,1.12706048,35416542.41,Virginia,"Cloud,Azure",A100,Calculated from Top500,6/1/2021,Listed in Top500,11.56014121,,,,,,,1312,NVIDIA,,TRUE,,18,17.61208843,8.19E+17,8.19E+17,4.09E+17,2.04672E+17,1.12706048,,3.63196E+11,35416542.41,,,,,,Sunway OceanLight,0.137700479,https://web.archive.org/web/20241109163038/https://www.top500.org/system/179969/,,,,
Microsoft Azure Pioneer-SCUS,Existing,Confirmed,Yes,17.91311842,413.6877211,NVIDIA A100,1312,United States of America,Microsoft,6/1/2021,,Private,1.12706048,35416542.41,Texas,"Cloud,Azure",A100,Calculated from Top500,6/1/2021,Listed in Top500,11.56014121,,,,,,,1312,NVIDIA,,TRUE,,18,17.61208843,8.19E+17,8.19E+17,4.09E+17,2.04672E+17,1.12706048,,3.63196E+11,35416542.41,,,,,,Sunway OceanLight,0.137700479,https://web.archive.org/web/20241109090838/https://www.top500.org/system/179970/,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,17.90308999,400,,2000,China,,5/15/2023,,Public/Private,1,,China,,,,,,11.60205999,,,,,,,2000,Anonymized,Anonymized,TRUE,,49,17.60205999,8.00E+17,8.00E+17,4.00E+17,,1,,4E+11,,,,,,,Microsoft GPT-4 cluster,0.05,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,17.93989847,400,,400,Hong Kong,,4/15/2024,,Public,0.6,20000000,Hong Kong,,,,,,11.82390874,,,,,,,400,Anonymized,Anonymized,TRUE,,104,17.60205999,8.71E+17,8.71E+17,4.00E+17,1.9998E+17,0.6,,6.66667E+11,20000000,,,,,,Meta GenAI 2024a,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,17.90308999,400,,30000,China,,4/15/2022,,Public/Private,,,China,,,,,,,,,,,,,30000,Anonymized,Anonymized,TRUE,,36,17.90308999,8.00E+17,,8.00E+17,,,,,,,,,,,DeepSeek Fire-Flyer 2,0.1,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,17.90308999,400,,400,China,,8/15/2023,,,0.6,10000000,China,,,,,,11.82390874,,,,,,,400,Anonymized,Anonymized,TRUE,,63,17.60205999,8.00E+17,8.00E+17,4.00E+17,2E+17,0.6,,6.66667E+11,10000000,,,,,,Microsoft GPT-4 cluster,0.05,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,17.84509804,400,,1000,China,,6/15/2023,,,1,30000000,China,,,,,,11.60205999,,,,,,,1000,Anonymized,Anonymized,TRUE,,61,17.60205999,7.00E+17,7.00E+17,4.00E+17,2E+17,1,,4E+11,30000000,,,,,,Microsoft GPT-4 cluster,0.05,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,17.90308999,400,,,China,,3/15/2018,,Public,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,TRUE,,2,17.90308999,8.00E+17,,8.00E+17,,,,,,,,,,,Meta 2017 V100 Cluster,0.3,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,17.90308999,400,,,China,,3/15/2024,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,TRUE,,91,17.90308999,8.00E+17,,8.00E+17,,,,,,,,,,,Meta GenAI 2024a,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,17.90308999,400,,,China,,7/15/2021,,Public,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,TRUE,,24,,8.00E+17,8.00E+17,,,,,,,,,,,,DeepSeek Fire-Flyer 2,0.1,,,,,
Princeton Della Phase 2,Existing,Confirmed,Yes,17.89374401,395.6382011,NVIDIA H100 SXM5 80GB,296,United States of America,Princeton University,3/15/2024,,Public,0.68001024,16807979.65,"300 Forrestal Rd, Princeton, NJ 08544",Academia,"H100,A100",Princeton University has invested in a new cluster of 300 Nvidia H100 GPUs to support AI research at the institution.,3/15/2024,,11.76016573,Princeton Della Phase 1,,,,NVIDIA A100 SXM4 80 GB,316,612,NVIDIA,NVIDIA,TRUE,,98,17.59268118,7.83E+17,7.83E+17,3.91E+17,1.95727E+17,0.68001024,,5.7566E+11,16807979.65,,,,,,Meta GenAI 2024a,0.01609856,https://web.archive.org/web/20240320225612/https://www.datacenterdynamics.com/en/news/princeton-university-acquires-300-strong-nvidia-h100-gpu-cluster/,https://web.archive.org/web/20240521031936/https://researchcomputing.princeton.edu/systems/della,,,
Core42 AI-03,Existing,Confirmed,Yes,17.89316232,395.1086407,AMD Instinct MI210,4320,United Arab Emirates,G42,6/15/2025,,Public/Private,,,United Arab Emirates,"Cloud,G42",AMD MI210,Calculated from Top500,2025-06,,,,,,,,,4320,AMD,,TRUE,,,17.89316232,7.82E+17,,7.82E+17,,,,,,,,,,,,,,,,,
Microsoft Explorer-WUS3,Existing,Confirmed,Yes,17.8665,371.5816069,AMD Radeon Instinct MI250X,1920,United States of America,Microsoft,6/1/2023,Chip count from Top500,Private,1.991808,48843651.94,"Phoenix, Arizona","Cloud,Azure",AMD MI250X,"""The highest ranked new entry to the June 2023 Top500 list is #11 - a #Microsoft #Azure #AI supercomputer with nearly 2000 #AMD MI250X #GPUs and InfiniBand HDR interconnect: https://lnkd.in/e2WrGDGa""
""The cluster was spun up in Azure’s West US3 datacenter, and Microsoft has confirmed it is a permanent system that has or will run real HPC and AI workloads""
Chip count from Top500",6/1/2023,,11.56725253,,,,,,,1920,AMD,,TRUE,,60,17.8665,7.35E+17,7.35E+17,7.35E+17,1.83744E+17,1.991808,,3.69192E+11,48843651.94,,,,,,Microsoft GPT-4 cluster,0.047138462,https://web.archive.org/web/20240406153948/https://www.hpcwire.com/2023/05/22/top500-frontier-gains-92-petaflops-henri-gets-a-little-greener/,https://archive.ph/83Jbr,https://web.archive.org/web/20241109085342/https://www.top500.org/system/180171/,,
Naver DGX Superpod,Existing,Confirmed,Yes,17.84440261,353.1480546,NVIDIA A100,1120,Korea (Republic of),NAVER,10/5/2020,,Private,0.978432,30365543.16,,NAVER,A100,NAVER CLOVA is using its DGX SuperPOD built with 140 DGX A100,10/5/2020,,11.55284197,,,,,,,1120,NVIDIA,,TRUE,,10,17.54337262,6.99E+17,6.99E+17,3.49E+17,1.7472E+17,0.978432,,3.57143E+11,30365543.16,,,,,,Oak Ridge NL Summit,0.202222222,https://web.archive.org/web/20240825043935/https://nvidianews.nvidia.com/news/nvidia-announces-ready-made-nvidia-dgx-superpods-offered-by-global-network-of-certified-partners,,,,
University of Florida HiPerGator 3.0 Superpod,Existing,Confirmed,Yes,17.84440261,353.1480546,NVIDIA A100,1120,United States of America,University of Florida,1/31/2021,,Public,1.0261,30312641.52,"Gainesville, FL 32611",University of Florida,A100,"UF’s HiPerGator 3 supercomputer will integrate 140 NVIDIA DGX A100 systems powered by a combined 1,120 NVIDIA A100 Tensor Core GPUs",1/31/2021,,11.53218293,,,,,,,1120,NVIDIA,,TRUE,,14,17.54337262,6.99E+17,6.99E+17,3.49E+17,1.7472E+17,0.9621248,1.0261,3.40552E+11,30312641.52,,,,,,Oak Ridge NL Summit,0.202222222,https://web.archive.org/web/20240521065742/https://blogs.nvidia.com/blog/university-of-florida-nvidia-ai-supercomputer/,,,,
PCSS Poznan Proxima,Existing,Confirmed,Yes,17.83802504,348,NVIDIA H100 SXM5 80GB,348,Poland,Polish Academy of Sciences,6/1/2024,,Public,0.49655424,12424141.09,"Poznań, Poland",Academia,H100,Calculated from Top500,6/1/2024,,11.84098446,,,,,,,348,NVIDIA,,TRUE,,134,17.53695115,6.89E+17,6.89E+17,3.44E+17,1.72156E+17,0.49655424,,6.93401E+11,12424141.09,,,,,,Meta GenAI 2024a,0.014160156,https://web.archive.org/web/20250121170241/https://top500.org/system/180290/,,,,
Yandex Lyapunov,Existing,Confirmed,Yes,17.83499514,345.5805963,NVIDIA A100 SXM4 40 GB,1096,Russia,Yandex,11/1/2021,,Private,0.94150784,29515251.56,"ул. Пушкина, 21, Sasovo, Ryazan Oblast, Russia, 391431",Yandex,A100,"GPU
1,096
NVIDIA A100 40G",11/1/2021,,11.56014121,,,,,,,1096,NVIDIA,,TRUE,,35,17.53396515,6.84E+17,6.84E+17,3.42E+17,1.70976E+17,0.94150784,,3.63196E+11,29515251.56,,,,,,DeepSeek Fire-Flyer 2,0.1096,https://web.archive.org/web/20231115155013/https://yandex.com/supercomputers,,,,
Yandex Galushkin,Existing,Confirmed,Yes,17.83181349,343.0581102,NVIDIA A100 SXM4 80 GB,1088,Russia,Yandex,11/1/2021,,Private,0.93463552,44664347.2,"Ulitsa Energetikov, 34, Vladimir, Vladimir Oblast, Russia, 600902",Yandex,A100,"GPU
1,088
NVIDIA A100 80G",11/1/2021,,11.56014121,,,,,,,1088,NVIDIA,,TRUE,,36,17.53078349,6.79E+17,6.79E+17,3.39E+17,1.69728E+17,0.93463552,,3.63196E+11,44664347.2,,,,,,DeepSeek Fire-Flyer 2,0.1088,https://web.archive.org/web/20231115155013/https://yandex.com/supercomputers,,,,
NVIDIA SATURN V Phase 2,Existing,Likely,Yes,17.81954394,333.5017686,NVIDIA V100,5280,United States of America,NVIDIA,3/26/2018,"This was first mentioned November 2017, but it appears to not be operational yet at that time
A later version of the supercomputer was actually four clustesrs, but its unclear if this was",Private,3.6324288,104199705.2,,NVIDIA,V100,"If you look at our Saturn V Supercomputer add-in video which we use for our internal training, that has 5,280 GPUs",3/26/2018,,11.25934683,NVIDIA SATURN V Phase 1,NVIDIA SATURN V Phase 3,,,,,5280,NVIDIA,,TRUE,,3,17.81954394,6.60E+17,,6.60E+17,8.27376E+16,3.6324288,,1.81697E+11,104199705.2,,,,,,Meta 2017 V100 Cluster,0.24,https://web.archive.org/web/20240528104941/https://insidehpc.com/2018/04/inside-new-nvidia-dgx-2-supercomputer-nvswitch/,https://top500.org/system/178928/,,,
SK Telecom Titan Phase 2,Existing,Confirmed,Yes,17.81221793,327.9231935,NVIDIA A100,1040,Korea (Republic of),SK Telecom,2/14/2023,,Private,0.8631168,23330676.1,Korea (Republic of),SK Telecom,A100,"by increasing its capacity to 1,040 NVIDIA A100 GPUs",2/14/2023,,11.57511836,,,,,,,1040,NVIDIA,,TRUE,,58,17.51118793,6.49E+17,6.49E+17,3.24E+17,1.6224E+17,0.8631168,,3.7594E+11,23330676.1,,,,,,Microsoft GPT-4 cluster,0.0416,https://web.archive.org/web/20240725135908/https://insidehpc.com/2023/02/sk-telecom-doubles-capacity-of-titan-supercomputer/,,,,
Nemotron-3-8B training cluster,Existing,Confirmed,Yes,17.80548455,322.8782213,NVIDIA A100,1024,,NVIDIA,4/4/2024,,Private,0.83492864,20047992.4,,NVIDIA,A100,"""1,024 A100s were used for 19 days to train the model""",4/4/2024,,11.58280519,,,TRUE,NVIDIA Selene Phase 2,,,1024,NVIDIA,,,,117,17.50445455,6.39E+17,6.39E+17,3.19E+17,1.59744E+17,0.83492864,,3.82653E+11,20047992.4,,,,,,Meta GenAI 2024a,0.013137948,https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/nemotron-3-8b-base-4k,,,,
Paper on HyperCLOVA,Existing,Confirmed,Yes,17.80548455,322.8782213,NVIDIA A100,1024,Korea (Republic of),NAVER,9/10/2021,,Private,0.87965696,35020899.7,,NAVER,A100,"""Our model is based on megatron-LM (Shoeybi et al., 2019) and trained on the NVIDIA Superpod, which includes 128 strongly clustered DGX servers with 1,024 A100 GPUs.""",9/10/2021,,11.56014121,,,TRUE,Naver DGX Superpod,,,1024,NVIDIA,,,,32,17.50445455,6.39E+17,6.39E+17,3.19E+17,1.59744E+17,0.87965696,,3.63196E+11,35020899.7,,,,,,DeepSeek Fire-Flyer 2,0.1024,https://arxiv.org/abs/2109.04650,,,,
Gcore data center Phase 1,Existing,Confirmed,Yes,17.80159577,320,NVIDIA H100 SXM5 80GB,320,Korea (Republic of),"Gcore,NHN Corporation",4/15/2024,,Private,0.4566016,11422779.04,"Incheon, South Korea",Cloud,H100,"Gcore will initially have the data center equipped with 320 Nvidia H100 GPUs stacked onto 40 servers. Each server also has 2 terabytes of data storage and 112 CPU cores, with a bandwidth of 3.2 Tbps.",4/15/2024,,11.84098446,,,,,,,320,NVIDIA,,TRUE,,120,17.50052188,6.33E+17,6.33E+17,3.17E+17,1.58304E+17,0.4566016,,6.93401E+11,11422779.04,,,,,,Meta GenAI 2024a,0.013020833,https://web.archive.org/web/20240723133814/https://www.datacenterdynamics.com/en/news/gcore-expands-into-south-korea-with-nhn-cloud/,,,,
Paper on OPT,Existing,Confirmed,Yes,17.79169626,312.7882769,NVIDIA A100 SXM4 80 GB,992,,Meta AI,5/2/2022,,Private,0.84494592,22320916.85,,Meta,A100,"""We are also releasing both the logbook of our model creation as well as our codebase, metaseq,3 which enabled training OPT-175B on 992 80GB A100 GPUs,""",5/2/2022,,11.56383735,,,TRUE,Meta Research SuperCluster (RSC-1) Phase 1,,,992,NVIDIA,,,,46,17.49066627,6.19E+17,6.19E+17,3.10E+17,1.54752E+17,0.84494592,,3.663E+11,22320916.85,,,,,,Microsoft GPT-4 cluster,0.03968,https://arxiv.org/abs/2205.01068,,,,
Wroclaw Centre for Networking and Supercomputing Lem,Existing,Confirmed,Yes,17.77931938,304,NVIDIA H100 SXM5 80GB,304,Poland,Wroclaw Tech (Wrocław University of Science and Technology),6/1/2024,,Public,0.43377152,10853272.68,"plac Grunwaldzki 9, 50-366 Wrocław, Poland","Academia,Researchers",H100,Calculated from Top500,6/1/2024,,11.84098446,,,,,,,304,NVIDIA,,TRUE,,143,17.47824549,6.02E+17,6.02E+17,3.01E+17,1.50389E+17,0.43377152,,6.93401E+11,10853272.68,,,,,,Meta GenAI 2024a,0.012369792,https://web.archive.org/web/20250121115648/https://top500.org/system/180272/,,,,
AIST ABCI 2.0,Existing,Confirmed,Yes,17.77745582,302.6983325,NVIDIA A100 SXM4 40 GB,960,Japan,National Institute of Advanced Industrial Science and Technology (AIST),5/30/2021,"Builds off of ABCI 1.0, which was first operational in August 2018",Public,2.8185,115981977.9,"Kashiwa Campus, The University of Tokyo
Japan, 〒277-0882 Chiba, Kashiwa, Kashiwanoha, 5 Chome−１−5","industry,academia,and government in Japan","A100,V100","120 Compute Nodes (A) that form in total 960 NVIDIA A100 GPU accelerators, 1,088 Compute Nodes (V) that form in total 4,352 NVIDIA GPU V100 accelerators",5/30/2021,Info from detailed official spec sheet,11.47607735,AIST ABCI 1.0,AIST ABCI 3.0,,,NVIDIA Tesla V100 SXM2,4352,5312,NVIDIA,NVIDIA,TRUE,,21,17.92609538,5.99E+17,5.99E+17,8.44E+17,2.17956E+17,3.62858496,2.8185,2.9928E+11,115981977.9,,,,,,Sunway OceanLight,0.100756448,https://web.archive.org/web/20240901124715/https://abci.ai/en/about_abci/computing_resource.html,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,17.69897,300,,,China,,3/15/2024,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,TRUE,,117,17.69897,5.00E+17,,5.00E+17,,,,,,,,,,,Meta GenAI 2024a,0.01,,,,,
Anonymized Chinese System,Planned,Confirmed,Yes,17.69897,300,,,China,,,,Public/Private,,,China,,,,,,,,,,,,,500,Anonymized,Anonymized,,,,17.69897,5.00E+17,,5.00E+17,,,,,,,,,,,,,,,,,
Anonymized Chinese System,Planned,Likely,Yes,17.77815125,300,,,China,,,,Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,17.77815125,6.00E+17,,6.00E+17,,,,,,,,,,,,,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,17.77815125,300,,20000,China,,9/15/2020,,Public,20,,China,,,,,,10.47712125,,,,,,,20000,Anonymized,Anonymized,TRUE,,10,17.77815125,6.00E+17,,6.00E+17,,20,,30000000000,,,,,,,Oak Ridge NL Summit,0.2,,,,,
Anonymized Chinese System,Planned,Likely,Yes,17.69897,300,,,China,,,,Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,17.69897,5.00E+17,,5.00E+17,,,,,,,,,,,,,,,,,
Anonymized Chinese System,Planned,Confirmed,Yes,17.69897,300,,,China,,,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,17.69897,5.00E+17,,5.00E+17,,,,,,,,,,,,,,,,,
Anonymized Chinese System,Existing,Likely,Yes,17.69897,300,,,China,,12/15/2023,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,TRUE,,108,17.69897,5.00E+17,,5.00E+17,,,,,,,,,,,Microsoft Azure Eagle,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,17.69897,300,,,China,,7/15/2021,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,TRUE,,32,17.69897,5.00E+17,,5.00E+17,,,,,,,,,,,DeepSeek Fire-Flyer 2,0.08,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,17.69897,300,,2000,China,,8/15/2023,,,1,,China,,,,,,11.69897,,,,,,,2000,Anonymized,Anonymized,TRUE,,80,17.69897,5.00E+17,,5.00E+17,3E+17,1,,5E+11,,,,,,,Microsoft GPT-4 cluster,0.03,,,,,
NEC Corp Japan Supercomputer,Existing,Confirmed,Yes,17.76273257,292.6083881,NVIDIA A100 SXM4 80 GB,928,Japan,NEC Corporation,7/24/2023,,Private,0.77016576,20652087.22,Japan,NEC,A100,"116 GPU servers, each equipped with 8 NVIDIA A100 80 GB Tensor Core GPUs per node",7/24/2023,,11.57511836,,,,,,,928,NVIDIA,,TRUE,,73,17.46170257,5.79E+17,5.79E+17,2.90E+17,1.44768E+17,0.77016576,,3.7594E+11,20652087.22,,,,,,Microsoft GPT-4 cluster,0.03712,https://web.archive.org/web/20240314221209/https://www.nec.com/en/global/rd/aisupercomputer/index.html,,,,
Paper on CoCa,Existing,Confirmed,Yes,17.75066265,284.5881759,Google TPU v4,2048,United States of America,Google,5/4/2022,,Private,0.741371904,18738807.13,,Google,TPUv4,"""Pretraining CoCa takes about 5 days on 2,048 CloudTPUv4 chips""",5/4/2022,,11.88062652,,,TRUE,Paper on PaLM,,,2048,Google,,,,49,17.75066265,5.63E+17,5.63E+17,5.63E+17,,0.741371904,,7.59673E+11,18738807.13,,,,,,Microsoft GPT-4 cluster,0.036102564,https://arxiv.org/abs/2205.01917v2,,,,
AIST ABCI 1.0,Existing,Confirmed,Yes,17.7355989,274.8863062,NVIDIA Tesla V100 SXM2,4352,Japan,National Institute of Advanced Industrial Science and Technology (AIST),6/1/2018,,Public,2.99400192,177109248.9,"Kashiwa Campus, The University of Tokyo
Japan, 〒277-0882 Chiba, Kashiwa, Kashiwanoha, 5 Chome−１−5","industry,academia,and government in Japan",V100,The original ABCI system 4352 Nvidia V100 GPUs,6/1/2018,Confirmed by Top500,11.25934683,,AIST ABCI 2.0,,,,,4352,NVIDIA,,TRUE,,5,17.7355989,5.44E+17,,5.44E+17,6.81958E+16,2.99400192,,1.81697E+11,93816848.26,162000000,177109248.9,"$172 for machine, prototype, and the associated datacenter (which was $10M), subtracting out cost of datacenter gives $162",,,Meta 2017 V100 Cluster,0.197818182,https://web.archive.org/web/20240421015736/https://www.nextplatform.com/2021/07/08/inside-look-inside-japans-abci-ai-supercomputer-upgrade/,,,,
GENCI Adastra,Existing,Confirmed,Yes,17.71417547,261.6553815,AMD Radeon Instinct MI250X,1352,France,GENCI,6/1/2022,,Public,1.6092,34758841.57,"Montpellier, France","Academic research in Europe,Industry in France",AMD MI250X,Adastra contains a total of 1352 AMD MI250X on its 338 accelerated nodes.,6/1/2022,Listed in Top500,11.50756544,,,,,,,1352,AMD,,TRUE,,54,17.71417547,5.18E+17,5.18E+17,5.18E+17,1.29386E+17,1.4394744,1.6092,3.21785E+11,34758841.57,,,,,,Microsoft GPT-4 cluster,0.033193333,https://web.archive.org/web/20240411122725/https://dci.dci-gitlab.cines.fr/webextranet/architecture/index.html,,,,
MPT-30B training cluster,Existing,Confirmed,Yes,17.70468576,256,NVIDIA H100 SXM5 80GB,256,,MosaicML,6/20/2023,,,0.37180416,9220185.837,,,H100,"""Training was completed on 256 H100-80GBs""",6/20/2023,,11.83329763,,,,,,,256,NVIDIA,,TRUE,,75,17.40361187,5.07E+17,5.07E+17,2.53E+17,1.26643E+17,0.37180416,,6.81236E+11,9220185.837,,,,,,Microsoft GPT-4 cluster,0.032475897,https://huggingface.co/mosaicml/mpt-30b/commits/main,,,,
Paper on Falcon Mamba 7B,Existing,Confirmed,Yes,17.70468576,256,NVIDIA H100 SXM5 80GB,256,,Technology Innovation Institute,7/24/2024,Possibly used cloud,Public,0.36528128,9130592.914,,Technology Innovation Institute,H100,"""Falcon-Mamba-7B was trained on 256 H100 80GB GPUs""",7/24/2024,,11.84098446,,,TRUE,,,,256,NVIDIA,,,,168,17.40361187,5.07E+17,5.07E+17,2.53E+17,1.26643E+17,0.36528128,,6.93401E+11,9130592.914,,,,,,Meta GenAI 2024a,0.010416667,https://huggingface.co/tiiuae/falcon-mamba-7b/tree/main,,,,
MITRE Federal AI Sandbox,Planned,Confirmed,Yes,17.70468576,256,NVIDIA H100 SXM5 80GB,256,United States of America,MITRE,,,Public/Private,0.3587584,12032125.83,"Ashburn, Virginia","MITRE,US government agencies",H100,The organization's upcoming Federal AI Sandbox is basically an Nvidia DGX SuperPOD – a modular supercomputer built from 256 Nv H100 GPUs,Planned Q4 2024,,11.84880979,,,,,,,256,NVIDIA,,,,,17.40361187,5.07E+17,5.07E+17,2.53E+17,1.26643E+17,0.3587584,,7.06008E+11,12032125.83,,,,,,,,https://web.archive.org/web/20240927024647/https://www.theregister.com/2024/05/08/mitre_ai_computer/,,,,
Google MLPerf 0.7 Submission,Existing,Confirmed,Yes,17.70226506,254.5770591,Google TPU v3,4096,United States of America,Google,7/29/2020,,Private,1.96804608,39128671.26,,Google,TPU v3,From MLPerf 0.7,7/29/2020,,11.4082298,,,,,,,4096,Google,,TRUE,,11,17.70226506,5.04E+17,,5.04E+17,,1.96804608,,2.55994E+11,39128671.26,,,,,,Oak Ridge NL Summit,0.145777778,https://web.archive.org/web/20250217182530/https://github.com/mlcommons/training_results_v0.7/blob/master/all_hyper_params.md,,,,
Paper on Gopher,Existing,Confirmed,Yes,17.70226506,254.5770591,Google TPU v3,4096,United States of America,Google DeepMind,12/8/2021,4096 TPUV3 chips reported in table A27,Private,1.935245312,38807760.93,"Georgia, USA",Google,TPU v3,,12/8/2021,,11.41552904,,,TRUE,Google MLPerf 0.7 Submission,,,4096,Google,,,,42,17.70226506,5.04E+17,,5.04E+17,,1.935245312,,2.60333E+11,38807760.93,,,,,,DeepSeek Fire-Flyer 2,0.080738462,https://arxiv.org/abs/2112.11446,,,,
Paper on YaLM,Existing,Likely,Yes,17.69827458,252.2486104,NVIDIA A100,800,,Yandex,6/23/2022,,,0.681408,18062089.08,,,A100,"""It took us 65 days to train the model on a pool of 800 A100 graphics cards""",6/23/2022,,11.56383735,,,TRUE,Yandex Chervonenkis,,,800,NVIDIA,,,,58,17.39724458,4.99E+17,4.99E+17,2.50E+17,1.248E+17,0.681408,,3.663E+11,18062089.08,,,,,,Microsoft GPT-4 cluster,0.032,https://medium.com/yandex/yandex-publishes-yalm-100b-its-the-largest-gpt-like-neural-network-in-open-source-d1df53d0e9a6,,,,
EuroHPC MeluXina,Existing,Confirmed,Yes,17.69827458,252.2486104,NVIDIA A100,800,Luxembourg,EuroHPC JU,6/7/2021,,Public,0.6873,37543482.03,"1 ZA et Commerciale Klengbusbierg 7795, 7795 Bissen, Luxembourg","European researchers,Cloud",A100,"200 GPU nodes are part of the Accelerator Module, each featuring 2 AMD Rome CPUs (32 cores @ 2.35 GHz - 128HT cores total) and 4 NVIDIA A100-40 GPUs",6/7/2021,,11.56009824,,,,,,,800,NVIDIA,,TRUE,,30,17.39724458,4.99E+17,4.99E+17,2.50E+17,1.248E+17,0.687232,0.6873,3.6316E+11,21595452.69,35770200,37543482.03,"""MeluXina is funded via a joint investment of about EUR 30 million from the European Union and Luxembourg""",,,Sunway OceanLight,0.083963707,https://web.archive.org/web/20240406154520/https://www.hpcwire.com/off-the-wire/meluxina-named-eus-greenest-supercomputer/,https://web.archive.org/web/20241014163423/https://docs.lxp.lu/system/overview/,https://eurohpc-ju.europa.eu/meluxina-new-eurohpc-world-class-supercomputer-luxembourg-2020-09-29_en,,
KT SuperPOD,Existing,Confirmed,Yes,17.69390977,249.7261243,NVIDIA A100,792,Korea (Republic of),KT,6/1/2023,,Private,0.65729664,17686696.14,Korea (Republic of),KT,A100,Calculated from Top500,6/1/2023,,11.57511836,,,,,,,792,NVIDIA,,TRUE,,79,17.39287978,4.94E+17,4.94E+17,2.47E+17,1.23552E+17,0.65729664,,3.7594E+11,17686696.14,,,,,,Microsoft GPT-4 cluster,0.03168,https://web.archive.org/web/20241205015244/https://top500.org/system/180163/,,,,
SberCloud Christofari Neo,Existing,Confirmed,Yes,17.69390977,249.7261243,NVIDIA A100 SXM4 80 GB,792,Russia,SberCloud,11/1/2021,"SberCloud, which owns this supercomputer, was originally owned by Sber, but was sold off to Noviye Vozmozhnosti in March 2022
Sberbank is majority owned by the Russian Government.",Private,0.68035968,32513017.45,Russia,Cloud,A100,Calculated from Top500,11/1/2021,,11.56014121,,,,,,,792,NVIDIA,,TRUE,,45,17.39287978,4.94E+17,4.94E+17,2.47E+17,1.23552E+17,0.68035968,,3.63196E+11,32513017.45,,,,,,DeepSeek Fire-Flyer 2,0.0792,https://web.archive.org/web/20230517125435/https://en.wikipedia.org/wiki/Christofari,,,,
Cineca Marconi-100,Existing,Confirmed,Yes,17.69372695,249.6210207,NVIDIA V100,3952,Italy,Cineca,4/20/2020,,Public,2.6436,75928622.78,"Bologna, Italy",Academic research in Europe,V100,"Nodes: 980
Processors: 2x16 cores IBM POWER9 AC922 at 2.6(3.1) GHz
Accelerators: 4 x NVIDIA Volta V100 GPUs/node, Nvlink 2.0, 16GB",4/20/2020,,11.27153121,,,,,,,3952,NVIDIA,,TRUE,,10,17.69372695,4.94E+17,,4.94E+17,6.19278E+16,2.5893504,2.6436,1.86866E+11,75928622.78,,,,,,Oak Ridge NL Summit,0.142939815,https://web.archive.org/web/20240523132015/https://wiki.u-gov.it/confluence/pages/viewpage.action?pageId=336727645,https://web.archive.org/web/20240520090110/https://www.top500.org/system/179845/,,,
Opera Iceland KEF-1 SuperPOD,Existing,Confirmed,Yes,17.69089748,248,NVIDIA H100 SXM5 80GB,248,Iceland,Opera,2/28/2024,,Private,0.35386624,8880926.781,"Sjónarhóli 6, 260 Reykjanesbæ, Iceland",Opera,H100,Calculated from Top500,2/28/2024,,11.84098446,,,,,,,248,NVIDIA,,TRUE,,122,17.38982359,4.91E+17,4.91E+17,2.45E+17,1.22686E+17,0.35386624,,6.93401E+11,8880926.781,,,,,,Microsoft Azure Eagle,0.017222222,https://web.archive.org/web/20240705020431/https://press.opera.com/2024/02/07/opera-deploying-green-powered-ai-data-cluster-in-iceland/,,,,
Fastweb NeXXt AI Factory,Existing,Confirmed,Yes,17.69089748,248,NVIDIA H100 SXM5 80GB,248,Italy,Fastweb,7/9/2024,31x8=248,Private,0.35386624,8837808.614,"Via di San Clemente, 53 24036 Ponte San Pietro Bergamo, Italy","Cloud,Fastweb",H100,Fastweb entered an agreement with NVIDIA to acquire 31 NVIDIA DGX H100 systems,7/9/2024,,11.84098446,,,,,,,248,NVIDIA,,TRUE,,177,17.38982359,4.91E+17,4.91E+17,2.45E+17,1.22686E+17,0.35386624,,6.93401E+11,8837808.614,,,,,,Meta GenAI 2024a,0.010091146,https://web.archive.org/web/20240714211301/https://www.hpcwire.com/off-the-wire/fastweb-unveils-italys-1st-nvidia-dgx-superpod-ai-supercomputer/,,,,
Condor Galaxy 9 (CG-9),Planned,Likely,Yes,17.68124124,242.5467408,Cerebras CS-2,64,,"G42,Cerebras Systems",,"Ninth of 9 planned supercomputers that will be interconnected
No news on this since the original announcement of the plan. Unclear if they will use CS2 or CS3. They originally planned to use CS2 for CG2, but ended up using CS3, suggesting they might also use CS3 for this
Unconventional hardware
Reported FLOP/s is with sparcity
A UAE government holds a significant but unknown stake in G42, possibly qualifying this as a government project",Public/Private,,221766010.2,"Undisclosed, could be anywhere ""across the globe world""",Cloud,Cerebras CS2,"""CG-1 links 64 Cerebras CS-2 systems together into a single, easy-to-use AI supercomputer, with an AI training capacity of 4 exaFLOPs""
""Upon completion in 2024, the nine inter-connected supercomputers will have 36 ExaFLOPS of AI compute, making it one of the most powerful cloud AI supercomputers in the world.""",Planned,No news on this since original announcement of plan. Unclear if they will use CS2 or CS3,,,,,,,,64,Cerebras,,,,,17.68124124,4.80E+17,,4.80E+17,,,,,221766010.2,,,,,,,,https://web.archive.org/web/20240831173119/https://cerebras.ai/blog/introducing-condor-galaxy-1-a-4-exaflop-supercomputer-for-generative-ai/,https://web.archive.org/web/20240603001926/https://www.hpcwire.com/off-the-wire/cerebras-and-g42-unveil-condor-galaxy-to-accelerate-global-ai-advancement/,https://web.archive.org/web/20240827205627/https://cerebras.ai/condor-galaxy,https://web.archive.org/web/20240519195141/https://www.servethehome.com/detail-of-the-giant-cerebras-wafer-scale-cluster-nvidia/,
Condor Galaxy 8 (CG-8),Planned,Likely,Yes,17.68124124,242.5467408,Cerebras CS-2,64,,"G42,Cerebras Systems",,"Eighth of 9 planned supercomputers that will be interconnected
No news on this since the original announcement of the plan. Unclear if they will use CS2 or CS3. They originally planned to use CS2 for CG2, but ended up using CS3, suggesting they might also use CS3 for this
Unconventional hardware
Reported FLOP/s is with sparcity
A UAE government holds a significant but unknown stake in G42, possibly qualifying this as a government project",Public/Private,,221766010.2,"Undisclosed, could be anywhere ""across the globe world""",Cloud,Cerebras CS2,"""CG-1 links 64 Cerebras CS-2 systems together into a single, easy-to-use AI supercomputer, with an AI training capacity of 4 exaFLOPs""
""Upon completion in 2024, the nine inter-connected supercomputers will have 36 ExaFLOPS of AI compute, making it one of the most powerful cloud AI supercomputers in the world.""",Planned,No news on this since original announcement of plan. Unclear if they will use CS2 or CS3,,,,,,,,64,Cerebras,,,,,17.68124124,4.80E+17,,4.80E+17,,,,,221766010.2,,,,,,,,https://web.archive.org/web/20240831173119/https://cerebras.ai/blog/introducing-condor-galaxy-1-a-4-exaflop-supercomputer-for-generative-ai/,https://web.archive.org/web/20240603001926/https://www.hpcwire.com/off-the-wire/cerebras-and-g42-unveil-condor-galaxy-to-accelerate-global-ai-advancement/,https://web.archive.org/web/20240827205627/https://cerebras.ai/condor-galaxy,https://web.archive.org/web/20240519195141/https://www.servethehome.com/detail-of-the-giant-cerebras-wafer-scale-cluster-nvidia/,
Condor Galaxy 1 (CG-1),Existing,Confirmed,Yes,17.68124124,242.5467408,Cerebras CS-2,64,United States of America,"G42,Cerebras Systems",11/13/2023,"First of 9 planned supercomputers that will be interconnected
Unconventional hardware
Reported FLOP/s is with sparcity
A UAE government holds a significant but unknown stake in G42, possibly qualifying this as a government project",Public/Private,,223267457.5,"Colovore LLC Data Center
1101 Space Park Dr, Santa Clara, CA 95054",Cloud,Cerebras CS2,"CG-1 links 64 Cerebras CS-2 systems together into a single, easy-to-use AI supercomputer, with an AI training capacity of 4 exaFLOPs",11/13/2023,Reported on their official website,,,,,,,,64,Cerebras,,TRUE,,107,17.68124124,4.80E+17,,4.80E+17,,,,,223267457.5,,,,,,NVIDIA CoreWeave Eos-DFW Phase 1,0.022558291,https://web.archive.org/web/20240831173119/https://cerebras.ai/blog/introducing-condor-galaxy-1-a-4-exaflop-supercomputer-for-generative-ai/,https://web.archive.org/web/20240603001926/https://www.hpcwire.com/off-the-wire/cerebras-and-g42-unveil-condor-galaxy-to-accelerate-global-ai-advancement/,https://web.archive.org/web/20240827205627/https://cerebras.ai/condor-galaxy,,https://web.archive.org/web/20240519195141/https://www.servethehome.com/detail-of-the-giant-cerebras-wafer-scale-cluster-nvidia/
Condor Galaxy 5 (CG-5),Planned,Likely,Yes,17.68124124,242.5467408,Cerebras CS-2,64,,"G42,Cerebras Systems",,"Fifth of 9 planned supercomputers that will be interconnected
No news on this since the original announcement of the plan. Unclear if they will use CS2 or CS3. They originally planned to use CS2 for CG2, but ended up using CS3, suggesting they might also use CS3 for this
Unconventional hardware
Reported FLOP/s is with sparcity
A UAE government holds a significant but unknown stake in G42, possibly qualifying this as a government project",Public/Private,,221766010.2,"Undisclosed, could be anywhere ""across the globe world""",Cloud,Cerebras CS2,"""CG-1 links 64 Cerebras CS-2 systems together into a single, easy-to-use AI supercomputer, with an AI training capacity of 4 exaFLOPs""
""Upon completion in 2024, the nine inter-connected supercomputers will have 36 ExaFLOPS of AI compute, making it one of the most powerful cloud AI supercomputers in the world.""",Planned,No news on this since original announcement of plan. Unclear if they will use CS2 or CS3,,,,,,,,64,Cerebras,,,,,17.68124124,4.80E+17,,4.80E+17,,,,,221766010.2,,,,,,,,https://web.archive.org/web/20240831173119/https://cerebras.ai/blog/introducing-condor-galaxy-1-a-4-exaflop-supercomputer-for-generative-ai/,https://web.archive.org/web/20240603001926/https://www.hpcwire.com/off-the-wire/cerebras-and-g42-unveil-condor-galaxy-to-accelerate-global-ai-advancement/,https://web.archive.org/web/20240827205627/https://cerebras.ai/condor-galaxy,https://web.archive.org/web/20240519195141/https://www.servethehome.com/detail-of-the-giant-cerebras-wafer-scale-cluster-nvidia/,
Condor Galaxy 7 (CG-7),Planned,Likely,Yes,17.68124124,242.5467408,Cerebras CS-2,64,,"G42,Cerebras Systems",,"Seventh of 9 planned supercomputers that will be interconnected
No news on this since the original announcement of the plan. Unclear if they will use CS2 or CS3. They originally planned to use CS2 for CG2, but ended up using CS3, suggesting they might also use CS3 for this
Unconventional hardware
Reported FLOP/s is with sparcity
A UAE government holds a significant but unknown stake in G42, possibly qualifying this as a government project",Public/Private,,221766010.2,"Undisclosed, could be anywhere ""across the globe world""",Cloud,Cerebras CS2,"""CG-1 links 64 Cerebras CS-2 systems together into a single, easy-to-use AI supercomputer, with an AI training capacity of 4 exaFLOPs""
""Upon completion in 2024, the nine inter-connected supercomputers will have 36 ExaFLOPS of AI compute, making it one of the most powerful cloud AI supercomputers in the world.""",Planned,No news on this since original announcement of plan. Unclear if they will use CS2 or CS3,,,,,,,,64,Cerebras,,,,,17.68124124,4.80E+17,,4.80E+17,,,,,221766010.2,,,,,,,,https://web.archive.org/web/20240831173119/https://cerebras.ai/blog/introducing-condor-galaxy-1-a-4-exaflop-supercomputer-for-generative-ai/,https://web.archive.org/web/20240603001926/https://www.hpcwire.com/off-the-wire/cerebras-and-g42-unveil-condor-galaxy-to-accelerate-global-ai-advancement/,https://web.archive.org/web/20240827205627/https://cerebras.ai/condor-galaxy,https://web.archive.org/web/20240519195141/https://www.servethehome.com/detail-of-the-giant-cerebras-wafer-scale-cluster-nvidia/,
Condor Galaxy 4 (CG-4),Planned,Likely,Yes,17.68124124,242.5467408,Cerebras CS-2,64,,"G42,Cerebras Systems",,"Fourth of 9 planned supercomputers that will be interconnected
No news on this since the original announcement of the plan. Unclear if they will use CS2 or CS3. They originally planned to use CS2 for CG2, but ended up using CS3, suggesting they might also use CS3 for this
Unconventional hardware
Reported FLOP/s is with sparcity
A UAE government holds a significant but unknown stake in G42, possibly qualifying this as a government project",Public/Private,,221766010.2,"Undisclosed, could be anywhere ""across the globe world""",Cloud,Cerebras CS2,"""CG-1 links 64 Cerebras CS-2 systems together into a single, easy-to-use AI supercomputer, with an AI training capacity of 4 exaFLOPs""
""Upon completion in 2024, the nine inter-connected supercomputers will have 36 ExaFLOPS of AI compute, making it one of the most powerful cloud AI supercomputers in the world.""",Planned,No news on this since original announcement of plan. Unclear if they will use CS2 or CS3,,,,,,,,64,Cerebras,,,,,17.68124124,4.80E+17,,4.80E+17,,,,,221766010.2,,,,,,,,https://web.archive.org/web/20240831173119/https://cerebras.ai/blog/introducing-condor-galaxy-1-a-4-exaflop-supercomputer-for-generative-ai/,https://web.archive.org/web/20240603001926/https://www.hpcwire.com/off-the-wire/cerebras-and-g42-unveil-condor-galaxy-to-accelerate-global-ai-advancement/,https://web.archive.org/web/20240827205627/https://cerebras.ai/condor-galaxy,https://web.archive.org/web/20240519195141/https://www.servethehome.com/detail-of-the-giant-cerebras-wafer-scale-cluster-nvidia/,
Condor Galaxy 6 (CG-6),Planned,Likely,Yes,17.68124124,242.5467408,Cerebras CS-2,64,,"G42,Cerebras Systems",,"Sixth of 9 planned supercomputers that will be interconnected
No news on this since the original announcement of the plan. Unclear if they will use CS2 or CS3. They originally planned to use CS2 for CG2, but ended up using CS3, suggesting they might also use CS3 for this
Unconventional hardware
Reported FLOP/s is with sparcity
A UAE government holds a significant but unknown stake in G42, possibly qualifying this as a government project",Public/Private,,221766010.2,"Undisclosed, could be anywhere ""across the globe world""",Cloud,Cerebras CS2,"""CG-1 links 64 Cerebras CS-2 systems together into a single, easy-to-use AI supercomputer, with an AI training capacity of 4 exaFLOPs""
""Upon completion in 2024, the nine inter-connected supercomputers will have 36 ExaFLOPS of AI compute, making it one of the most powerful cloud AI supercomputers in the world.""",Planned,No news on this since original announcement of plan. Unclear if they will use CS2 or CS3,,,,,,,,64,Cerebras,,,,,17.68124124,4.80E+17,,4.80E+17,,,,,221766010.2,,,,,,,,https://web.archive.org/web/20240831173119/https://cerebras.ai/blog/introducing-condor-galaxy-1-a-4-exaflop-supercomputer-for-generative-ai/,https://web.archive.org/web/20240603001926/https://www.hpcwire.com/off-the-wire/cerebras-and-g42-unveil-condor-galaxy-to-accelerate-global-ai-advancement/,https://web.archive.org/web/20240827205627/https://cerebras.ai/condor-galaxy,https://web.archive.org/web/20240519195141/https://www.servethehome.com/detail-of-the-giant-cerebras-wafer-scale-cluster-nvidia/,
FZJ JURECA,Existing,Confirmed,Yes,17.68054581,242.158666,NVIDIA A100 SXM4 40 GB,768,Germany,Julich Supercomputing Center,6/23/2021,,Public,0.6765,20682757.87,"52428 Jülich, Germany","Academia,Researchers",A100,"In total, 192 of the 768 nodes are equipped with four NVIDIA A100 graphics processing units (GPUs)",6/23/2021,,11.54924801,,,,,,,768,NVIDIA,,TRUE,,33,17.37951581,4.79E+17,4.79E+17,2.40E+17,1.19808E+17,0.65974272,0.6765,3.542E+11,20682757.87,,,,,,Sunway OceanLight,0.080605159,https://web.archive.org/web/20231211004104/https://www.hpcwire.com/off-the-wire/julich-upgrades-jureca-supercomputer-for-large-data-volumes/,,,,
Paper on GLM-130B,Existing,Confirmed,Yes,17.68054581,242.158666,NVIDIA A100,768,,Zhipu AI,10/5/2022,,Public/Private,0.65415168,17354466.6,,,A100,"""Specifically, GLM-130B is a bilingual (English and Chinese) bidirectional dense model with 130 billion parameters, pre-trained over 400 billion tokens on a cluster of 96 NVIDIA DGX-A100 (8×40G) GPU nodes""",10/5/2022,,11.56383735,,,TRUE,,,,768,NVIDIA,,,,69,17.37951581,4.79E+17,4.79E+17,2.40E+17,1.19808E+17,0.65415168,,3.663E+11,17354466.6,,,,,,Microsoft GPT-4 cluster,0.03072,https://arxiv.org/abs/2210.02414,,,,
Max-Planck-Gesellschaft Raven,Existing,Confirmed,Yes,17.68054581,242.158666,NVIDIA A100,768,Germany,Max Planck Society,6/1/2021,,Public,0.664,20731634.58,"Gießenbachstraße 2, 85748 Garching bei München, Germany","Academia,Max Planck Society",A100,"is creating with Lenovo a system called Raven-GPU, powered by 768 NVIDIA A100 GPUs.",6/1/2021,,11.55734773,,,,,,,768,NVIDIA,,TRUE,,31,17.37951581,4.79E+17,4.79E+17,2.40E+17,1.19808E+17,0.65974272,0.664,3.60867E+11,20731634.58,,,,,,Sunway OceanLight,0.080605159,https://web.archive.org/web/20240520113356/https://www.datacenterdynamics.com/en/news/atos-to-build-215m-supercomputer-for-german-science-and-technology-organization/,https://web.archive.org/web/20241212234331/https://blogs.nvidia.com/blog/hpc-supercomputers-a100-gpus/,,,
Paper on xTrimoPGLM,Existing,Confirmed,Yes,17.68054581,242.158666,NVIDIA A100,768,,BioMap Research,7/6/2023,,Public/Private,0.63737856,17108567.05,,,A100,"xTrimoPGLM-100B is trained on a cluster of 96 DGX-A100 GPU (8×40G) servers. Calculation - 96 servers×8 GPUs/server= 768 GPUs
​
",7/6/2023,,11.57511836,,,TRUE,,,,768,NVIDIA,,,,87,17.37951581,4.79E+17,4.79E+17,2.40E+17,1.19808E+17,0.63737856,,3.7594E+11,17108567.05,,,,,,Microsoft GPT-4 cluster,0.03072,https://www.biorxiv.org/content/10.1101/2023.07.05.547496v4.article-info,,,,
NSC Berzelius Phase 2,Existing,Confirmed,Yes,17.67140243,237.1136938,NVIDIA A100 SXM4 80 GB,752,Sweden,Linköping University,6/1/2023,,Public,0.62409984,16793428.66,"583 30 Linköping, Sweden",Swedish academic research groups,"A100,752 A100s (480 40GB, 272 80GB)","addition will bring 34 additional nodes to the SuperPod, also based on the DGX A100 structure; this time, though, the A100s in the systems will be the 80GB variants rather than the 40GB GPUs in the remainder of the system",6/1/2023,,11.57511836,NSC Berzelius Phase 1,,,,,,752,NVIDIA,,TRUE,,87,17.37037243,4.69E+17,4.69E+17,2.35E+17,1.17312E+17,0.62409984,,3.7594E+11,16793428.66,,,,,,Microsoft GPT-4 cluster,0.03008,https://web.archive.org/web/20230712130753/https://www.hpcwire.com/2023/01/26/sweden-plans-expansion-for-nvidia-powered-berzelius-supercomputer/,,,,
Karlsruher Institut für Technologie HoreKa,Existing,Confirmed,Yes,17.66441631,233.3299646,NVIDIA A100,740,Germany,Karlsruhe Institute of Technology,6/1/2021,,Public,0.6319,19975793.74,"Karlsruhe, Germany",Academia,A100,to build a new 17-petaflops system that will pack 740 A100 GPUs on an NVIDIA Mellanox 200 Gbit/s InfiniBand network,6/1/2021,,11.56273796,,,,,,,740,NVIDIA,,TRUE,,32,17.36338631,4.62E+17,4.62E+17,2.31E+17,1.1544E+17,0.6356896,0.6319,3.65374E+11,19975793.74,,,,,,Sunway OceanLight,0.077666429,https://web.archive.org/web/20210816050117/https://www.datacenterdynamics.com/en/news/horeka-hybrid-supercomputer-inaugurated-at-karlsruhe-institute-of-technology/,https://web.archive.org/web/20241212234331/https://blogs.nvidia.com/blog/hpc-supercomputers-a100-gpus/,,,
Paper on AlphaZero,Existing,Likely,Yes,17.66275783,232.4406266,Google TPU v1,5000,United States of America,Google,12/5/2017,,Private,0.880425,51389012.91,,Google,TPU v1,"Training proceeded
for 700,000 steps (mini-batches of size 4,096) starting from randomly initialised parameters,
using 5,000 first-generation TPUs (15) to generate self-play games",12/5/2017,,,,,,,,,5000,Google,,TRUE,,2,,4.60E+17,4.60E+17,,,0.880425,,,51389012.91,,,,,,Meta 2017 V100 Cluster,0.167272727,https://arxiv.org/pdf/1712.01815,,,,
KT Internal MI250 Cluster,Existing,Confirmed,Yes,17.66238002,232.2385043,AMD Radeon Instinct MI250X,1200,Korea (Republic of),KT,12/15/2022,,Private,1.27764,30794896.33,Korea (Republic of),KT,AMD MI250X,"1,200 MI250s for KT's internal LLM development",12/15/2022,,11.55597152,,,,,,,1200,AMD,,TRUE,,77,17.66238002,4.60E+17,4.60E+17,4.60E+17,1.1484E+17,1.27764,,3.59726E+11,30794896.33,,,,,,Microsoft GPT-4 cluster,0.029461538,https://web.archive.org/web/20241005005645/https://moreh.io/blog/training-221b-parameter-korean-llm-on-1200-amd-mi250-gpu-cluster-230814,,,,
Petrobras Gaia,Existing,Confirmed,Yes,17.64275725,221.9787772,NVIDIA A100,704,Brazil,Petrobras,6/1/2023,,Public/Private,0.9767,15721507.68,"Rio de Janeiro, Brazil",Petrobras,A100,Calculated from Top500,6/1/2023,,11.35196607,,,,,,,704,NVIDIA,,TRUE,,91,17.34172725,4.39E+17,4.39E+17,2.20E+17,1.09824E+17,0.58426368,0.9767,2.24888E+11,15721507.68,,,,,,Microsoft GPT-4 cluster,0.02816,https://web.archive.org/web/20240813050918/https://www.top500.org/system/180173/,,,,
NSTDA Supercomputer Center (ThaiSC) LANTA,Existing,Confirmed,Yes,17.64275725,221.9787772,NVIDIA A100,704,Thailand,National Science and Technology Development Agency (NSTDA),11/1/2022,,Public,0.5421,15907443.03,"Pathum Thani, Thailand","Academia,NSTDA,Thai Government",A100,"With 704 A100 GPUs, the new public high performance computing system will be Southeast Asia’s biggest.",11/1/2022,,11.60764785,,,,,,,704,NVIDIA,,TRUE,,76,17.34172725,4.39E+17,4.39E+17,2.20E+17,1.09824E+17,0.59963904,0.5421,4.0518E+11,15907443.03,,,,,,Microsoft GPT-4 cluster,0.02816,https://web.archive.org/web/20240523025236/https://www.nvidia.com/en-sg/news/nvidia-powers-thailand-research-agency-new-supercomputer-with-region-largest-gpu-cluster/,,,,
TotalEnergies Pangea III,Existing,Confirmed,Yes,17.62634037,213.7443153,NVIDIA V100,3384,France,TotalEnergies,6/18/2019,,Private,2.4892,66661774.26,"Av. Larribau, 64000 Pau, France",TotalEnergies,V100,Calculated from Top500,6/18/2019,Listed in Top500,11.23028058,,,,,,,3384,NVIDIA,,TRUE,,8,17.62634037,4.23E+17,,4.23E+17,5.30273E+16,2.25415008,2.4892,1.69934E+11,66661774.26,,,,,,Oak Ridge NL Summit,0.122395833,https://web.archive.org/web/20240518130419/https://www.top500.org/system/179689/,,,,
Paper on Flamingo ,Existing,Confirmed,Yes,17.62572391,213.4411319,Google TPU v4,1536,United States of America,Google,4/29/2022,,Private,0.556028928,14054105.35,,Google,TPUv4,"""All training and evaluation was performed on TPUv4 instances. The
largest model containing 80 billion parameters is trained on 1536 chips""",4/29/2022,,11.88062652,,,TRUE,Paper on PaLM,,,1536,Google,,,,60,17.62572391,4.22E+17,4.22E+17,4.22E+17,,0.556028928,,7.59673E+11,14054105.35,,,,,,DeepSeek Fire-Flyer 2,0.067692308,https://arxiv.org/abs/2204.14198,,,,
University of Illinois NCSA Delta,Existing,Confirmed,Yes,17.62246271,211.8443658,NVIDIA A100,480,United States of America,University of Illinois Urbana-Champaign (UIUC),10/11/2022,,Public,0.6643728,10353794.2,"1205 W Clark St, Urbana, IL 61801",University of Illinois,"A100,A40","100 quad A100 GPU nodes consisting of: Four NVIDIA A100 GPUs with 40 GB HBM2 RAM and NVLink
Five eight-way A100 GPU nodes consisting of: Eight NVIDIA A100 GPUs with 40 GB HBM2 RAM and NVLink
100 quad A40 GPU nodes consisting of: Four NVIDIA A40 GPUs with 48 GB GDDR6 RAM",10/11/2022,,11.49906231,,,,,NVIDIA A40 PCIe,400,880,NVIDIA,NVIDIA,TRUE,,76,17.32147415,4.19E+17,4.19E+17,2.10E+17,1.048E+17,0.6643728,,3.15546E+11,14995688.72,10000000,10353794.2,"The original Delta machine, which we profiled here and which cost $10 million as well",,,Microsoft GPT-4 cluster,0.026874359,https://web.archive.org/web/20240503170712/https://www.datacenterdynamics.com/en/news/ncsa-announces-delta-supercomputer-is-ready-for-operations/,https://web.archive.org/web/20240927183658/https://www.ncsa.illinois.edu/research/project-highlights/delta/,"https://www.nextplatform.com/2023/07/11/ncsa-builds-out-delta-supercomputer-with-an-ai-extension/#:~:text=The%20original%20Delta%20machine%2C%20which,and%20now%20controlled%20by%20HPE.",,
AIRAWAT-PSAI Phase 2,Existing,Confirmed,Yes,17.61208843,206.8438605,NVIDIA A100,656,India,Center for Development of Advanced Computing (C-DAC),5/23/2023,This consists of an original portion of 336 GPUs (PARAM Siddhi - AI) and a newer portion of 320 GPUs (AIRAWAT PoC),Public,0.54442752,14649586.7,"Innovation Park, 34/B/1, Panchawati Rd, Mansarovar, Panchawati, Pashan, Pune, Maharashtra 411008, India",Cloud,A100,656 (82 nodes * 8 gpus per node),5/23/2023,,11.57511836,PARAM Siddhi-AI Phase 1,,,,,,656,NVIDIA,,TRUE,,91,17.31105843,4.09E+17,4.09E+17,2.05E+17,1.02336E+17,0.54442752,,3.7594E+11,14649586.7,,,,,,Microsoft GPT-4 cluster,0.02624,https://drive.google.com/file/d/1bLDkmBo4dklKX5Psch-ifNpzNDcbELXA/view?usp=sharing,https://web.archive.org/web/20240720151011/https://cdac.in/index.aspx?id=hpc_nsf_siddhi-spec,,,
MSU-270,Existing,Likely,Yes,17.60205999,202.122284,,,Russia,Moscow State Univeristy,9/2/2023,,Public,,,"Moscow, Russia","Academia,Researchers",,with a peak computational power of 400 'AI' PetaFLOPS,9/2/2023,,,,,,,,,100,Unknown,,TRUE,,109,17.60205999,4.00E+17,,4.00E+17,,,,,,,,,,,Tesla 10k H100 Cluster,0.020212228,https://web.archive.org/web/20240909123929/https://www.tomshardware.com/news/russian-400-petaflops-supercomputer-for-ai-comes-online,,,,
NVIDIA Cambridge-1,Existing,Confirmed,Yes,17.60136456,201.7988883,NVIDIA A100,640,United Kingdom of Great Britain and Northern Ireland,NVIDIA,6/1/2021,,Private,0.5497856,54262990.44,"London Rd, Harlow CM17 9NA, United Kingdom","Biomedical scientific community,including companies and the UK gov,NVIDIA",A100,"The Cambridge-A1 consists of 80 DGX A100 systems connected by Infiniband networking. Each DGX A100 has 8 A100 GPUs, 320GB GPU RAM, two AMD EPYC 7742 CPUs (64 cores each), up to 2TB system RAM, up to 30TB NVME data cache drives, and 2 1.92TB NVME SSDs.",6/1/2021,Reported by official NVIDIA post,11.56014121,,,,,,,640,NVIDIA,,TRUE,,35,17.30033457,3.99E+17,3.99E+17,2.00E+17,9.984E+16,0.5497856,,3.63196E+11,17276362.15,51700000,54262990.44,NVIDIA will invest around £40 million ($51.7 million) in Cambridge-1.,,,Sunway OceanLight,0.067170966,https://web.archive.org/web/20240617042217/https://www.theregister.com/2021/07/07/nvidia_launches_cambridge1_supercomputer_in/,https://nvidianews.nvidia.com/news/nvidia-building-uks-most-powerful-supercomputer-dedicated-to-ai-research-in-healthcare,,,
Calcul Québec Narval,Existing,Confirmed,Yes,17.59864171,200.5376453,NVIDIA A100,636,Canada,Compute Canada,11/1/2021,From Top500,Public,0.548,21713933.65,"1100 Notre-Dame St W, Montreal, Quebec H3C 1K3, Canada",Academia,A100,it features more than 632 state-of-the-art graphics processing units (GPUs) that are particularly well-suited to work in artificial intelligence.,11/1/2021,In Top500,11.55883115,,,,,,,636,NVIDIA,,TRUE,,54,17.29761171,3.97E+17,3.97E+17,1.98E+17,9.9216E+16,0.54634944,0.548,3.62102E+11,21713933.65,,,,,,DeepSeek Fire-Flyer 2,0.0636,https://web.archive.org/web/20240714041506/https://www.calculquebec.ca/en/nouvelle/narval-a-new-supercomputer-dedicated-to-scientific-research/,,,,
Lawrence Livermore NL Lassen Phase 2,Existing,Confirmed,Yes,17.59769519,200.1010611,NVIDIA V100,3168,United States of America,US Department of Energy,8/19/2020,,Public,2.0756736,62708285.24,"7000 East Ave, Livermore, CA 94550",Lawrence Livermore NL,"V100,CS-1","""At Livermore, the Cerebras CS-1 machine was recently integrated into the National Nuclear Security Administration’s Lassen supercomputer, the unclassified companion system to Sierra.""
V100 count is from Top500",8/19/2020,Listed in Top500,11.28053612,Lawrence Livermore NL Lassen Phase 1,,,,Cerebras CS-1,1,3169,NVIDIA,Cerebras,TRUE,,15,17.59769519,3.96E+17,,3.96E+17,4.96426E+16,2.0756736,,1.90781E+11,62708285.24,,,,,,Oak Ridge NL Summit,0.114583333,https://web.archive.org/web/20240327161123/https://www.hpcwire.com/2020/08/19/cerebras-llnl-deployment-teases-second-gen-wafer-scale-ai-chip/,https://web.archive.org/web/20240613070121/https://www.top500.org/system/179567/,,,
Anonymized Chinese System,Existing,Confirmed,Yes,17.47712125,200,,1000,China,,7/15/2023,,Private,0.5,,China,,,,,,11.77815125,,,,,,,1000,Anonymized,Anonymized,TRUE,,117,17.47712125,3.00E+17,,3.00E+17,2E+17,0.5,,6E+11,,,,,,,Microsoft GPT-4 cluster,0.02,,,,,
Anonymized Chinese System,Planned,Confirmed,Yes,17.47712125,200,,,China,,,,Public,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,17.47712125,3.00E+17,,3.00E+17,,,,,,,,,,,,,,,,,
Anonymized Chinese System,Planned,Likely,Yes,17.47712125,200,,,China,,,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,17.47712125,3.00E+17,,3.00E+17,,,,,,,,,,,,,,,,,
Anonymized Chinese System,Planned,Likely,Yes,17.60205999,200,,,China,,,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,17.60205999,4.00E+17,,4.00E+17,,,,,,,,,,,,,,,,,
Anonymized Chinese System,Planned,Likely,Yes,17.47712125,200,,,China,,,,Public,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,17.47712125,3.00E+17,,3.00E+17,,,,,,,,,,,,,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,17.47712125,200,,,China,,6/15/2023,,Public,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,TRUE,,114,17.47712125,3.00E+17,,3.00E+17,,,,,,,,,,,Microsoft GPT-4 cluster,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,17.47712125,200,,,China,,4/15/2023,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,TRUE,,93,17.47712125,3.00E+17,,3.00E+17,,,,,,,,,,,Microsoft GPT-4 cluster,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,17.47712125,200,,500,China,,9/15/2023,,Private,0.4,10000000,China,,,,,,11.69897,,,,,,,500,Anonymized,Anonymized,TRUE,,119,17.30103,3.00E+17,3.00E+17,2.00E+17,8E+16,0.4,,5E+11,10000000,,,,,,Tesla 10k H100 Cluster,0.02,,,,,
Anonymized Chinese System,Planned,Confirmed,Yes,17.60205999,200,,,China,,,,Public,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,17.60205999,4.00E+17,,4.00E+17,,,,,,,,,,,,,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,17.69897,200,,1000,China,,5/15/2022,,Public/Private,0.6,,China,,,,,,11.52287875,,,,,,,1000,Anonymized,Anonymized,TRUE,,58,17.30103,5.00E+17,5.00E+17,2.00E+17,,0.6,,3.33333E+11,,,,,,,Microsoft GPT-4 cluster,0.03,,,,,
Anonymized Chinese System,Planned,Confirmed,Yes,17.60205999,200,,,China,,,,Public,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,17.60205999,4.00E+17,,4.00E+17,,,,,,,,,,,,,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,17.47712125,200,,,China,,5/15/2022,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,TRUE,,73,17.47712125,3.00E+17,,3.00E+17,,,,,,,,,,,Microsoft GPT-4 cluster,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,17.69897,200,,1000,China,,9/15/2021,,Public/Private,0.6,,China,,,,,,11.69897,,,,,,,1000,Anonymized,Anonymized,TRUE,,39,17.47712125,5.00E+17,5.00E+17,3.00E+17,,0.6,,5E+11,,,,,,,DeepSeek Fire-Flyer 2,0.08,,,,,
hessian.AI fortytwo,Existing,Confirmed,Yes,17.59590167,199.2764022,NVIDIA A100,632,Germany,hessian.AI,6/15/2023,,Public,0.52450944,15850702.07,"Planckstraße 1, 64291 Darmstadt, Germany","Academia,Industry",A100,"In fortytwo‘s engine room, a total of 632 NVIDIA A100 Tensor Core GPUs, each with 80GB of memory, as well as four IPU units from Graphcore are performing their duties",6/15/2023,,11.57511836,,,,,,,632,NVIDIA,,TRUE,,101,17.29487167,3.94E+17,3.94E+17,1.97E+17,9.8592E+16,0.52450944,,3.7594E+11,14113626.21,15490930,15850702.07,"""The 14.5 million project"" presumably this is Euros",,,Microsoft GPT-4 cluster,0.02528,https://web.archive.org/web/20240623043843/https://hessian.ai/supercomputer-for-cutting-edge-ai-research-in-hesse/,,,,
Japanese Research Institute 2 Supercomputer 1,Existing,Confirmed,Yes,17.59036918,196.7539161,NVIDIA A100,624,Japan,,6/1/2022,,,0.53149824,14082761.55,Japan,,A100,Calculated from Top500,6/1/2022,,11.56383735,,,,,,,624,NVIDIA,,TRUE,,72,17.28933918,3.89E+17,3.89E+17,1.95E+17,9.7344E+16,0.53149824,,3.663E+11,14082761.55,,,,,,Microsoft GPT-4 cluster,0.02496,https://web.archive.org/web/20241210014053/https://top500.org/system/180083/,,,,
EuroHPC Karolina,Existing,Confirmed,Yes,17.55560707,181.6189995,NVIDIA A100,576,Czechia,EuroHPC JU,6/1/2021,,Public,0.5477,18428093.6,"Studentská 6231/1b, 708 00 Ostrava 8, Czechia",European researchers,A100,"an accelerated part with 72 servers, and each of them is equipped with 8 GPU accelerators providing a performance of 11.6 PFlop/s for standard HPC simulations and up to 360 PFlop/s for artificial intelligence computations,  ",6/1/2021,,11.51603434,,,,,,,576,NVIDIA,,TRUE,,37,17.25457708,3.59E+17,3.59E+17,1.80E+17,8.9856E+16,0.49480704,0.5477,3.28121E+11,15548725.94,17557684,18428093.6,"The cost of the procured system was almost EUR 15 million, where 35%, i.e.  EUR 5.13 million, of which was paid by the EuroHPC JU. The remaining costs amounting to EUR 9.73 million was funded using the resources of the European Structural and Investment Funds",,,Sunway OceanLight,0.060453869,https://web.archive.org/web/20241118204224/https://www.it4i.cz/en/infrastructure/karolina,https://www.acrossproject.eu/karolina-supercomputer-belongs-to-the-worlds-top-energy-efficient-supercomputers/,,,
Lawrence Livermore NL Lassen Phase 1,Existing,Confirmed,Yes,17.53402611,172.8145528,NVIDIA V100,2736,United States of America,US Department of Energy,11/1/2018,,Public,1.88225856,54388512.36,"7000 East Ave, Livermore, CA 94550",Lawrence Livermore NL,V100,Chip count from Top500,11/1/2018,Listed in Top500,11.25934683,,Lawrence Livermore NL Lassen Phase 2,,,,,2736,NVIDIA,,TRUE,,8,17.53402611,3.42E+17,,3.42E+17,4.28731E+16,1.88225856,,1.81697E+11,54388512.36,,,,,,Oak Ridge NL Summit,0.098958333,https://web.archive.org/web/20240327161123/https://www.hpcwire.com/2020/08/19/cerebras-llnl-deployment-teases-second-gen-wafer-scale-ai-chip/,https://web.archive.org/web/20240613070121/https://www.top500.org/system/179567/,,,
Jean Zay Supercomputer Phase 2,Existing,Confirmed,Yes,17.5276299,170.2880243,NVIDIA Tesla V100 SXM2,2696,France,GENCI,6/1/2020,2696,Public,1.7664192,56284456.1,"Campus universitaire d'Orsay, Batiment 506, Rue John Von Neumann, 91403 Orsay, France",Researchers in academia and industry,V100,"261 four-GPU accelerated compute nodes with:
2 Intel Cascade Lake 6248 processors (20 cores at 2.5 GHz), namely 40 cores per node
192 GB of memory per node
4 Nvidia Tesla V100 SXM2 GPUs (32 GB)
31 eight-GPU accelerated compute nodes, currently dedicated to the AI community with:
2 Intel Cascade Lake 6226 processors (12 cores at 2.7 GHz), namely 24 cores per node
20 nodes with 384 GB of memory and 11 nodes with 768 GB of memory
8 Nvidia Tesla V100 SXM2 GPUs (32 GB)
Extension in the summer of 2020, 351 four-GPU accelerated compute nodes with:
2 Intel Cascade Lake 6248 processors (20 cores at 2.5 GHz), namely 40 cores per node
192 GB of memory per node
4 Nvidia Tesla V100 SXM2 GPUs (16 GB)",6/1/2020,,11.28053612,Jean Zay Supercomputer Phase 1,Jean Zay Supercomputer Phase 3,,,,,2696,NVIDIA,,TRUE,,14,17.5276299,3.37E+17,,3.37E+17,4.22463E+16,1.7664192,,1.90781E+11,56284456.1,,,,,,Oak Ridge NL Summit,0.097511574,https://web.archive.org/web/20240528032205/http://www.idris.fr/eng/annonces/idris-extension-jean-zay-h100-eng.html,https://web.archive.org/web/20220319163248/http://www.idris.fr/eng/jean-zay/cpu/jean-zay-cpu-hw-eng.html,,,
Oracle 2020 A100 Cluster,Existing,Likely,Yes,17.50445455,161.4391107,NVIDIA A100,512,United States of America,Oracle,9/30/2020,"Slightly uncertain if they were just saying that they could build clusters up to 512 A100 in size, or if this was actually already available and built. It seems likely it was available, though",Private,0.4472832,13881391.16,,Cloud,A100,allowing customers to scale up to 512 GPUs in a single cluster,9/30/2020,,11.55284197,,,,,,,512,NVIDIA,,TRUE,,20,17.20342455,3.19E+17,3.19E+17,1.60E+17,7.9872E+16,0.4472832,,3.57143E+11,13881391.16,,,,,,Oak Ridge NL Summit,0.092444444,https://web.archive.org/web/20240219152835/https://blogs.oracle.com/cloud-infrastructure/post/oracle-cloud-infrastructure-compute-and-high-performance-computing-roadmap-update,,,,
Paper on CodeFuse,Existing,Confirmed,Yes,17.50445455,161.4391107,NVIDIA A100,512,,Ant Group,10/10/2023,,Private,0.42491904,10121180.24,,,A100,"CodeFuse-13B was trained using 512 Nvidia A100 GPU cards, with a Hardware FLOPs Utilization (HFU) of approximately 60%. The training process took approximately 40 days to complete.",10/10/2023,,11.57511836,,,TRUE,,,,512,NVIDIA,,,,123,17.20342455,3.19E+17,3.19E+17,1.60E+17,7.9872E+16,0.42491904,,3.7594E+11,10121180.24,,,,,,Tesla 10k H100 Cluster,0.016143911,https://arxiv.org/abs/2310.06266,,,,
Los Alamos NL Chicoma,Existing,Likely,Yes,17.50445455,161.4391107,NVIDIA A100,512,United States of America,US Department of Energy,12/20/2021,,Public,0.43982848,17533893.94,"Los Alamos, New Mexico",Los Alamos National Laboratory,A100,the remaining 128 are equipped with individual AMD Epyc 7713 CPUs and quadruple Nvidia A100 GPUs,12/20/2021,"Slightly unclear when the GPUs were installed, but they seem to be installed by December 2021 at the latest",11.56014121,,,,,,,512,NVIDIA,,TRUE,,60,17.20342455,3.19E+17,3.19E+17,1.60E+17,7.9872E+16,0.43982848,,3.63196E+11,17533893.94,,,,,,DeepSeek Fire-Flyer 2,0.0512,https://web.archive.org/web/20240722055614/https://www.hpcwire.com/2022/03/17/los-alamos-chicoma-supercomputer-to-host-75-new-projects/,,,,
Paper on BloombergGPT,Existing,Confirmed,Yes,17.50445455,161.4391107,NVIDIA A100,512,,Bloomberg,3/30/2023,,Private,0.42491904,11466651.11,,,A100,"""This yields a total of 512 40GB A100 GPUs""",3/30/2023,,11.57511836,,,TRUE,,,,512,NVIDIA,,,,95,17.20342455,3.19E+17,3.19E+17,1.60E+17,7.9872E+16,0.42491904,,3.7594E+11,11466651.11,,,,,,Microsoft GPT-4 cluster,0.02048,https://arxiv.org/abs/2303.17564,,,,
Aleph Alpha alpha ONE,Existing,Confirmed,Yes,17.50445455,161.4391107,NVIDIA A100,512,Germany,Aleph Alpha,6/1/2022,,Private,0.43610112,11555086.4,"Max-Urich-Straße 3, 13355 Berlin, Germany","German Government Agencies,Private Sector,Aleph Alpha",A100,Calculated from Top500,6/1/2022,,11.56383735,,,,,,,512,NVIDIA,,TRUE,,76,17.20342455,3.19E+17,3.19E+17,1.60E+17,7.9872E+16,0.43610112,,3.663E+11,11555086.4,,,,,,Microsoft GPT-4 cluster,0.02048,https://web.archive.org/web/20241210021004/https://top500.org/system/180080/,,,,
Paper on StarCoder,Existing,Confirmed,Yes,17.50445455,161.4391107,NVIDIA A100,512,,,5/9/2023,,,0.42491904,11457697.2,,,A100,"""We trained our model on a GPU cluster with 512 A100 80 GB GPUs distributed across 64 nodes.""",5/9/2023,,11.57511836,,,TRUE,,,,512,NVIDIA,,,,96,17.20342455,3.19E+17,3.19E+17,1.60E+17,7.9872E+16,0.42491904,,3.7594E+11,11457697.2,,,,,,Microsoft GPT-4 cluster,0.02048,https://arxiv.org/abs/2305.06161,,,,
Paper on Florence,Existing,Confirmed,Yes,17.50445455,161.4391107,NVIDIA A100 SXM4 40 GB,512,,Microsoft,11/22/2021,,Private,0.43982848,13791613.29,,Microsoft,A100,"""The model takes 10 days to train on 512 NVIDIA-A100 GPUs
with 40GB memory per GPU.""",11/22/2021,,11.56014121,,,TRUE,Microsoft Azure Voyager-EUS2,,,512,NVIDIA,,,,59,17.20342455,3.19E+17,3.19E+17,1.60E+17,7.9872E+16,0.43982848,,3.63196E+11,13791613.29,,,,,,DeepSeek Fire-Flyer 2,0.0512,https://arxiv.org/abs/2111.11432v1,,,,
Paper on Stable LM,Existing,Confirmed,Yes,17.50445455,161.4391107,NVIDIA A100 SXM4 40 GB,512,,Stability AI,1/18/2023,,Private,0.42491904,11279538.29,,,A100,"""Hardware: Stable LM 2 1.6B was trained on the Stability AI cluster across 512 NVIDIA A100 40GB GPUs (AWS P4d instances).""",1/18/2023,,11.57511836,,,TRUE,,,,512,NVIDIA,,,,91,17.20342455,3.19E+17,3.19E+17,1.60E+17,7.9872E+16,0.42491904,,3.7594E+11,11279538.29,,,,,,Microsoft GPT-4 cluster,0.02048,https://web.archive.org/web/20241218081859/https://huggingface.co/stabilityai/stablelm-2-1_6b,,,,
PLaMO-13B training cluster,Existing,Likely,Yes,17.47642583,151.3491662,NVIDIA A100,480,Japan,Preferred Networks Inc,9/25/2023,,Private,0.3983616,9488606.472,,,A100,A100 40GB x480,9/25/2023,,11.57511836,,,TRUE,AIST ABCI 2.0,,,480,NVIDIA,,,,132,17.17539583,3.00E+17,3.00E+17,1.50E+17,7.488E+16,0.3983616,,3.7594E+11,9488606.472,,,,,,Tesla 10k H100 Cluster,0.015134917,https://huggingface.co/pfnet/plamo-13b,https://web.archive.org/web/20250216174632/https://tech.preferred.jp/en/blog/llm-plamo/,https://www.preferred.jp/en/news/pr20230928/,,
NSC Berzelius Phase 1,Existing,Confirmed,Yes,17.47642583,151.3491662,NVIDIA A100 SXM4 40 GB,480,Sweden,Linköping University,3/23/2021,,Public,0.4123392,12957271.61,"583 30 Linköping, Sweden",Swedish academic research groups,A100,"Berzelius, as it stands, is an Nvidia SuperPod consisting of 60 DGX A100 nodes. Each node, in turn, carries eight A100 (40GB) GPUs",3/23/2021,,11.56014121,,NSC Berzelius Phase 2,,,,,480,NVIDIA,,TRUE,,28,17.17539583,3.00E+17,3.00E+17,1.50E+17,7.488E+16,0.4123392,,3.63196E+11,12957271.61,,,,,,Sunway OceanLight,0.050378224,https://web.archive.org/web/20230712130753/https://www.hpcwire.com/2023/01/26/sweden-plans-expansion-for-nvidia-powered-berzelius-supercomputer/,,,,
Pawsey Supercomputing Centre Setonix,Existing,Confirmed,Yes,17.46855999,148.6326428,AMD Radeon Instinct MI250X,768,Australia,Pawsey Supercomputing Centre,7/1/2022,,Public,0.8323,19752614.1,"1 Bryce Ave, Kensington WA 6151, Australia",Academia,AMD MI250X,Calculated from Top500,7/1/2022,,11.5482801,,,,,,,768,AMD,,TRUE,,83,17.46855999,2.94E+17,2.94E+17,2.94E+17,7.34976E+16,0.8176896,0.8323,3.53411E+11,19752614.1,,,,,,Microsoft GPT-4 cluster,0.018855385,https://web.archive.org/web/20240616091315/https://www.hpcwire.com/off-the-wire/pawseys-setonix-supercomputer-fires-up-for-researchers/,,,,
Paper on GLaM,Existing,Confirmed,Yes,17.44963265,142.2940879,Google TPU v4,1024,United States of America,Google,12/13/2021,,Private,0.373854208,9518052.054,,Google,TPUv4,"""Moreover, to reach similar (and slightly exceeded) scores as GPT-3, we
train using 1,024 TPU-v4 chips for 574 hours (with 280B
tokens).""",12/13/2021,,11.87693038,,,TRUE,Google TPU v4 Pod,,,1024,Google,,,,63,17.44963265,2.82E+17,2.82E+17,2.82E+17,,0.373854208,,7.53235E+11,9518052.054,,,,,,DeepSeek Fire-Flyer 2,0.045128205,https://arxiv.org/abs/2112.06905,,,,
Paper on ViT-22B,Existing,Confirmed,Yes,17.44963265,142.2940879,Google TPU v4,1024,United States of America,Google,2/10/2023,,Private,0.361181184,9341262.431,,Google,TPUv4,"""ViT-22B was trained on 1024 TPU V4 chips for 177K steps""",2/10/2023,,11.89190753,,,TRUE,Google Oklahoma TPU v4 Pods,,,1024,Google,,,,99,17.44963265,2.82E+17,2.82E+17,2.82E+17,,0.361181184,,7.79664E+11,9341262.431,,,,,,Microsoft GPT-4 cluster,0.018051282,https://arxiv.org/abs/2302.05442v1,,,,
Paper on PaLI,Existing,Confirmed,Yes,17.44963265,142.2940879,Google TPU v4,1024,United States of America,Google,9/14/2022,,Private,0.370685952,9418434.469,,Google,TPUv4,"""The largest model, PaLI-17B, is pretrained using 1,024 GCP-TPUv4 chips for 7 days.""",9/14/2022,,11.88062652,,,TRUE,Google Oklahoma TPU v4 Pods,,,1024,Google,,,,88,17.44963265,2.82E+17,2.82E+17,2.82E+17,,0.370685952,,7.59673E+11,9418434.469,,,,,,Microsoft GPT-4 cluster,0.018051282,https://arxiv.org/abs/2209.06794v4,,,,
Paper on Minerva,Existing,Confirmed,Yes,17.44963265,142.2940879,Google TPU v4,1024,United States of America,Google,6/29/2022,,Private,0.370685952,9401336.144,,Google,TPUv4,"""the 540B model was trained for 29 days on a v4-1024.""",6/29/2022,,11.88062652,,,TRUE,Google Oklahoma TPU v4 Pods,,,1024,Google,,,,83,17.44963265,2.82E+17,2.82E+17,2.82E+17,,0.370685952,,7.59673E+11,9401336.144,,,,,,Microsoft GPT-4 cluster,0.018051282,https://arxiv.org/abs/2206.14858,,,,
University of Edinburgh DiRAC Tursa,Existing,Confirmed,Yes,17.4464626,141.2592218,NVIDIA A100,448,United Kingdom of Great Britain and Northern Ireland,Distributed Research using Advanced Computing (DiRAC),11/1/2021,,Public,0.4296,15295349.49,"Edinburgh, Scotland",Academia,A100,"114 GPU nodes: 4x Nvidia RedStone A100-40 per node, 640 Tensor cores, 6,912 CUDA cores",11/1/2021,,11.51236834,,,,,,,448,NVIDIA,,TRUE,,61,17.14543261,2.80E+17,2.80E+17,1.40E+17,6.9888E+16,0.38484992,0.4296,3.25363E+11,15295349.49,,,,,,DeepSeek Fire-Flyer 2,0.0448,https://web.archive.org/web/20220121113452/https://www.epcc.ed.ac.uk/hpc-services/dirac-tursa-gpu,,,,
MosaicML MPT training cluster,Existing,Confirmed,Yes,17.43863727,138.7367357,NVIDIA A100,440,,MosaicML,3/5/2023,,,0.3651648,9850806.273,,,A100,"""over 9.5 days on 440 GPUs,""",3/5/2023,,11.57511836,,,,,,,440,NVIDIA,,TRUE,,106,17.13760727,2.75E+17,2.75E+17,1.37E+17,6.864E+16,0.3651648,,3.7594E+11,9850806.273,,,,,,Microsoft GPT-4 cluster,0.0176,https://www.databricks.com/blog/mpt-7b,,,,
Petrobras Dragão,Existing,Confirmed,Yes,17.4345689,137.4431531,NVIDIA V100,2176,Brazil,Petrobras,6/1/2021,,Public/Private,1.661,41189375.62,"Rio de Janeiro, Brazil",Petrobras,V100,Calculated from Top500,6/1/2021,,11.21419927,,,,,,,2176,NVIDIA,,TRUE,,42,17.4345689,2.72E+17,,2.72E+17,3.40979E+16,1.40195328,1.661,1.63757E+11,41189375.62,,,,,,Sunway OceanLight,0.045749456,https://web.archive.org/web/20241126215645/https://top500.org/system/179941/,,,,
Paper on BLOOM,Existing,Confirmed,Yes,17.41427792,131.1692774,NVIDIA A100,416,,,11/9/2022,,,0.35433216,9399852.699,,,A100,"Training was conducted on 48 nodes, each
having 8 NVIDIA A100 80GB GPUs (a total of 384 GPUs); due to possible hardware failures during training, we also maintained a reserve of 4 spare nodes.",11/9/2022,,11.56383735,,,TRUE,,,,416,NVIDIA,,,,100,17.11324792,2.60E+17,2.60E+17,1.30E+17,6.4896E+16,0.35433216,,3.663E+11,9399852.699,,,,,,Microsoft GPT-4 cluster,0.01664,https://arxiv.org/abs/2211.05100,,,,
Jean Zay Supercomputer Phase 3,Existing,Confirmed,Yes,17.41427792,131.1692774,NVIDIA A100,416,France,GENCI,6/1/2022,,Public,2.07659088,64360101.46,"Campus universitaire d'Orsay, Batiment 506, Rue John Von Neumann, 91403 Orsay, France",Researchers in academia and industry,"V100,A100","The power boost will come from an additional 52 nodes made up of HPE Apollo 6500 Gen10 nodes, each with eight Nvidia A100 80GB GPUs – for a total of 640GB of high-bandwidth memory (HBM2) per server – and some minor additional updates that include three Apollo 6500 Gen10 nodes, each with eight Nvidia A100 40GB PCIe GPUs",6/1/2022,,11.35177246,Jean Zay Supercomputer Phase 2,,,,NVIDIA Tesla V100 SXM2,2696,3112,NVIDIA,NVIDIA,TRUE,,86,17.6691234,2.60E+17,2.60E+17,4.67E+17,1.07142E+17,2.07659088,,2.24788E+11,64360101.46,,,,,,Microsoft GPT-4 cluster,0.01664,https://web.archive.org/web/20240528032205/http://www.idris.fr/eng/annonces/idris-extension-jean-zay-h100-eng.html,https://web.archive.org/web/20240425051414/https://www.hpcwire.com/2021/11/17/frances-jean-zay-supercomputer-gets-ai-boost-from-hpe-nvidia/,,,
AWS Fast BERT Training,Existing,Confirmed,Yes,17.40823997,129.3582618,NVIDIA V100,2048,United States of America,Amazon,12/3/2019,,Private,1.36421376,39839471.71,,Amazon,V100,"With TensorFlow, we achieved unprecedented scale with 2,048 GPUs on 256 P3dn.24xlarge instances to train BERT in 62 minutes",12/3/2019,,11.27335754,,,,,,,2048,NVIDIA,,TRUE,,11,17.40823997,2.56E+17,,2.56E+17,3.20922E+16,1.36421376,,1.87654E+11,39839471.71,,,,,,Oak Ridge NL Summit,0.074074074,https://web.archive.org/web/20241003202056/https://aws.amazon.com/blogs/machine-learning/amazon-web-services-achieves-fastest-training-times-for-bert-and-mask-r-cnn/,,,,
Taiwania 2,Existing,Confirmed,Yes,17.40140054,127.3370389,NVIDIA Tesla V100 SXM2,2016,Taiwan,National Applied Research Laboratories Taiwan,11/1/2018,,Public,1.4999,15302391.31,"Hsinchu, Taiwan",Taiwan Cloud,V100,"Powered by 2,016 NVIDIA V100 Tensor Core GPUs, TAIWANIA 2 can deliver 9 petaflops of computing power to support high performance computing and AI workloads at scale",11/1/2018,,11.22533824,,,,,,,2016,NVIDIA,,TRUE,,9,17.40140054,2.52E+17,,2.52E+17,3.15907E+16,1.38692736,1.4999,1.68011E+11,43816148.91,13882937,15302391.31,"""“Taiwania 2” system, an NT$430 million""",,,Oak Ridge NL Summit,0.072916667,https://web.archive.org/web/20240420074136/https://blogs.nvidia.com/blog/taiwania-2/,https://web.archive.org/web/20240303090755/https://www.nchc.org.tw/Message/MessageView/288?mid=92&page=4,"https://www.nextplatform.com/2024/10/09/taiwans-fastest-ai-supercomputer-goes-to-foxconn/#:~:text=The%20NCHC%2C%20as%20the%20center,supercomputer%20rankings%20at%20number%2020.",,
Paper on ViT,Existing,Confirmed,Yes,17.40123506,127.2885296,Google TPU v3,2048,,Google,6/8/2021,,Private,0.967622656,19445351.83,,Google,TPU v3,"""For models ViT-g and ViT-G, to speed up the training, we scale up the batch size at most to 32 768 and distribute the training to 2048 TPUv3 chips.""",6/8/2021,,11.41552904,,,TRUE,Google MLPerf 0.7 Submission,,,2048,Google,,,,47,17.40123506,2.52E+17,,2.52E+17,,0.967622656,,2.60333E+11,19445351.83,,,,,,Sunway OceanLight,0.042369378,https://arxiv.org/abs/2106.04560,,,,
NVIDIA In-house DGX A100 Cluster,Existing,Likely,Yes,17.39724458,126.1243052,NVIDIA A100,400,United States of America,NVIDIA,5/14/2020,50 nodes x 8 A100 per node,Private,0.34944,11082112.84,,NVIDIA,A100,"The tech company also built its own in-house DGX A100 supercomputer, consisting of 50 nodes",5/14/2020,,11.55284197,,,,,,,400,NVIDIA,,TRUE,,17,17.09621459,2.50E+17,2.50E+17,1.25E+17,6.24E+16,0.34944,,3.57143E+11,11082112.84,,,,,,Oak Ridge NL Summit,0.072222222,https://web.archive.org/web/20241129110307/https://fedscoop.com/argonne-national-lab-ai-supercomputer/,,,,
Paper on Code Llama,Existing,Likely,Yes,17.39724458,126.1243052,NVIDIA A100,400,,Meta AI,8/24/2023,,Private,0.331968,8915080.852,,Meta,A100,,8/24/2023,,11.57511836,,,TRUE,Meta Research SuperCluster (RSC-1) Phase 2,,,400,NVIDIA,,,,144,17.09621459,2.50E+17,2.50E+17,1.25E+17,6.24E+16,0.331968,,3.7594E+11,8915080.852,,,,,,Microsoft GPT-4 cluster,0.016,https://arxiv.org/abs/2308.12950,,,,
AGH Cyfronet Athena,Existing,Confirmed,Yes,17.37951581,121.079333,NVIDIA A100 SXM4 40 GB,384,Poland,AGH University of Krakow,6/1/2022,,Public,0.32707584,4172106.174,"Podole 60, 30-394 Kraków, Poland","AGH University of Krakow,Researchers in Poland,Cloud",A100,"Athena's configuration includes: 48 servers with AMD EPYC processors and 1 TB of RAM (6,144 CPU compute cores in total) as well as 384 NVIDIA A100 GPGPU cards.",6/1/2022,,11.56383735,,,,,,,384,NVIDIA,,TRUE,,92,17.07848582,2.40E+17,2.40E+17,1.20E+17,5.9904E+16,0.32707584,,3.663E+11,10212759,4034620,4172106.174,"the new supercomputer, which cost 20 million zloty (€4.15 million) to build, not including infrastructure costs",,,Microsoft GPT-4 cluster,0.01536,https://web.archive.org/web/20230205120608/https://www.hpcwire.com/2022/10/10/poland-inaugurates-athena-supercomputer/,https://web.archive.org/web/20240426185305/https://www.datacenterdynamics.com/en/news/polands-agh-launches-athena-supercomputer-in-krakow/,https://web.archive.org/web/20240715093943/https://top500.org/system/180055/,https://notesfrompoland.com/2022/10/05/polands-fastest-supercomputer-launched-with-blessing-from-archbishop/,
Indiana University Bloomington Jetstream2,Existing,Confirmed,Yes,17.35148709,113.5118747,NVIDIA A100 SXM4 40 GB,360,United States of America,Indiana University Bloomington,2/20/2022,Jetstream2 is a larger project across multiple universities. Indiana University has the largest portion. ,Public,0.3066336,10551295.26,"Bloomington, Indiana","Academia,Researchers",A100,GPUs: 360 (NVIDIA A100 SXM4 40GB),2/20/2022,,11.56383735,,,,,,,360,NVIDIA,,TRUE,,77,17.05045709,2.25E+17,2.25E+17,1.12E+17,5.616E+16,0.3066336,,3.663E+11,9769389.419,10000000,10551295.26,"Indiana University has been awarded a $10 million NSF grant to build ‘Jetstream 2,’",,,DeepSeek Fire-Flyer 2,0.036,https://web.archive.org/web/20211207065233/https://docs.jetstream-cloud.org/overview/config/,https://web.archive.org/web/20240721082615/https://news.iu.edu/it/live/news/30451-jetstream2-ai-for-everyone,,,
University of Tokyo Wisteria/BDEC-01 (Aquarius),Existing,Confirmed,Yes,17.35148709,113.5118747,NVIDIA A100,360,Japan,University of Tokyo,6/1/2021,,Public,0.3239,9717953.711,"7 Chome-3-1 Hongo, Bunkyo City, Tokyo 113-8654, Japan",Academia,A100,Calculated from Top500,6/1/2021,,11.54004615,,,,,,,360,NVIDIA,,TRUE,,47,17.05045709,2.25E+17,2.25E+17,1.12E+17,5.616E+16,0.3092544,0.3239,3.46774E+11,9717953.711,,,,,,Sunway OceanLight,0.037783668,https://web.archive.org/web/20241126224540/https://top500.org/system/179963/,,,,
NSCC ASPIRE 2A Phase 2,Existing,Confirmed,Yes,17.34172725,110.9893886,NVIDIA A100,352,Singapore,National Supercomputing Center Singapore,11/1/2022,,Public,0.29981952,7953721.515,Singapore,Cloud,A100,Calculated from Top500,11/1/2022,Top500,11.56383735,NSCC ASPIRE 2A Phase 1,,,,,,352,NVIDIA,,TRUE,,109,17.04069726,2.20E+17,2.20E+17,1.10E+17,5.4912E+16,0.29981952,,3.663E+11,7953721.515,,,,,,Microsoft GPT-4 cluster,0.01408,https://drive.google.com/file/d/1ARed1E6KguYXjkL3Qta2ZKv_isej3xNn/view?usp=drive_link,https://web.archive.org/web/20240530103453/https://www.top500.org/system/180077/,https://web.archive.org/web/20231004100738/https://www.hpcwire.com/2021/04/28/hpe-will-build-singapores-new-national-supercomputer/,,
Osaka University SQUID,Existing,Confirmed,Yes,17.32152387,105.9444164,NVIDIA A100 SXM4 80 GB,336,Japan,Osaka University,5/1/2021,42x8=336,Public,0.28863744,10581771.82,"Osaka, Japan","Osaka University,Academia,Researchers",A100,42 Nodes; GPU：NVIDIA A100 8 units,5/1/2021,,11.56014121,,,,,,,336,NVIDIA,,TRUE,,34,17.02049387,2.10E+17,2.10E+17,1.05E+17,5.2416E+16,0.28863744,,3.63196E+11,10581771.82,,,,,,Sunway OceanLight,0.035264757,https://archive.ph/fcLwW#selection-705.1-705.24,,,,
PARAM Siddhi-AI Phase 1,Existing,Confirmed,Yes,17.32152387,105.9444164,NVIDIA A100,336,India,Center for Development of Advanced Computing (C-DAC),11/1/2020,8 GPUs per node,Public,0.2935296,9109662.949,"Innovation Park, 34/B/1, Panchawati Rd, Mansarovar, Panchawati, Pashan, Pune, Maharashtra 411008, India",Cloud,A100,42 x DGX A100 Nodes,11/1/2020,,11.55284197,,AIRAWAT-PSAI Phase 2,,,,,336,NVIDIA,,TRUE,,26,17.02049387,2.10E+17,2.10E+17,1.05E+17,5.2416E+16,0.2935296,,3.57143E+11,9109662.949,,,,,,Oak Ridge NL Summit,0.060666667,https://drive.google.com/file/d/1bLDkmBo4dklKX5Psch-ifNpzNDcbELXA/view?usp=sharing,https://web.archive.org/web/20240720151011/https://cdac.in/index.aspx?id=hpc_nsf_siddhi-spec,,,
Recursion BioHive-1,Existing,Confirmed,Yes,17.30033457,100.8994442,NVIDIA A100,320,United States of America,Recursion Pharmaceuticals,4/15/2021,,Private,0.2748928,8638181.076,"41 400 W, Salt Lake City, UT 84101","Recursion Pharmaceuticals,Pharmaceutical Research",A100,BioHive-1 consists of 40 NVIDIA DGX A100 640GB nodes,4/15/2021,,11.56014121,,,,,,,320,NVIDIA,,TRUE,,36,16.99930457,2.00E+17,2.00E+17,9.98E+16,4.992E+16,0.2748928,,3.63196E+11,8638181.076,,,,,,Sunway OceanLight,0.033585483,https://web.archive.org/web/20240908095141/https://www.genengnews.com/gen-edge/from-model-to-molecule-nvidia-doubles-down-on-ai-drug-discovery/,,,,
University of Cambridge Wilkes-3,Existing,Confirmed,Yes,17.30033457,100.8994442,NVIDIA A100,320,United Kingdom of Great Britain and Northern Ireland,University of Cambridge,6/1/2021,,Public,0.2581,8638181.076,"Cambridge, England","Academia,Researchers",A100,Calculated from Top500,6/1/2021,,11.58751657,,,,,,,320,NVIDIA,,TRUE,,52,16.99930457,2.00E+17,2.00E+17,9.98E+16,4.992E+16,0.2748928,0.2581,3.86827E+11,8638181.076,,,,,,Sunway OceanLight,0.033585483,https://web.archive.org/web/20241210022223/https://top500.org/system/179930/,,,,
Leonardo SpA davinci-1,Existing,Confirmed,Yes,17.30033457,100.8994442,NVIDIA A100,320,Italy,Leonardo SpA,11/1/2020,,Public/Private,0.279552,8675869.475,"Genoa, Italy","Aerospace, Defense, and Security,Leonardo SpA",A100,Calculated from Top500,11/1/2020,,11.55284197,,,,,,,320,NVIDIA,,TRUE,,28,16.99930457,2.00E+17,2.00E+17,9.98E+16,4.992E+16,0.279552,,3.57143E+11,8675869.475,,,,,,Oak Ridge NL Summit,0.057777778,https://web.archive.org/web/20240512034721/https://www.leonardo.com/en/press-release-detail/-/detail//02-12-2020-davinci-1-is-the-name-of-the-new-leonardo-supercomputer,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,17.30103,100,,400,China,,5/15/2023,,Public,0.3,,China,,,,,,11.52287875,,,,,,,400,Anonymized,Anonymized,TRUE,,132,17,2.00E+17,2.00E+17,1.00E+17,,0.3,,3.33333E+11,,,,,,,Microsoft GPT-4 cluster,0.01,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,17.47712125,100,,500,China,,12/15/2022,,Private,0.3,,China,,,,,,11.52287875,,,TRUE,,,,500,Anonymized,Anonymized,,,101,17,3.00E+17,3.00E+17,1.00E+17,,0.3,,3.33333E+11,,,,,,,Microsoft GPT-4 cluster,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,17.30103,100,,400,China,,5/15/2021,,Public,0.3,,China,,,,,,11.52287875,,,,,,,400,Anonymized,Anonymized,TRUE,,36,17,2.00E+17,2.00E+17,1.00E+17,,0.3,,3.33333E+11,,,,,,,Sunway OceanLight,0.03,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,17.47712125,100,,2000,China,,11/15/2019,,Private,1,40000000,China,,,,,,11.47712125,,,,,,,2000,Anonymized,Anonymized,TRUE,,10,17.47712125,3.00E+17,,3.00E+17,3E+16,1,,3E+11,40000000,,,,,,Oak Ridge NL Summit,0.08,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,17.30103,100,,300,China,,10/15/2022,,Private,0.3,7000000,China,,,,,,11.52287875,,,,,,,300,Anonymized,Anonymized,TRUE,,111,17,2.00E+17,2.00E+17,1.00E+17,5E+16,0.3,,3.33333E+11,7000000,,,,,,Microsoft GPT-4 cluster,0.01,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,17.30103,100,,,China,,,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,TRUE,,,17.30103,2.00E+17,,2.00E+17,,,,,,,,,,,,,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,17.47712125,100,,,China,,6/15/2021,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,TRUE,,45,,3.00E+17,3.00E+17,,,,,,,,,,,,Sunway OceanLight,0.04,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,17.30103,100,,400,China,,6/15/2023,,Public/Private,0.3,,China,,,,,,11.52287875,,,,,,,400,Anonymized,Anonymized,TRUE,,136,17,2.00E+17,2.00E+17,1.00E+17,,0.3,,3.33333E+11,,,,,,,Microsoft GPT-4 cluster,0.01,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,17.30103,100,,,China,,9/15/2022,,Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,TRUE,,106,17.30103,2.00E+17,,2.00E+17,,,,,,,,,,,Microsoft GPT-4 cluster,0.01,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,17.30103,100,,400,China,,3/15/2018,,Public,,,China,,,,,,,,,,,,,400,Anonymized,Anonymized,TRUE,,4,17.30103,2.00E+17,,2.00E+17,,,,,,,,,,,Meta 2017 V100 Cluster,0.07,,,,,
Princeton Della Phase 1,Existing,Confirmed,Yes,17.29487167,99.63820111,NVIDIA A100,316,United States of America,Princeton University,2/1/2023,,Public,0.26225472,7088936.198,"300 Forrestal Rd, Princeton, NJ 08544",Academia,A100,Specifications on website,2/1/2023,,11.57511836,,Princeton Della Phase 2,,,,,316,NVIDIA,,TRUE,,126,16.99384168,1.97E+17,1.97E+17,9.86E+16,4.9296E+16,0.26225472,,3.7594E+11,7088936.198,,,,,,Microsoft GPT-4 cluster,0.01264,https://web.archive.org/web/20230201020753/https://researchcomputing.princeton.edu/systems/della,,,,
NVIDIA DGX SuperPOD 2019,Existing,Confirmed,Yes,17.28330123,97.01869631,NVIDIA Tesla V100 DGXS 32 GB,1536,,NVIDIA,6/17/2019,,Private,0.8526336,47352411.13,,NVIDIA,V100,"NVIDIA sets the bar once again in supercomputing, building a well-balanced system with 96 NVIDIA® DGX-2H™ servers containing 1,536 NVIDIA Tesla® V100 SXM3 GPUs. The DGX SuperPOD has earned the 22nd spot on the June 2019 TOP500 list.",6/17/2019,,11.35253879,,,,,,,1536,NVIDIA,,TRUE,,11,17.28330123,1.92E+17,,1.92E+17,2.40691E+16,0.8526336,,2.25185E+11,47352411.13,,,,,,Oak Ridge NL Summit,0.055555556,https://web.archive.org/web/20240530072101/https://developer.nvidia.com/blog/dgx-superpod-world-record-supercomputing-enterprise/,https://web.archive.org/web/20240704024932/https://developer.nvidia.com/blog/training-bert-with-gpus/,,,
RPI AiMOS,Existing,Confirmed,Yes,17.2764618,95.50277918,NVIDIA V100,1512,United States of America,Rensselaer Polytechnic Institute,12/11/2019,,Public,0.9288,29412734.97,"Troy, New York","Researchers,Academia",V100,"AiMOS is comprised of 252 compute nodes with a total of 504 IBM Power9 processors and 1,512 Nvidia Volta GPUs.",12/11/2019,Listed in Top500,11.3085396,,,,,,,1512,NVIDIA,,TRUE,,15,17.2764618,1.89E+17,,1.89E+17,2.3693E+16,1.00717344,0.9288,2.03488E+11,29412734.97,,,,,,Oak Ridge NL Summit,0.0546875,https://web.archive.org/web/20201206000554/https://www.hpcwire.com/2019/12/11/rpi-powers-up-aimos-ai-supercomputer/,,,,
Google TensorFlow Research Cloud,Existing,Likely,Yes,17.26557246,93.13794846,Google TPU v2,4096,United States of America,Google,5/17/2017,"1 cloud TPU = 4 TPU chips, according to the Forbes doccument and the FLOP counts we know for TPU v2",Private,2.692644864,38946125.27,,"Google,Cloud",TPUv2,"""Google also announced the TensorFlow Research Cloud, a 1,000-TPU (4,000 Cloud TPU Chip) supercomputer delivering 180 PetaFlops""

""the TensorFlow Research Cloud (TFRC), a cluster of 1,000 Cloud TPUs""...""Up to 180 teraflops of floating-point performance per Cloud TPU""",5/17/2017,,10.83539338,,,,,,,4096,Google,,TRUE,,1,17.26557246,1.84E+17,,1.84E+17,1.2288E+16,2.692644864,,68453141543,38946125.27,,,,,,Google TensorFlow Research Cloud,1,https://web.archive.org/web/20240724203521/https://research.google/blog/introducing-the-tensorflow-research-cloud/,https://web.archive.org/web/20240619124934/https://www.forbes.com/sites/moorinsights/2017/05/22/google-cloud-tpu-strategic-implications-for-google-nvidia-and-the-machine-learning-industry/,,,
SURF Snellius Phase 1,Existing,Confirmed,Yes,17.25457708,90.80949975,NVIDIA A100,288,Netherlands,SURF,9/16/2021,,Public,0.24740352,9849628.041,"Science Park 120, 1098 XG Amsterdam, Netherlands","Academia,Researchers",A100,The 72 Phase 1 GPU nodes each contain an NVIDIA HGX A100 baseboard with 4 GPUs,9/16/2021,,11.56014121,,SURF Snellius Phase 3,,,,,288,NVIDIA,,TRUE,,74,16.95354708,1.80E+17,1.80E+17,8.99E+16,4.4928E+16,0.24740352,,3.63196E+11,9849628.041,,,,,,DeepSeek Fire-Flyer 2,0.0288,https://web.archive.org/web/20240712185633/https://www.surf.nl/en/news/gpu-expansion-of-supercomputer-snellius-enables-even-faster-data-processing-for-dutch,https://web.archive.org/web/20240725104223/https://visualization.surf.nl/snellius-virtual-tour/#/,https://web.archive.org/web/20240716063628/https://www.cwi.nl/en/stories/hurray-a-new-national-supercomputer-snellius/,,
Anonymized Chinese System,Existing,Likely,Yes,17.30103,90,,40000,China,,11/15/2017,,Public,20,,China,,,,,,,,,,,,,40000,Anonymized,Anonymized,TRUE,,3,,2.00E+17,,,2E+17,20,,,,,,,,,Meta 2017 V100 Cluster,0.06,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,17.30103,90,,1000,China,,11/15/2018,,Private,1,30000000,China,,,,,,11.30103,,,,,,,1000,Anonymized,Anonymized,TRUE,,12,17.30103,2.00E+17,,2.00E+17,2E+16,1,,2E+11,30000000,,,,,,Oak Ridge NL Summit,0.05,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,17.30103,90,,1000,China,,11/15/2018,,Private,1,30000000,China,,,,,,11.30103,,,,,,,1000,Anonymized,Anonymized,TRUE,,12,17.30103,2.00E+17,,2.00E+17,2E+16,1,,2E+11,30000000,,,,,,Oak Ridge NL Summit,0.05,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,17.30103,90,,1000,China,,11/15/2018,,Private,1,30000000,China,,,,,,11.30103,,,,,,,1000,Anonymized,Anonymized,TRUE,,12,17.30103,2.00E+17,,2.00E+17,2E+16,1,,2E+11,30000000,,,,,,Oak Ridge NL Summit,0.05,,,,,
NVIDIA SATURN V Phase 3,Existing,Unlikely,Yes,17.24234262,88.28701364,NVIDIA A100,280,United States of America,NVIDIA,5/14/2020,"This is actually four clusters, so the minimum size of the maximum constituent cluster is 1/4 the total size
This builds off earlier versions of the SaturnV, which has been around in different forms since 2016
Very uncertain about number of chips here",Private,3.704064,109200578.3,,NVIDIA,"A100,V100","""Nvidia has updated its own internal supercomputer known as SaturnV with four DGX SuperPODS - or 1,120 A100s - to add another 2.8 exaFLOPS of power""
""Saturn-V is not really one system. It sits in four different locations under one management interface""",5/14/2020,Uncertain about what chips are in which of the four clusters,11.30485137,NVIDIA SATURN V Phase 2,,,,NVIDIA V100,5280,5560,NVIDIA,NVIDIA,,,,17.87352985,1.75E+17,1.75E+17,7.47E+17,1.26418E+17,3.704064,,2.01768E+11,109200578.3,,,,,,,,https://web.archive.org/web/20240416013949/https://www.hpcwire.com/2020/05/14/nvidias-ampere-a100-gpu-2-5x-the-hpc-20x-the-training/,https://web.archive.org/web/20240307200601/https://www.theregister.com/2020/05/14/ampere_nvidias_gpu/,https://web.archive.org/web/20240730095809/https://blogs.nvidia.com/blog/dgx-superpod-a100/,https://web.archive.org/web/20240416004832/https://www.nextplatform.com/2017/11/30/inside-nvidias-next-gen-saturn-v-ai-cluster/,
U.S. Army Jean,Existing,Confirmed,Yes,17.24234262,88.28701364,NVIDIA A100,280,United States of America,US Department of Defense,5/30/2021,,Public,0.2405312,7558408.442,"Aberdeen Proving Ground,
Maryland, 21005","US Department of Defense,US Army",A100,"Jean incorporates 1,202 Cascade Lake-AP CPUs, 280 A100 GPUs",5/30/2021,"No exact confirmation on when it was operational, but it said ""mid fiscal 2021"" for when it was expected to be operational. Known to be operational by 2023",11.56014121,,,,,,,280,NVIDIA,,TRUE,,50,16.94131263,1.75E+17,1.75E+17,8.74E+16,4.368E+16,0.2405312,,3.63196E+11,7558408.442,,,,,,Sunway OceanLight,0.029387297,https://web.archive.org/web/20231004094910/https://www.hpcwire.com/2020/08/24/dod-orders-two-ai-focused-supercomputers-from-liqid/,,,,
TU Dresden AlphaCentauri,Existing,Confirmed,Yes,17.22975349,85.76452754,NVIDIA A100 SXM4 40 GB,272,Germany,Technische Universität Dresden,2/28/2021,,Public,0.2882,7336080.257,"Dresden, Germany","Academia,Researchers",A100,At the heart of the system and essential for the computing power are a total of 272 NVIDIA A100 GPUs,2/28/2021,,11.46902952,,,,,,,272,NVIDIA,,TRUE,,41,16.9287235,1.70E+17,1.70E+17,8.49E+16,4.2432E+16,0.23365888,0.2882,2.94462E+11,7336080.257,,,,,,Oak Ridge NL Summit,0.049111111,https://web.archive.org/web/20241202181003/https://tu-dresden.de/tu-dresden/newsportal/news/alpha-centauri-platzierung?set_language=en,,,,
University of Minnesota Agate,Existing,Confirmed,Yes,17.21678852,83.24204144,NVIDIA A100 SXM4 40 GB,264,United States of America,University of Minnesota,11/1/2021,,Public,0.22678656,7109513.149,"117 Pleasant St SE # 5, Minneapolis, MN 55455","Academia,Researchers",A100,264 NVIDIA A100 Tensor Core GPUs,11/1/2021,,11.56014121,,,,,,,264,NVIDIA,,TRUE,,87,16.91575852,1.65E+17,1.65E+17,8.24E+16,4.1184E+16,0.22678656,,3.63196E+11,7109513.149,,,,,,DeepSeek Fire-Flyer 2,0.0264,https://web.archive.org/web/20241202175121/https://www-archive.msi.umn.edu/content/agate,,,,
G42 Artemis,Existing,Confirmed,Yes,17.20951501,81.85952501,NVIDIA V100,1296,United Arab Emirates,G42,11/1/2019,,Public/Private,1.2255,25278626.3,"Abu Dhabi, United Arab Emirates",Scientific research and cloud,V100,Calculated from Top500,11/1/2019,,11.1212017,,,,,,,1296,NVIDIA,,TRUE,,19,17.20951501,1.62E+17,,1.62E+17,2.03083E+16,0.86329152,1.2255,1.32191E+11,25278626.3,,,,,,Oak Ridge NL Summit,0.046875,https://web.archive.org/web/20240530181044/https://blogs.nvidia.com/blog/artemis-genomics/,,,,
JAMSTEC ZettaScaler-2.2 Gyoukou,Existing,Likely,Yes,17.20520436,81.05103588,PEZY-SC2,10000,Japan,Japan Agency for Marine-Earth Science and Technology,11/15/2017,,Public,2.5996,,,Japan Agency for Marine-Earth Science and Technology,PEZY-SC2,"10,000 PEZY-SC2 processor modules",2017-11,,10.79029784,JAMSTEC ZettaScaler-2.0 Gyoukou,,,,,,10000,PEZY,,TRUE,,4,17.20520436,1.60E+17,,1.60E+17,8.192E+16,4.22604,2.5996,61701800277,,,,,,,Meta 2017 V100 Cluster,0.058327273,https://web.archive.org/web/20241222180018/https://en.wikipedia.org/wiki/Gyoukou,https://www.top500.org/system/179102/,,,
Paper on SAM,Existing,Confirmed,Yes,17.20342455,80.71955533,NVIDIA A100,256,,Meta AI,4/5/2023,,Private,0.21245952,5733325.557,,,A100,"""Wedistribute training across 256 GPUs,""",4/5/2023,,11.57511836,,,TRUE,Meta Research SuperCluster (RSC-1) Phase 1,,,256,NVIDIA,,,,148,16.90239456,1.60E+17,1.60E+17,7.99E+16,3.9936E+16,0.21245952,,3.7594E+11,5733325.557,,,,,,Microsoft GPT-4 cluster,0.01024,https://arxiv.org/abs/2304.02643,,,,
Indiana University Big Red 200,Existing,Confirmed,Yes,17.20342455,80.71955533,NVIDIA A100,256,United States of America,Indiana University Bloomington,11/30/2021,"Unclear exactly when this was first operational. It was first listed in Top500 in November 2021, but it was planned to be operational Fall 2020",Public,0.21991424,10054424.08,"Bloomington, Indiana",Indiana University,A100,Big Red 200 is accelerated by 256 NVIDIA A100 Tensor Core GPUs,11/30/2021,,11.56014121,,,,,,,256,NVIDIA,,TRUE,,92,16.90239456,1.60E+17,1.60E+17,7.99E+16,3.9936E+16,0.21991424,,3.63196E+11,8742397.146,9600000,10054424.08,is a $9.6 million Cray Shasta system,,,DeepSeek Fire-Flyer 2,0.0256,https://web.archive.org/web/20240709091139/https://news.iu.edu/it/live/news/30305-indiana-university-unveils-supercomputer-big-red,https://web.archive.org/web/20230530192726/https://www.hpcwire.com/off-the-wire/big-red-200-is-a-leap-forward-for-indiana-university-students-and-researchers/,https://web.archive.org/web/20240915161038/https://www.top500.org/system/180020/,https://www.hpcwire.com/2020/01/24/indiana-university-dedicates-big-red-200-cray-shasta-supercomputer/,
NSCC ASPIRE 2A Phase 1,Existing,Confirmed,Yes,17.20342455,80.71955533,NVIDIA A100,256,Singapore,National Supercomputing Center Singapore,6/1/2022,,Public,0.21805056,5777543.201,Singapore,Cloud,A100,Calculated from Top500,6/1/2022,Top500,11.56383735,,NSCC ASPIRE 2A Phase 2,,,,,256,NVIDIA,,TRUE,,116,16.90239456,1.60E+17,1.60E+17,7.99E+16,3.9936E+16,0.21805056,,3.663E+11,5777543.201,,,,,,Microsoft GPT-4 cluster,0.01024,https://drive.google.com/file/d/1ARed1E6KguYXjkL3Qta2ZKv_isej3xNn/view?usp=drive_link,https://web.archive.org/web/20240530103453/https://www.top500.org/system/180077/,https://web.archive.org/web/20231004100738/https://www.hpcwire.com/2021/04/28/hpe-will-build-singapores-new-national-supercomputer/,,
Paper on EVA,Existing,Confirmed,Yes,17.20342455,80.71955533,NVIDIA A100,256,,,11/14/2022,,Public/Private,0.21805056,5784524.738,,,A100,"""These modifications allow us to train a 1.1B CLIP with a batch size of
41k on 256× NVIDIA A100 40GB GPUs.""",11/14/2022,,11.56383735,,,TRUE,,,,256,NVIDIA,,,,134,16.90239456,1.60E+17,1.60E+17,7.99E+16,3.9936E+16,0.21805056,,3.663E+11,5784524.738,,,,,,Microsoft GPT-4 cluster,0.01024,https://arxiv.org/abs/2211.07636,,,,
Paper on Pythia,Existing,Confirmed,Yes,17.20342455,80.71955533,NVIDIA A100,256,,EleutherAI,4/3/2023,Appendix D table with GPU counts for each model size,Private,0.21245952,5733325.557,,Cloud,A100,"""All GPUs are A100s with 40GB of memory"", ",4/3/2023,,11.57511836,,,TRUE,,,,256,NVIDIA,,,,148,16.90239456,1.60E+17,1.60E+17,7.99E+16,3.9936E+16,0.21245952,,3.7594E+11,5733325.557,,,,,,Microsoft GPT-4 cluster,0.01024,https://arxiv.org/abs/2304.01373,,,,
Paper on Polyglot,Existing,Confirmed,Yes,17.20342455,80.71955533,NVIDIA A100,256,,"EleutherAI,Stability AI",6/4/2023,,Private,0.21245952,5716911.883,,,A100,"""Additionally, we received invaluable assistance from Stability AI2 , who provided access to 256 A100s (8 * 32 nodes) on HPC clusters""",6/4/2023,,11.57511836,,,TRUE,Paper on Stable Diffusion,,,256,NVIDIA,,,,161,16.90239456,1.60E+17,1.60E+17,7.99E+16,3.9936E+16,0.21245952,,3.7594E+11,5716911.883,,,,,,Microsoft GPT-4 cluster,0.01024,https://arxiv.org/abs/2306.02254,,,,
Paper on XGLM,Existing,Confirmed,Yes,17.20342455,80.71955533,NVIDIA A100,256,,Meta AI,12/20/2021,,Private,0.21991424,8766946.969,,Meta,A100,"""The XGLM 7.5B model was trained on 256 A100 GPUs for about 3 weeks, at a speed of 311.6k words per second""",12/20/2021,,11.56014121,,,TRUE,Meta Research SuperCluster (RSC-1) Phase 1,,,256,NVIDIA,,,,95,16.90239456,1.60E+17,1.60E+17,7.99E+16,3.9936E+16,0.21991424,,3.63196E+11,8766946.969,,,,,,DeepSeek Fire-Flyer 2,0.0256,https://arxiv.org/abs/2112.10668,,,,
Paper on Stable Diffusion,Existing,Likely,Yes,17.20342455,80.71955533,NVIDIA A100,256,,Stability AI,12/20/2021,,Private,0.21991424,8766946.969,,Cloud,A100,,12/20/2021,,11.56014121,,,TRUE,,,,256,NVIDIA,,,,95,16.90239456,1.60E+17,1.60E+17,7.99E+16,3.9936E+16,0.21991424,,3.63196E+11,8766946.969,,,,,,DeepSeek Fire-Flyer 2,0.0256,https://arxiv.org/abs/2112.10752,,,,
Universitaet Frankfurt Goethe-NHR,Existing,Confirmed,Yes,17.20216125,80.48509348,AMD Instinct MI210,880,Germany,Goethe University Frankfurt,6/1/2023,,Public,0.3322,,"Brüningstraße 50, 65926 Frankfurt am Main, Germany","Academia,Researchers",AMD MI210,of 880 AMD MI210 graphics cards,6/1/2023,,11.68076162,,,,,,,880,AMD,,TRUE,,168,17.20216125,1.59E+17,,1.59E+17,,,0.3322,4.7947E+11,,,,,,,Microsoft GPT-4 cluster,0.010210256,https://archive.ph/cFXgi,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,17.30103,80,,,China,,,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,TRUE,,,17.30103,2.00E+17,,2.00E+17,,,,,,,,,,,,,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,17.30103,80,,1000,China,,7/15/2020,,Private,0.8,20000000,China,,,,,,11.39794001,,,,,,,1000,Anonymized,Anonymized,TRUE,,31,17.30103,2.00E+17,,2.00E+17,2E+16,0.8,,2.5E+11,20000000,,,,,,Oak Ridge NL Summit,0.05,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,17.30103,80,,300,China,,4/15/2023,,Public,0.2,,China,,,,,,11.69897,,,,,,,400,Anonymized,Anonymized,TRUE,,146,17,2.00E+17,2.00E+17,1.00E+17,,0.2,,5E+11,,,,,,,Microsoft GPT-4 cluster,0.01,,,,,
TACC Lonestar6,Existing,Confirmed,Yes,17.19658513,79.45831228,NVIDIA A100 PCIe,252,United States of America,University of Texas at Austin,11/14/2022,,Public,0.16098264,5591851.571,"10100 Burnet Rd, Austin, TX 78758","Academia,Researchers",A100,Lonestar6 hosts 84 A100 GPU nodes that are configured identically to the compute nodes with the addition of 3 NVIDIA A100 GPUs,11/14/2022,,11.68877609,,,,,,,252,NVIDIA,,TRUE,,139,16.89555513,1.57E+17,1.57E+17,7.86E+16,3.9312E+16,0.16098264,,4.884E+11,5591851.571,,,,,,Microsoft GPT-4 cluster,0.01008,https://web.archive.org/web/20240808090149/https://www.hpcwire.com/off-the-wire/taccs-lonestar6-supercomputer-gets-gpu-and-server-boost/,https://web.archive.org/web/20241006113309/https://docs.tacc.utexas.edu/hpc/lonestar6/,,,
Microsoft Azure Immunity Bio,Existing,Confirmed,Yes,17.19382003,78.95401718,NVIDIA V100,1250,United States of America,Microsoft,4/1/2020,,Private,0.819,23994725.87,,Cloud,V100,"The cluster contains over 1,250  NVIDIA V100 Tensor Core high performance graphics processing units (GPUs)",4/1/2020,,11.28053612,,,,,,,1250,NVIDIA,,TRUE,,25,17.19382003,1.56E+17,,1.56E+17,1.95875E+16,0.819,,1.90781E+11,23994725.87,,,,,,Oak Ridge NL Summit,0.045211227,https://web.archive.org/web/20240910193454/https://immunitybio.com/immunitybio-combines-supercomputing-power-with-microsoft-azure-to-target-infection-doorway-of-the-coronavirus/,,,,
Eni HPC4 Phase 2,Existing,Confirmed,Yes,17.1931246,78.82769075,NVIDIA A100,250,Italy,Eni,10/30/2021,,Private,1.98653,60465865.26,"Via per la Corradina, 35, 27032 Ferrera Erbognone PV, Italy",Eni,"V100,A100","Comprising the bulk of the system, there are 1,375 nodes that house two Nvidia “V100 O&G” GPUs; 125 nodes have two Nvidia A100s",10/30/2021,,11.32695996,Eni HPC4 Phase 1,,,,NVIDIA V100,2750,3000,NVIDIA,NVIDIA,TRUE,,82,17.62505509,1.56E+17,1.56E+17,4.22E+17,8.20925E+16,1.98653,,2.12305E+11,60465865.26,,,,,,DeepSeek Fire-Flyer 2,0.025,https://web.archive.org/web/20220124003756/https://www.hpcwire.com/2021/10/13/eni-goes-back-to-hpe-for-hpc4-refresh-via-greenlake/,https://web.archive.org/web/20250213075947/https://top500.org/system/179444/,,,
SberCloud Christofari,Existing,Confirmed,Yes,17.17609126,75.79585649,NVIDIA V100,1200,Russia,SberCloud,11/9/2019,"SberCloud, which owns this supercomputer, was originally owned by Sber, but was sold off to Noviye Vozmozhnosti in March 2022
Sberbank is majority owned by the Russian Government.",Private,0.799344,23406135.46,"Skolkovo Innovation Center
Инновационный центр Сколково
Moscow Oblast
Russia",Cloud,V100,Calculated from Top500,11/9/2019,,11.27335754,,,,,,,1200,NVIDIA,,TRUE,,21,17.17609126,1.50E+17,,1.50E+17,1.8804E+16,0.799344,,1.87654E+11,23406135.46,,,,,,Oak Ridge NL Summit,0.043402778,https://web.archive.org/web/20230517125435/https://en.wikipedia.org/wiki/Christofari,,,,
EuroHPC Vega,Existing,Confirmed,Yes,17.17539583,75.67458312,NVIDIA A100 PCIe,240,Slovenia,"EuroHPC JU,Institute of Information Science (IZUM)",4/1/2021,,Public,0.1546272,21170555.63,"Prešernova ulica 17, 2000 Maribor, Slovenia","Cloud,European researchers",A100,With 960 CPU nodes (overall 1920 CPUs AMD Epyc 7H12 – 122000 cores) and 60 GPU nodes (overall 240 GPUs NVidia A100),4/1/2021,,11.68507994,,,,,,,240,NVIDIA,,TRUE,,49,16.87436584,1.50E+17,1.50E+17,7.49E+16,3.744E+16,0.1546272,,4.84262E+11,6478635.807,20170612,21170555.63,"""The Vega supercomputer was jointly procured by Slovenia and the EU with an investment of €17.2 million""
This supercomputer contains a lot of CPUs, so the price might be higher than normal",,,Sunway OceanLight,0.025189112,https://web.archive.org/web/20210420154820/https://www.hpcwire.com/off-the-wire/vega-first-new-eurohpc-supercomputer-to-be-delivered-in-the-eu-will-be-operational-april-2021/,https://archive.ph/fCnkQ,https://www.hpcwire.com/off-the-wire/vega-online-the-eu-first-eurohpc-supercomputer-is-operational/,,
Anonymized Chinese System,Existing,Confirmed,Yes,17,70,,500,China,,12/15/2018,,Public,0.5,20000000,China,,,,,,10.07918125,,,,,,200,700,Anonymized,Anonymized,TRUE,,17,15.77815125,1.00E+17,1.00E+17,6E+15,4E+16,0.5,,12000000000,20000000,,,,,,Oak Ridge NL Summit,0.04,,,,,
Petrobras Atlas,Existing,Confirmed,Yes,17.13353891,68.72157655,NVIDIA V100,1088,Brazil,Petrobras,6/1/2020,,Public/Private,0.9789,20775184.64,"Rio de Janeiro, Brazil",Petrobras,V100,Calculated from Top500,6/1/2020,,11.14280058,,,,,,,1088,NVIDIA,,TRUE,,31,17.13353891,1.36E+17,,1.36E+17,1.7049E+16,0.7128576,0.9789,1.38931E+11,20775184.64,,,,,,Oak Ridge NL Summit,0.039351852,https://web.archive.org/web/20241210015105/https://www.top500.org/system/179854/,,,,
Japan Atomic Energy Agency and Quantum and Radiological Science and Technology HPE SGI8600,Existing,Confirmed,Yes,17.13353891,68.72157655,NVIDIA Tesla V100 SXM2,1088,Japan,Japan Atomic Energy Agency,11/1/2020,,Public,0.7128576,22615099.77,Japan,"Japan Atomic Energy Agency,National Institutes for Quantum and Radiological Science and Technology",V100,Calculated from Top500,11/1/2020,,11.28053612,,,,,,,1088,NVIDIA,,TRUE,,41,17.13353891,1.36E+17,,1.36E+17,1.7049E+16,0.7128576,,1.90781E+11,22615099.77,,,,,,Oak Ridge NL Summit,0.039351852,https://web.archive.org/web/20240331015933/https://ccse.jaea.go.jp/computer/index_eng.html,,,,
Jean Zay Supercomputer Phase 1,Existing,Confirmed,Yes,17.11561051,65.94239515,NVIDIA Tesla V100 SXM2 32 GB,1044,France,GENCI,6/1/2019,,Public,0.69542928,30548105.98,"Campus universitaire d'Orsay, Batiment 506, Rue John Von Neumann, 91403 Orsay, France",Researchers in academia and industry,V100,"the system encompasses 1,528 Intel next-generation Xeon nodes and 261 GPU nodes, each with four Nvidia Tesla V100 (32GB) GPUs, 1,044 in all.",6/1/2019,,11.27335754,,Jean Zay Supercomputer Phase 2,,,,,1044,NVIDIA,,TRUE,,18,17.11561051,1.31E+17,,1.31E+17,1.63595E+16,0.69542928,,1.87654E+11,22443812.56,27916750,30548105.98,approximately €25 million for the first partition of the Jean-Zay machine,,,Oak Ridge NL Summit,0.037760417,https://web.archive.org/web/20230123052742/https://www.hpcwire.com/2019/01/22/france-to-deploy-ai-focused-supercomputer-jean-zay/,https://news.cnrs.fr/opinions/computing-the-cost-of-computation,,,
Preferred Networks MN-2,Existing,Confirmed,Yes,17.10720997,64.67913088,NVIDIA V100,1024,Japan,Preferred Networks Inc,7/30/2019,,Private,0.68210688,20153662.29,Japan,Preferred Networks,V100,"GPUs: 1,024",7/30/2019,,11.27335754,,,,,,,1024,NVIDIA,,TRUE,,21,17.10720997,1.28E+17,,1.28E+17,1.60461E+16,0.68210688,,1.87654E+11,20153662.29,,,,,,Oak Ridge NL Summit,0.037037037,https://web.archive.org/web/20241126234046/https://fuse.wikichip.org/news/3063/japanese-ai-startup-preferred-networks-designed-a-custom-half-petaflops-training-chip/,,,,
Paper on DALL-E,Existing,Confirmed,Yes,17.10720997,64.67913088,NVIDIA V100,1024,,OpenAI,2/24/2021,,Private,0.65974272,19366409.86,,OpenAI,V100,"We trained the model using 1024, 16 GB NVIDIA V100 GPUs",2/24/2021,,11.28783536,,,TRUE,Azure OpenAI GPT-3 Cluster,,,1024,NVIDIA,,,,50,17.10720997,1.28E+17,,1.28E+17,1.60461E+16,0.65974272,,1.94015E+11,19366409.86,,,,,,Oak Ridge NL Summit,0.037037037,https://arxiv.org/abs/2102.12092,,,,
Paper on RoBERTa,Existing,Confirmed,Yes,17.10720997,64.67913088,NVIDIA Tesla V100 DGXS 32 GB,1024,United States of America,Meta AI,7/26/2019,,Private,0.5684224,31539757.13,,,,"We pretrain our model using
1024 V100 GPUs for approximately one day",7/26/2019,,11.35253879,,,TRUE,Meta 2017 V100 Cluster,,,1024,NVIDIA,,,,21,17.10720997,1.28E+17,,1.28E+17,1.60461E+16,0.5684224,,2.25185E+11,31539757.13,,,,,,Oak Ridge NL Summit,0.037037037,https://arxiv.org/abs/1907.11692,,,,
Paper on Meena,Existing,Confirmed,Yes,17.10020507,63.64426478,Google TPU v3,1024,,Google,1/27/2020,,Private,0.49201152,9859735.665,,,TPU v3,"""We trained our best model for 30 days on a TPUv3 Pod (2,048 TPU cores)""",1/27/2020,,11.4082298,,,TRUE,Google TPUv3 POD Generic,,,1024,Google,,,,29,17.10020507,1.26E+17,,1.26E+17,,0.49201152,,2.55994E+11,9859735.665,,,,,,Oak Ridge NL Summit,0.036444444,https://arxiv.org/abs/2001.09977,,,,
Paper on LaMDA,Existing,Confirmed,Yes,17.10020507,63.64426478,Google TPU v3,1024,,Google,1/20/2022,,Private,0.479711232,9719635.891,,Google,TPU v3,"""We pre-trained LaMDA on 1024 TPU-v3 chips""",1/20/2022,,11.41922518,,,TRUE,Google MLPerf 0.7 Submission,,,1024,Google,,,,110,17.10020507,1.26E+17,,1.26E+17,,0.479711232,,2.62558E+11,9719635.891,,,,,,DeepSeek Fire-Flyer 2,0.020184615,https://arxiv.org/abs/2201.08239,,,,
Paper on GShard,Existing,Confirmed,Yes,17.10020507,63.64426478,Google TPU v3,1024,,Google,10/30/2020,,Private,0.49201152,9765095.968,,Google,TPU v3,,10/30/2020,,11.4082298,,,TRUE,Google MLPerf 0.7 Submission,,,1024,Google,,,,44,17.10020507,1.26E+17,,1.26E+17,,0.49201152,,2.55994E+11,9765095.968,,,,,,Oak Ridge NL Summit,0.036444444,https://arxiv.org/abs/2006.16668,,,,
Paper on Noisy Student,Existing,Confirmed,Yes,17.10020507,63.64426478,Google TPU v3,1024,,Google,11/11/2019,,Private,0.500211712,10018621.29,,Google,TPU v3,,11/11/2019,,11.40105121,,,TRUE,Google TPUv3 POD Generic,,,1024,Google,,,,26,17.10020507,1.26E+17,,1.26E+17,,0.500211712,,2.51797E+11,10018621.29,,,,,,Oak Ridge NL Summit,0.036444444,https://arxiv.org/abs/1911.04252v4,,,,
Paper on Pseudo Labels,Existing,Confirmed,Yes,17.10020507,63.64426478,Google TPU v3,1024,,Google,3/23/2020,,Private,0.49201152,9859735.665,,Google,TPU v3,"""Specifically, our training process runs on a cluster of 2,048
TPUv3 cores.""",3/23/2020,,11.4082298,,,TRUE,Google TPUv3 POD Generic,,,1024,Google,,,,30,17.10020507,1.26E+17,,1.26E+17,,0.49201152,,2.55994E+11,9859735.665,,,,,,Oak Ridge NL Summit,0.036444444,https://arxiv.org/abs/2003.10580,,,,
Google TPUv3 POD Generic,Existing,Confirmed,Yes,17.10020507,63.64426478,Google TPU v3,1024,,Google,5/7/2019,"Refers to the general commercially available pod, not to a specific cluster using the pod",Private,0.500211712,10154990.91,,Google,TPU v3,A TPU v3 Pod is composed of 1024 chips interconnected with high-speed links,5/7/2019,,11.40105121,,,,,,,1024,Google,,TRUE,,18,17.10020507,1.26E+17,,1.26E+17,,0.500211712,,2.51797E+11,10154990.91,,,,,,Oak Ridge NL Summit,0.036444444,https://web.archive.org/web/20231128061217/https://cloud.google.com/blog/products/ai-machine-learning/googles-scalable-supercomputers-for-machine-learning-cloud-tpu-pods-are-now-publicly-available-in-beta,https://web.archive.org/web/20240626015135/https://cloud.google.com/tpu/docs/v3,,,
Paper on Switch transformer,Existing,Likely,Yes,17.10020507,63.64426478,Google TPU v3,1024,,Google,1/11/2021,The TPU count is from the expert parallelism (1 expert per core) with 2048 experts and 2 tensor cores per TPU v3.,Private,0.483811328,9756582.371,,Google,TPU v3,,1/11/2021,,11.41552904,,,TRUE,Google MLPerf 0.7 Submission,,,1024,Google,,,,50,17.10020507,1.26E+17,,1.26E+17,,0.483811328,,2.60333E+11,9756582.371,,,,,,Oak Ridge NL Summit,0.036444444,https://arxiv.org/abs/2101.03961,,,,
Texas A&M Grace,Existing,Confirmed,Yes,17.09621459,63.0621526,NVIDIA A100,200,United States of America,Texas A&M,11/1/2021,,Public,0.171808,6828281.021,"College Station, Texas",Academia,A100,Calculated from Top500,11/1/2021,,11.56014121,,,,,,,200,NVIDIA,,TRUE,,108,16.79518459,1.25E+17,1.25E+17,6.24E+16,3.12E+16,0.171808,,3.63196E+11,6828281.021,,,,,,DeepSeek Fire-Flyer 2,0.02,https://web.archive.org/web/20241203212138/https://campustechnology.com/articles/2020/11/19/new-texas-am-supercomputer-grace-goes-online-in-december.aspx?s=ct_it_031220,,,,
SiDi IARA,Existing,Confirmed,Yes,17.09621459,63.0621526,NVIDIA A100,200,Brazil,SiDi,6/1/2021,,Private,0.171808,5398863.173,"São Paulo, Brazil",SiDi,A100,Calculated from Top500,6/1/2021,,11.56014121,,,,,,,200,NVIDIA,,TRUE,,83,16.79518459,1.25E+17,1.25E+17,6.24E+16,3.12E+16,0.171808,,3.63196E+11,5398863.173,,,,,,Sunway OceanLight,0.020990927,https://web.archive.org/web/20250131005940/https://top500.org/system/179931/,,,,
Argonne NL Sophia (Formerly Theta),Existing,Confirmed,Yes,17.07848582,60.5396665,NVIDIA A100,192,United States of America,US Department of Energy,5/30/2020,"This was originally an addition of a GPU portion of the Theta supercomputer. The Theta supercomputer has since been retired, and these GPUs were shifted to the Sophia supercomputer",Public,0.1677312,5286779.722,,Argonne National Laboratory and general scientific community,A100,the augmented Theta architecture adds 24 NVIDIA DGX A100 nodes to the existing system. Each DGX A100 node comprises eight NVIDIA A100 Tensor Core GPUs,5/30/2020,,11.55284197,,,,,,,192,NVIDIA,,TRUE,,38,16.77745582,1.20E+17,1.20E+17,5.99E+16,2.9952E+16,0.1677312,,3.57143E+11,5286779.722,,,,,,Oak Ridge NL Summit,0.034666667,https://web.archive.org/web/20240926172236/https://docs.alcf.anl.gov/sophia/hardware-overview/machine-overview/,https://web.archive.org/web/20240930034827/https://www.datacenterdynamics.com/en/news/argonne-national-lab-retires-theta-supercomputer/,https://web.archive.org/web/20240927032645/https://www.hpcwire.com/off-the-wire/argonne-augments-theta-supercomputer-with-gpus-to-accelerate-coronavirus-research/,,
Atos Spartan2,Existing,Confirmed,Yes,17.07848582,60.5396665,NVIDIA A100,192,France,,11/1/2020,,Private,0.1894,5205521.685,France,Cloud,A100,Calculated from Top500,11/1/2020,,11.50007585,,,,,,,192,NVIDIA,,TRUE,,52,16.77745582,1.20E+17,1.20E+17,5.99E+16,2.9952E+16,0.1677312,0.1894,3.16283E+11,5205521.685,,,,,,Oak Ridge NL Summit,0.034666667,https://web.archive.org/web/20241210023753/https://top500.org/system/179893/,,,,
Commissariat a l'Energie Atomique Topaze,Existing,Confirmed,Yes,17.07848582,60.5396665,NVIDIA A100,192,France,French Alternative Energies and Atomic Energy Commission (CEA),6/23/2021,,Public,0.16493568,5170689.466,"Essonne, France","Researchers,Industry",A100,Calculated from Top500,6/23/2021,,11.56014121,,,,,,,192,NVIDIA,,TRUE,,89,16.77745582,1.20E+17,1.20E+17,5.99E+16,2.9952E+16,0.16493568,,3.63196E+11,5170689.466,,,,,,Sunway OceanLight,0.02015129,https://web.archive.org/web/20240617010640/https://atos.net/en/2021/press-release_2021_06_23/topaze-a-new-computer-at-the-ccrt-co-designed-by-atos-and-the-cea-to-meet-the-challenges-of-high-performance-computing-and-data-processing,,,,
Dell Rattler,Existing,Confirmed,Yes,17.07848582,60.5396665,NVIDIA A100,192,United States of America,Dell Technologies,11/1/2021,,Private,0.16493568,6555149.78,"Austin, Texas",Dell,A100,Calculated from Top500,11/1/2021,,11.56014121,,,,,,,192,NVIDIA,,TRUE,,110,16.77745582,1.20E+17,1.20E+17,5.99E+16,2.9952E+16,0.16493568,,3.63196E+11,6555149.78,,,,,,DeepSeek Fire-Flyer 2,0.0192,https://web.archive.org/web/20240716193131/https://www.top500.org/system/180040/,,,,
High-Performance Computing Center Stuttgart Hawk,Existing,Confirmed,Yes,17.07848582,60.5396665,NVIDIA A100,192,Germany,University of Stuttgart,9/20/2021,,Public,0.16493568,6554808.902,"Nobelstraße 19, 70569 Stuttgart, Germany","Academia,Industry",A100,Hawk's upgrade consists of 24 HPE Apollo 6500 Gen10 Plus systems with 192 NVIDIA A100 GPUs based on the NVIDIA Ampere architecture,9/20/2021,,11.56014121,,,,,,,192,NVIDIA,,TRUE,,98,16.77745582,1.20E+17,1.20E+17,5.99E+16,2.9952E+16,0.16493568,,3.63196E+11,6554808.902,,,,,,DeepSeek Fire-Flyer 2,0.0192,https://web.archive.org/web/20240714005409/https://www.hlrs.de/news/detail/hawk-upgrade-artificial-intelligence,,,,
MIT LLSC TX-GAIA,Existing,Confirmed,Yes,17.04921802,56.59423952,NVIDIA V100,896,United States of America,US Department of Defense,9/26/2019,,Public,2.5719,17476581.14,"MIT Lincoln Laboratory Supercomputing Center
50 Water St, Holyoke, MA 01040",MIT,V100,"has just installed a new GPU-accelerated supercomputer, powered by 896 NVIDIA Tensor Core V100 GPUs",9/26/2019,Confirmed by NVIDIA blog,10.63896394,,,,,,,896,NVIDIA,,TRUE,,24,17.04921802,1.12E+17,,1.12E+17,1.40403E+16,0.59684352,2.5719,43547571834,17476581.14,,,,,,Oak Ridge NL Summit,0.032407407,https://web.archive.org/web/20240805184855/https://developer.nvidia.com/blog/mit-lincoln-laboratory-supercomputing-center-tx-gaia/,,,,
Nagoya University Flow Type II (Furo),Existing,Confirmed,Yes,17.04336228,55.83628095,NVIDIA Tesla V100 SXM2,884,Japan,Nagoya University,7/30/2020,,Public,0.5791968,18406892.28,"Nagoya, Japan",Academia,V100,Calculated from Top500,7/30/2020,,11.28053612,,,,,,,884,NVIDIA,,TRUE,,45,17.04336228,1.11E+17,,1.11E+17,1.38523E+16,0.5791968,,1.90781E+11,18406892.28,,,,,,Oak Ridge NL Summit,0.03197338,https://web.archive.org/web/20231001220341/https://developer.nvidia.com/blog/nagoya-university-to-install-new-15-petaflop-gpu-accelerated-supercomputer/,,,,
CSCS Piz Daint Phase 2,Existing,Confirmed,Yes,17.0360745,54.90712481,NVIDIA Tesla P100 PCIe 16GB,5704,Switzerland,ETH Domain,6/1/2017,,Public,4.3744,121339528.6,,"Swiss Universities,Academia",P100,NVIDIA® Tesla® P100 16GB - 5704 Nodes,6/1/2017,,10.395156,CSCS Piz Daint Phase 1,,,,,,5704,NVIDIA,,TRUE,,2,17.0360745,1.09E+17,,1.09E+17,5.43363E+16,3.3479628,4.3744,24840252377,121339528.6,,,,,,Google TensorFlow Research Cloud,0.58952474,https://web.archive.org/web/20240925211711/https://www.cscs.ch/computers/piz-daint,https://web.archive.org/web/20240519145824/https://www.cscs.ch/publications/news/2017/piz-daint-one-of-the-most-powerful-supercomputers-in-the-world,,,
Japanese Research Institute 1 Supercomputer 1,Existing,Confirmed,Yes,17.01703334,52.55179384,NVIDIA Tesla V100 SXM2,832,Japan,,6/1/2019,,,0.55421184,17951819.92,Japan,,V100,Calculated from Top500,6/1/2019,,11.27335754,,,,,,,832,NVIDIA,,TRUE,,21,17.01703334,1.04E+17,,1.04E+17,1.30374E+16,0.55421184,,1.87654E+11,17951819.92,,,,,,Oak Ridge NL Summit,0.030092593,https://web.archive.org/web/20241205003621/https://top500.org/system/179698/,,,,
Microsoft Azure ND v2 Largest Stated,Existing,Confirmed,Yes,17,50.530571,NVIDIA V100,800,United States of America,Microsoft,11/18/2019,"This was the largest size mentioned. There were likely smaller ones as well, and possibly multiple of this size",Private,0.532896,15562293.64,,"Cloud,Azure",V100,"Built to handle the most demanding AI and high performance computing applications, the largest deployments of Azure’s new NDv2 instance rank among the world’s fastest supercomputers, offering up to 800 NVIDIA V100 Tensor Core GPUs",11/18/2019,,11.27335754,,,,,,,800,NVIDIA,,TRUE,,31,17,1.00E+17,,1.00E+17,1.2536E+16,0.532896,,1.87654E+11,15562293.64,,,,,,Oak Ridge NL Summit,0.028935185,https://web.archive.org/web/20240523151746/https://nvidianews.nvidia.com/news/nvidia-announces-scalable-gpu-accelerated-supercomputer-in-the-microsoft-azure-cloud,,,,
MTS Grom,Existing,Confirmed,Yes,16.99930457,50.44972208,NVIDIA A100,160,Russia,MTS,6/1/2021,,Private,0.1374464,4319090.538,Russia,"MTS,Cloud",A100,Calculated from Top500,6/1/2021,,11.56014121,,,,,,,160,NVIDIA,,TRUE,,91,16.69827458,9.98E+16,9.98E+16,4.99E+16,2.496E+16,0.1374464,,3.63196E+11,4319090.538,,,,,,Sunway OceanLight,0.016792741,https://web.archive.org/web/20240915111238/https://www.top500.org/system/179955/,,,,
NVIDIA Tethys,Existing,Confirmed,Yes,16.99930457,50.44972208,NVIDIA A100,160,United States of America,NVIDIA,11/1/2021,,Private,0.1259,5462624.817,,NVIDIA,A100,Calculated from Top500,11/1/2021,,11.59824885,,,,,,,160,NVIDIA,,TRUE,,121,16.69827458,9.98E+16,9.98E+16,4.99E+16,2.496E+16,0.1374464,0.1259,3.96505E+11,5462624.817,,,,,,DeepSeek Fire-Flyer 2,0.016,https://web.archive.org/web/20250126160950/https://top500.org/system/180034/,,,,
Vingroup VinAI Research Superpod,Existing,Likely,Yes,16.99930457,50.44972208,NVIDIA A100,160,Vietnam,Vingroup,5/30/2021,,Private,0.1374464,4319090.538,Vietnam,VinAI Research,A100,who hopes to have in May the new cluster of 20 DGX A100 systems linked together with an NVIDIA Mellanox HDR 200Gb/s InfiniBand network,5/30/2021,,11.56014121,,,,,,,160,NVIDIA,,TRUE,,78,16.69827458,9.98E+16,9.98E+16,4.99E+16,2.496E+16,0.1374464,,3.63196E+11,4319090.538,,,,,,Sunway OceanLight,0.016792741,https://web.archive.org/web/20241213185718/https://blogs.nvidia.com/blog/ai-superpod-vinai/,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.95424251,50,,700,China,,11/15/2019,,Private,0.5,10000000,China,,,,,,11.25527251,,,,,,,700,Anonymized,Anonymized,TRUE,,30,16.95424251,9.00E+16,,9.00E+16,1E+16,0.5,,1.8E+11,10000000,,,,,,Oak Ridge NL Summit,0.03,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,17,50,,,China,,10/15/2021,,Public,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,TRUE,,106,17,1.00E+17,,1.00E+17,,,,,,,,,,,DeepSeek Fire-Flyer 2,0.02,,,,,
Simon Fraser University/Compute Canada Cedar,Existing,Confirmed,Yes,16.98227123,48.50934816,NVIDIA V100,768,Canada,Compute Canada,6/1/2020,,Public,0.5552,14664836.21,"8888 University Dr W, Burnaby, BC V5A 1S6, Canada","Academia,Researchers",V100,Calculated from Top500,6/1/2020,,11.23782178,,,,,,,768,NVIDIA,,TRUE,,45,16.98227123,9.60E+16,,9.60E+16,1.20346E+16,0.5031936,0.5552,1.72911E+11,14664836.21,,,,,,Oak Ridge NL Summit,0.027777778,https://web.archive.org/web/20240915102455/https://top500.org/system/179859/,,,,
SAKURA Internet 2019 V100 Supercomputer,Existing,Confirmed,Yes,16.98227123,48.50934816,NVIDIA Tesla V100 SXM2,768,Japan,Sakura Internet,6/1/2019,,Private,0.51158016,16570910.7,,Cloud,V100,Calculated from Top500,6/1/2019,,11.27335754,,,,,,,768,NVIDIA,,TRUE,,22,16.98227123,9.60E+16,,9.60E+16,1.20346E+16,0.51158016,,1.87654E+11,16570910.7,,,,,,Oak Ridge NL Summit,0.027777778,https://web.archive.org/web/20241213053529/https://top500.org/system/179696/,,,,
Petrobras Fênix Phase 2,Existing,Confirmed,Yes,16.95424251,45.4775139,NVIDIA V100,720,Brazil,Petrobras,6/1/2020,,Public/Private,0.6981,13748283.95,"Rio de Janeiro, Brazil",Petrobras,V100,Calculated from Top500,6/1/2020,,11.11032487,Petrobras Fênix Phase 1,,,,,,720,NVIDIA,,TRUE,,47,16.95424251,9.00E+16,,9.00E+16,1.12824E+16,0.471744,0.6981,1.28921E+11,13748283.95,,,,,,Oak Ridge NL Summit,0.026041667,https://web.archive.org/web/20241119033423/https://top500.org/system/179681/,,,,
Samsung SSC-21 Scalable Module,Existing,Confirmed,Yes,16.95354708,45.40474987,NVIDIA A100,144,Korea (Republic of),Samsung,11/1/2021,This is different than Samsung SSC-21,Private,0.1814,4916362.335,Korea (Republic of),Samsung,A100,Calculated from Top500,11/1/2021,Listed in Top500,11.3938798,,,,,,,144,NVIDIA,,TRUE,,128,16.65251709,8.99E+16,8.99E+16,4.49E+16,2.2464E+16,0.12370176,0.1814,2.47674E+11,4916362.335,,,,,,DeepSeek Fire-Flyer 2,0.0144,https://web.archive.org/web/20241210015858/https://top500.org/system/180042/,,,,
Calcul Québec Béluga,Existing,Confirmed,Yes,16.93449845,43.45629106,NVIDIA V100,688,Canada,Compute Canada,4/26/2019,,Public,0.437,13602178.98,"1100 Notre-Dame St W, Montreal, Quebec H3C 1K3, Canada",Academia,V100,"The GPU subsection, meanwhile, was measured at 2.278 Linpack petaflops and consists of 172 Intel-based servers utilizing 688 Nvidia Volta GPUs",4/26/2019,,11.29401701,,,,,,,688,NVIDIA,,TRUE,,19,16.93449845,8.60E+16,,8.60E+16,1.0781E+16,0.45829056,0.437,1.96796E+11,13602178.98,,,,,,Oak Ridge NL Summit,0.024884259,https://web.archive.org/web/20201024200651/https://www.hpcwire.com/2019/04/26/beluga-supercomputer-now-serving-canadian-researchers/,,,,
Preferred Networks MN-1b,Decommissioned,Confirmed,Yes,16.91985599,42.01556342,NVIDIA V100,512,Japan,Preferred Networks Inc,7/30/2018,Decommissioned July 2022,Private,0.93929472,10104213.84,Japan,Preferred Networks,"V100,P100","The system featured 1,024 Nvidia Tesla P100 GPUs achieving a peak compute of 1.39 petaFLOPS and 9.3 petaFLOPS of SP from the GPU. At the time, the MN-1 ranked 1st in Japan and 12th in the world on the TOP500 among industrial supercomputers. In July 2018 PFN enhanced the MN-1 by adding 512 additional Tesla V100 GPUs",7/30/2018,,10.9470541,,,,,NVIDIA P100,1024,1536,NVIDIA,NVIDIA,,,13,16.91985599,8.31E+16,,8.31E+16,1.75462E+16,0.93929472,,88522588523,10104213.84,,,,,2021-07,Oak Ridge NL Summit,0.024059259,https://web.archive.org/web/20241126234046/https://fuse.wikichip.org/news/3063/japanese-ai-startup-preferred-networks-designed-a-custom-half-petaflops-training-chip/,https://web.archive.org/web/20241119124332/https://www.preferred.jp/en/projects/supercomputers/,,,
NCI Australia Gadi,Existing,Confirmed,Yes,16.90308999,40.4244568,NVIDIA Tesla V100 SXM2,640,Australia,National Computational Infrastructure Australia,1/23/2020,,Public,0.419328,13431927.61,"Canberra ACT 2601, Australia","Academia,Researchers",V100,160 nodes each containing four Nvidia V100 GPUs and two 24-core Intel Xeon Scalable 'Cascade Lake' processor,1/23/2020,,11.28053612,,,,,,,640,NVIDIA,,TRUE,,40,16.90308999,8.00E+16,,8.00E+16,1.00288E+16,0.419328,,1.90781E+11,13431927.61,,,,,,Oak Ridge NL Summit,0.023148148,https://web.archive.org/web/20241014023433/https://www.nci.org.au/our-systems/hpc-systems,,,,
PLUG training cluster,Existing,Likely,Yes,16.90239456,40.35977767,NVIDIA A100,128,,,4/19/2021,,,0.10995712,3455272.43,,,A100,,4/19/2021,,11.56014121,,,,,,,128,NVIDIA,,TRUE,,86,16.60136456,7.99E+16,7.99E+16,3.99E+16,1.9968E+16,0.10995712,,3.63196E+11,3455272.43,,,,,,Sunway OceanLight,0.013434193,https://mp.weixin.qq.com/s/DAQomIkDa52Sef-ruyH5qg,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.90308999,40,,600,China,,7/15/2019,,Private,0.4,10000000,China,,,,,,11.30103,,,,,,,600,Anonymized,Anonymized,TRUE,,28,16.90308999,8.00E+16,,8.00E+16,1E+16,0.4,,2E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.90308999,40,,600,China,,11/15/2019,,Public,0.4,10000000,China,,,,,,11.30103,,,,,,,600,Anonymized,Anonymized,TRUE,,40,16.90308999,8.00E+16,,8.00E+16,1E+16,0.4,,2E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.84509804,40,,600,China,,11/15/2019,,Private,0.4,10000000,China,,,,,,11.24303805,,,,,,,600,Anonymized,Anonymized,TRUE,,50,16.84509804,7.00E+16,,7.00E+16,9E+15,0.4,,1.75E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.90308999,40,,600,China,,7/15/2020,,Public,0.4,10000000,China,,,,,,11.30103,,,,,,,600,Anonymized,Anonymized,TRUE,,64,16.90308999,8.00E+16,,8.00E+16,1E+16,0.4,,2E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.90308999,40,,600,China,,6/15/2020,,Private,0.4,10000000,China,,,,,,11.30103,,,,,,,600,Anonymized,Anonymized,TRUE,,52,16.90308999,8.00E+16,,8.00E+16,1E+16,0.4,,2E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.84509804,40,,600,China,,7/15/2020,,Public/Private,0.4,10000000,China,,,,,,11.24303805,,,TRUE,,,,600,Anonymized,Anonymized,,,80,16.84509804,7.00E+16,,7.00E+16,9E+15,0.4,,1.75E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.90308999,40,,700,China,,6/15/2018,,Private,0.5,10000000,China,,,,,,11.20411998,,,,,,,700,Anonymized,Anonymized,TRUE,,12,16.90308999,8.00E+16,,8.00E+16,1E+16,0.5,,1.6E+11,10000000,,,,,,Meta 2017 V100 Cluster,0.03,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.90308999,40,,600,China,,7/15/2018,,Private,0.4,10000000,China,,,,,,11.30103,,,,,,,600,Anonymized,Anonymized,TRUE,,14,16.90308999,8.00E+16,,8.00E+16,1E+16,0.4,,2E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.84509804,40,,600,China,,6/15/2020,,Private,0.4,10000000,China,,,,,,11.24303805,,,,,,,600,Anonymized,Anonymized,TRUE,,76,16.84509804,7.00E+16,,7.00E+16,9E+15,0.4,,1.75E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.90308999,40,,600,China,,6/15/2020,,Private,0.4,10000000,China,,,,,,11.30103,,,,,,,600,Anonymized,Anonymized,TRUE,,52,16.90308999,8.00E+16,,8.00E+16,1E+16,0.4,,2E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.84509804,40,,600,China,,6/15/2020,,Public/Private,0.4,10000000,China,,,,,,11.24303805,,,,,,,600,Anonymized,Anonymized,TRUE,,71,16.84509804,7.00E+16,,7.00E+16,9E+15,0.4,,1.75E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.84509804,40,,600,China,,11/15/2019,,Private,0.4,10000000,China,,,,,,11.24303805,,,,,,,600,Anonymized,Anonymized,TRUE,,50,16.84509804,7.00E+16,,7.00E+16,9E+15,0.4,,1.75E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.84509804,40,,600,China,,6/15/2020,,Private,0.4,10000000,China,,,,,,11.24303805,,,,,,,600,Anonymized,Anonymized,TRUE,,76,16.84509804,7.00E+16,,7.00E+16,9E+15,0.4,,1.75E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.84509804,40,,600,China,,7/15/2020,,Private,0.3,10000000,China,,,,,,11.36797679,,,,,,,600,Anonymized,Anonymized,TRUE,,79,16.84509804,7.00E+16,,7.00E+16,9E+15,0.3,,2.33333E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.84509804,40,,600,China,,11/15/2019,,Private,0.4,10000000,China,,,,,,11.24303805,,,,,,,600,Anonymized,Anonymized,TRUE,,50,16.84509804,7.00E+16,,7.00E+16,9E+15,0.4,,1.75E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.90308999,40,,600,China,,11/15/2019,,Private,0.4,10000000,China,,,,,,11.30103,,,,,,,600,Anonymized,Anonymized,TRUE,,41,16.90308999,8.00E+16,,8.00E+16,9E+15,0.4,,2E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.90308999,40,,600,China,,6/15/2020,,Private,0.4,10000000,China,,,,,,11.30103,,,,,,,600,Anonymized,Anonymized,TRUE,,64,16.90308999,8.00E+16,,8.00E+16,9E+15,0.4,,2E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.90308999,40,,600,China,,6/15/2020,,Private,0.4,10000000,China,,,,,,11.30103,,,,,,,600,Anonymized,Anonymized,TRUE,,52,16.90308999,8.00E+16,,8.00E+16,1E+16,0.4,,2E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.84509804,40,,600,China,,6/15/2020,,Private,0.4,10000000,China,,,,,,11.24303805,,,,,,,600,Anonymized,Anonymized,TRUE,,76,16.84509804,7.00E+16,,7.00E+16,9E+15,0.4,,1.75E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.90308999,40,,600,China,,7/15/2019,,Private,0.4,10000000,China,,,,,,11.30103,,,TRUE,,,,600,Anonymized,Anonymized,,,32,16.90308999,8.00E+16,,8.00E+16,9E+15,0.4,,2E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.90308999,40,,600,China,,7/15/2019,,Private,0.4,10000000,China,,,,,,11.30103,,,TRUE,,,,600,Anonymized,Anonymized,,,32,16.90308999,8.00E+16,,8.00E+16,9E+15,0.4,,2E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.90308999,40,,600,China,,11/15/2019,,Private,0.4,10000000,China,,,,,,11.30103,,,,,,,600,Anonymized,Anonymized,TRUE,,37,16.90308999,8.00E+16,,8.00E+16,1E+16,0.4,,2E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.84509804,40,,600,China,,11/15/2019,,Private,0.4,10000000,China,,,,,,11.24303805,,,,,,,600,Anonymized,Anonymized,TRUE,,50,16.84509804,7.00E+16,,7.00E+16,9E+15,0.4,,1.75E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.84509804,40,,600,China,,7/15/2019,,Private,0.4,10000000,China,,,,,,11.24303805,,,TRUE,,,,600,Anonymized,Anonymized,,,37,16.84509804,7.00E+16,,7.00E+16,9E+15,0.4,,1.75E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.90308999,40,,600,China,,11/15/2019,,Private,0.4,10000000,China,,,,,,11.30103,,,,,,,600,Anonymized,Anonymized,TRUE,,41,16.90308999,8.00E+16,,8.00E+16,9E+15,0.4,,2E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.90308999,40,,600,China,,11/15/2019,,Private,0.4,10000000,China,,,,,,11.30103,,,,,,,600,Anonymized,Anonymized,TRUE,,41,16.90308999,8.00E+16,,8.00E+16,9E+15,0.4,,2E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.84509804,40,,600,China,,6/15/2020,,Private,0.4,10000000,China,,,,,,11.24303805,,,,,,,600,Anonymized,Anonymized,TRUE,,76,16.84509804,7.00E+16,,7.00E+16,9E+15,0.4,,1.75E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.90308999,40,,600,China,,6/15/2020,,Private,0.4,10000000,China,,,,,,11.30103,,,,,,,600,Anonymized,Anonymized,TRUE,,64,16.90308999,8.00E+16,,8.00E+16,9E+15,0.4,,2E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.90308999,40,,600,China,,6/15/2019,,Private,0.4,10000000,China,,,,,,11.30103,,,,,,,600,Anonymized,Anonymized,TRUE,,26,16.90308999,8.00E+16,,8.00E+16,1E+16,0.4,,2E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.84509804,40,,600,China,,7/15/2019,,Private,0.4,10000000,China,,,,,,11.24303805,,,TRUE,,,,600,Anonymized,Anonymized,,,37,16.84509804,7.00E+16,,7.00E+16,9E+15,0.4,,1.75E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.84509804,40,,600,China,,6/15/2020,,Private,0.4,10000000,China,,,,,,11.24303805,,,,,,,600,Anonymized,Anonymized,TRUE,,76,16.84509804,7.00E+16,,7.00E+16,9E+15,0.4,,1.75E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.84509804,40,,600,China,,7/15/2019,,Private,0.4,10000000,China,,,,,,11.24303805,,,TRUE,,,,600,Anonymized,Anonymized,,,37,16.84509804,7.00E+16,,7.00E+16,9E+15,0.4,,1.75E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.84509804,40,,600,China,,6/15/2020,,Private,0.4,10000000,China,,,,,,11.24303805,,,,,,,600,Anonymized,Anonymized,TRUE,,76,16.84509804,7.00E+16,,7.00E+16,9E+15,0.4,,1.75E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.84509804,40,,600,China,,11/15/2020,,Public/Private,0.4,10000000,China,,,,,,11.24303805,,,,,,,600,Anonymized,Anonymized,TRUE,,93,16.84509804,7.00E+16,,7.00E+16,9E+15,0.4,,1.75E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.84509804,40,,600,China,,7/15/2020,,Private,0.4,10000000,China,,,,,,11.24303805,,,TRUE,,,,600,Anonymized,Anonymized,,,80,16.84509804,7.00E+16,,7.00E+16,9E+15,0.4,,1.75E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.90308999,40,,600,China,,7/15/2020,,Private,0.4,10000000,China,,,,,,11.30103,,,TRUE,,,,600,Anonymized,Anonymized,,,66,16.90308999,8.00E+16,,8.00E+16,9E+15,0.4,,2E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.90308999,40,,600,China,,7/15/2019,,Private,0.4,10000000,China,,,,,,11.30103,,,TRUE,,,,600,Anonymized,Anonymized,,,31,16.90308999,8.00E+16,,8.00E+16,1E+16,0.4,,2E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.90308999,40,,600,China,,11/15/2019,,Private,0.4,10000000,China,,,,,,11.30103,,,,,,,600,Anonymized,Anonymized,TRUE,,37,16.90308999,8.00E+16,,8.00E+16,1E+16,0.4,,2E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
RPI Supercomputer 2,Existing,Confirmed,Yes,16.88081359,38.40323396,NVIDIA V100,608,United States of America,Rensselaer Polytechnic Institute,6/1/2020,,Public,0.3983616,11609662,"Troy, New York","Researchers,Academia",V100,Calculated from Top500,6/1/2020,,11.28053612,,,,,,,608,NVIDIA,,TRUE,,63,16.88081359,7.60E+16,,7.60E+16,9.52736E+15,0.3983616,,1.90781E+11,11609662,,,,,,Oak Ridge NL Summit,0.021990741,https://web.archive.org/web/20241008115946/https://www.top500.org/system/179852/,,,,
Oak Ridge NL Titan,Decommissioned,Confirmed,Yes,16.86650756,37.15880748,NVIDIA Tesla K20X,18688,United States of America,US Department of Energy,10/29/2012,,Public,15.8054,281156203.1,"Oak Ridge, Tennessee, United States",Oak Ridge National Laboratory,K20X,"It used 18,688 CPUs paired with an equal number of GPUs",10/29/2012,,,,,,,,,18688,NVIDIA,,,,1,,7.35E+16,,,7.35373E+16,8.79214336,15.8054,,281156203.1,,,,,8/2/2019,Oak Ridge NL Titan,1,https://web.archive.org/web/20241123132125/https://en.wikipedia.org/wiki/Titan_(supercomputer),,,,
Microsoft Research Hypercluster,Existing,Confirmed,Yes,16.8573325,36.38201112,NVIDIA V100,576,United States of America,Microsoft,11/1/2019,,Private,0.38368512,11234945.02,,Microsoft,V100,Calculated from Top500,11/1/2019,Listed in Top500,11.27335754,,,,,,,576,NVIDIA,,TRUE,,47,16.8573325,7.20E+16,,7.20E+16,9.02592E+15,0.38368512,,1.87654E+11,11234945.02,,,,,,Oak Ridge NL Summit,0.020833333,https://web.archive.org/web/20240530033239/https://www.top500.org/system/179775/,,,,
Petrobras Fênix Phase 1,Existing,Confirmed,Yes,16.8573325,36.38201112,NVIDIA V100,576,Brazil,Petrobras,6/1/2019,,Public/Private,0.5226,11367240.57,"Rio de Janeiro, Brazil",Petrobras,V100,Calculated from Top500,6/1/2019,,11.13916309,,Petrobras Fênix Phase 2,,,,,576,NVIDIA,,TRUE,,29,16.8573325,7.20E+16,,7.20E+16,9.02592E+15,0.38368512,0.5226,1.37773E+11,11367240.57,,,,,,Oak Ridge NL Summit,0.020833333,https://web.archive.org/web/20241119033423/https://top500.org/system/179681/,,,,
NVIDIA Circe,Existing,Confirmed,Yes,16.8573325,36.38201112,NVIDIA Tesla V100 SXM2,576,United States of America,NVIDIA,11/1/2018,,Private,0.39626496,12518899.69,,NVIDIA,V100,Calculated from Top500,11/1/2018,,11.25934683,,,,,,,576,NVIDIA,,TRUE,,22,16.8573325,7.20E+16,,7.20E+16,9.02592E+15,0.39626496,,1.81697E+11,12518899.69,,,,,,Oak Ridge NL Summit,0.020833333,https://web.archive.org/web/20241224052126/https://www.top500.org/system/179564/,,,,
TACC Frontera,Existing,Confirmed,Yes,16.80636994,32.353714,NVIDIA Quadro RTX 5000,360,United States of America,University of Texas at Austin,8/31/2019,,Public,0.48227088,8746120.582,"10100 Burnet Rd, Austin, TX 78758","Academia,Researchers","V100,360 RTX 5000s 448 V100s","A 360 NVIDIA Quadro RTX 5000 GPU (graphics processing unit) system submerged in liquid coolant racks...
An IBM POWER9-hosted system with 448 NVIDIA V100 GPUs will provide additional performance",8/31/2019,,11.1230789,,,,,NVIDIA V100,448,808,NVIDIA,NVIDIA,TRUE,,43,16.80636994,6.40E+16,,6.40E+16,1.10342E+16,0.48227088,,1.32764E+11,8746120.582,,,,,,Oak Ridge NL Summit,0.01852662,https://web.archive.org/web/20240816171341/https://new.nsf.gov/news/nsf-funded-leadership-class-computing-center,https://web.archive.org/web/20230714051808/https://texascale.org/2019/feature-stories/operation-frontera/,,,
Paper on SEER,Existing,Confirmed,Yes,16.80617997,32.33956544,NVIDIA V100,512,,Meta AI,3/2/2021,,Private,0.32987136,9683204.93,,Meta,V100,"""Training this model on 1 billion images requires 114,890 training iterations for a batch size of 8, 704 images, summing to 8 days of training over 512 GPUs.""",3/2/2021,,11.28783536,,,TRUE,Meta 2017 V100 Cluster,,,512,NVIDIA,,,,136,16.80617997,6.40E+16,,6.40E+16,8.02304E+15,0.32987136,,1.94015E+11,9683204.93,,,,,,Sunway OceanLight,0.010764578,https://arxiv.org/abs/2103.01988,,,,
Paper on Megatron-LM,Existing,Confirmed,Yes,16.80617997,32.33956544,NVIDIA Tesla V100 DGXS 32 GB,512,United States of America,NVIDIA,9/17/2019,,Private,0.2842112,15628697.91,,,V100,,9/17/2019,,11.35253879,,,TRUE,NVIDIA Circe,,,512,NVIDIA,,,,44,16.80617997,6.40E+16,,6.40E+16,8.02304E+15,0.2842112,,2.25185E+11,15628697.91,,,,,,Oak Ridge NL Summit,0.018518519,https://arxiv.org/pdf/1909.08053,,,,
Paper on SEER Training,Existing,Confirmed,Yes,16.80617997,32.33956544,NVIDIA Tesla V100 DGXS 32 GB,512,,Meta AI,3/5/2021,,Private,0.2748928,15153867.68,,,,,3/5/2021,,11.36701661,,,TRUE,Meta 2017 V100 Cluster,,,512,NVIDIA,,,,136,16.80617997,6.40E+16,,6.40E+16,8.02304E+15,0.2748928,,2.32818E+11,15153867.68,,,,,,Sunway OceanLight,0.010764578,https://web.archive.org/web/20240921010425/https://developer.nvidia.com/blog/facebook-self-supervised-ai/,Cluster performance database,,,
Paper on CogView,Existing,Confirmed,Yes,16.80617997,32.33956544,NVIDIA Tesla V100 SXM2 32 GB,512,,Alibaba,5/26/2021,,Private,0.32987136,10557469.74,,,V100,"We train the model with batch size of 6,144 sequences (6.7 million tokens per batch) for 144,000 steps on 512 V100 GPUs (32GB).",5/26/2021,,11.28783536,,,TRUE,Alibaba MLPerf 0.7 Submission,,,512,NVIDIA,,,,145,16.80617997,6.40E+16,,6.40E+16,8.02304E+15,0.32987136,,1.94015E+11,10557469.74,,,,,,Sunway OceanLight,0.010764578,https://arxiv.org/abs/2105.13290,,,,
University of Oxford Jade2,Existing,Confirmed,Yes,16.79934055,31.83425973,NVIDIA Tesla V100 SXM2,504,United Kingdom of Great Britain and Northern Ireland,University of Oxford,11/1/2020,Planned to be decommissioned in 2025,Public,0.2024,10476112.39,"The Hartree Centre STFC Laboratory Sci-Tech Daresbury Warrington, Warrington WA4 4AD, United Kingdom",Academia,V100,Calculated from Top500,11/1/2020,,11.49313004,,,,,,,504,NVIDIA,,TRUE,,133,16.79934055,6.30E+16,,6.30E+16,7.89768E+15,0.3302208,0.2024,3.11265E+11,10476112.39,,,,,,Oak Ridge NL Summit,0.018229167,https://web.archive.org/web/20241119014213/https://top500.org/system/179867/,,,,
Paper on ALIGN,Existing,Likely,Yes,16.79917507,31.82213239,Google TPU v3,512,,Google,2/11/2021,,Private,0.241905664,4874041.803,,Google,TPU v3,,2/11/2021,,11.41552904,,,TRUE,Google MLPerf 0.7 Submission,,,512,Google,,,,140,16.79917507,6.30E+16,,6.30E+16,,0.241905664,,2.60333E+11,4874041.803,,,,,,Oak Ridge NL Summit,0.018222222,https://arxiv.org/abs/2102.05918,,,,
Paper on T5,Existing,Likely,Yes,16.79917507,31.82213239,Google TPU v3,512,,Google,10/23/2019,,Private,0.250105856,5009310.644,,,TPU v3,,10/23/2019,,11.40105121,,,TRUE,Google TPUv3 POD Generic,,,512,Google,,,,48,16.79917507,6.30E+16,,6.30E+16,,0.250105856,,2.51797E+11,5009310.644,,,,,,Oak Ridge NL Summit,0.018222222,https://arxiv.org/abs/1910.10683,,,,
Paper on ALBERT,Existing,Confirmed,Yes,16.79917507,31.82213239,Google TPU v3,512,,Google,9/26/2019,,Private,0.250105856,5009310.644,,Google,TPU v3,,9/26/2019,,11.40105121,,,TRUE,Google TPUv3 POD Generic,,,512,Google,,,,48,16.79917507,6.30E+16,,6.30E+16,,0.250105856,,2.51797E+11,5009310.644,,,,,,Oak Ridge NL Summit,0.018222222,https://arxiv.org/abs/1909.11942,,,,
Paper on XLM-R,Existing,Confirmed,Yes,16.79588002,31.58160687,NVIDIA Tesla V100 SXM2 32 GB,500,,Meta AI,4/8/2020,,Private,0.3276,10455368.65,,,,,4/8/2020,,11.28053612,,,TRUE,Meta 2017 V100 Cluster,,,500,NVIDIA,,,,84,16.79588002,6.25E+16,,6.25E+16,7.835E+15,0.3276,,1.90781E+11,10455368.65,,,,,,Oak Ridge NL Summit,0.018084491,https://arxiv.org/pdf/1911.02116.pdf,Cluster performance database,,,
MIT Supercloud,Existing,Confirmed,Yes,16.78532984,30.82364831,NVIDIA V100,488,United States of America,Massachusetts Institute of Technology (MIT),9/21/2020,,Public,0.3197376,9277625.666,"50 Water St, Holyoke, MA 01040","Academia,MIT",V100,Total GPUs: 488,9/21/2020,,11.28053612,,,,,,,488,NVIDIA,,TRUE,,126,16.78532984,6.10E+16,,6.10E+16,7.64696E+15,0.3197376,,1.90781E+11,9277625.666,,,,,,Oak Ridge NL Summit,0.017650463,https://web.archive.org/web/20200921072313/https://supercloud.mit.edu/systems-and-software,,,,
Paper on M6-T,Existing,Confirmed,Yes,16.77815125,30.3183426,NVIDIA Tesla V100 SXM2 32 GB,480,,Alibaba,5/31/2021,,Private,0.3092544,9897627.877,,,V100,"Weadvance our models to 1 trillion parameters and successfully implement it on solely 480 NVIDIA V100-32GB GPUs, in comparison with the recent SOTAs on 2048 TPU cores.",5/31/2021,,11.28783536,,,TRUE,Alibaba MLPerf 0.7 Submission,,,480,NVIDIA,,,,165,16.77815125,6.00E+16,,6.00E+16,7.5216E+15,0.3092544,,1.94015E+11,9897627.877,,,,,,Sunway OceanLight,0.010091792,https://arxiv.org/abs/2105.15082,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.84509804,30,,500,China,,11/15/2019,,Public/Private,0.3,10000000,China,,,,,,11.36797679,,,,,,,500,Anonymized,Anonymized,TRUE,,60,16.84509804,7.00E+16,,7.00E+16,8E+15,0.3,,2.33333E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.77815125,30,,500,China,,6/15/2019,,Private,0.3,9000000,China,,,,,,11.30103,,,,,,,500,Anonymized,Anonymized,TRUE,,32,16.77815125,6.00E+16,,6.00E+16,8E+15,0.3,,2E+11,9000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.69897,30,,400,China,,11/15/2019,,Private,0.3,8000000,China,,,,,,11.22184875,,,,,,,400,Anonymized,Anonymized,TRUE,,91,16.69897,5.00E+16,,5.00E+16,6E+15,0.3,,1.66667E+11,8000000,,,,,,Oak Ridge NL Summit,0.01,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.77815125,30,,500,China,,7/15/2019,,Public,0.3,10000000,China,,,,,,11.30103,,,,,,,500,Anonymized,Anonymized,TRUE,,43,16.77815125,6.00E+16,,6.00E+16,8E+15,0.3,,2E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.77815125,30,,500,China,,11/15/2018,,Private,0.3,10000000,China,,,,,,11.30103,,,,,,,500,Anonymized,Anonymized,TRUE,,23,16.77815125,6.00E+16,,6.00E+16,8E+15,0.3,,2E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.84509804,30,,500,China,,11/15/2019,,Private,0.4,10000000,China,,,,,,11.24303805,,,,,,,500,Anonymized,Anonymized,TRUE,,57,16.84509804,7.00E+16,,7.00E+16,9E+15,0.4,,1.75E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.77815125,30,,500,China,,11/15/2020,,Public,0.3,9000000,China,,,,,,11.30103,,,,,,,500,Anonymized,Anonymized,TRUE,,140,16.77815125,6.00E+16,,6.00E+16,8E+15,0.3,,2E+11,9000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.84509804,30,,500,China,,11/15/2019,,Private,0.3,10000000,China,,,,,,11.36797679,,,,,,,500,Anonymized,Anonymized,TRUE,,60,16.84509804,7.00E+16,,7.00E+16,8E+15,0.3,,2.33333E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.77815125,30,,500,China,,7/15/2019,,Private,0.3,10000000,China,,,,,,11.30103,,,,,,,500,Anonymized,Anonymized,TRUE,,41,16.77815125,6.00E+16,,6.00E+16,8E+15,0.3,,2E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.84509804,30,,500,China,,7/15/2020,,Public/Private,0.3,10000000,China,,,,,,11.36797679,,,TRUE,,,,500,Anonymized,Anonymized,,,98,16.84509804,7.00E+16,,7.00E+16,8E+15,0.3,,2.33333E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.69897,30,,400,China,,11/15/2019,,Private,0.3,8000000,China,,,,,,11.22184875,,,,,,,400,Anonymized,Anonymized,TRUE,,91,16.69897,5.00E+16,,5.00E+16,6E+15,0.3,,1.66667E+11,8000000,,,,,,Oak Ridge NL Summit,0.01,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.84509804,30,,500,China,,6/15/2020,,Private,0.3,10000000,China,,,,,,11.36797679,,,,,,,500,Anonymized,Anonymized,TRUE,,92,16.84509804,7.00E+16,,7.00E+16,8E+15,0.3,,2.33333E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.77815125,30,,500,China,,7/15/2020,,Private,0.3,10000000,China,,,,,,11.30103,,,TRUE,,,,500,Anonymized,Anonymized,,,121,16.77815125,6.00E+16,,6.00E+16,8E+15,0.3,,2E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.84509804,30,,500,China,,6/15/2020,,Private,0.3,10000000,China,,,,,,11.36797679,,,,,,,500,Anonymized,Anonymized,TRUE,,92,16.84509804,7.00E+16,,7.00E+16,8E+15,0.3,,2.33333E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.84509804,30,,500,China,,11/15/2019,,Private,0.4,10000000,China,,,,,,11.24303805,,,,,,,500,Anonymized,Anonymized,TRUE,,57,16.84509804,7.00E+16,,7.00E+16,9E+15,0.4,,1.75E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.77815125,30,,500,China,,11/15/2019,,Private,0.3,9000000,China,,,,,,11.30103,,,,,,,500,Anonymized,Anonymized,TRUE,,75,16.77815125,6.00E+16,,6.00E+16,8E+15,0.3,,2E+11,9000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.77815125,30,,500,China,,11/15/2019,,Private,0.3,9000000,China,,,,,,11.30103,,,,,,,500,Anonymized,Anonymized,TRUE,,75,16.77815125,6.00E+16,,6.00E+16,8E+15,0.3,,2E+11,9000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.69897,30,,400,China,,6/15/2019,,Private,0.3,8000000,China,,,,,,11.22184875,,,,,,,400,Anonymized,Anonymized,TRUE,,40,16.69897,5.00E+16,,5.00E+16,6E+15,0.3,,1.66667E+11,8000000,,,,,,Oak Ridge NL Summit,0.01,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.84509804,30,,500,China,,11/15/2020,,Public/Private,0.3,10000000,China,,,,,,11.36797679,,,,,,,500,Anonymized,Anonymized,TRUE,,113,16.84509804,7.00E+16,,7.00E+16,8E+15,0.3,,2.33333E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.84509804,30,,500,China,,6/15/2020,,Public/Private,0.3,10000000,China,,,,,,11.36797679,,,,,,,500,Anonymized,Anonymized,TRUE,,92,16.84509804,7.00E+16,,7.00E+16,8E+15,0.3,,2.33333E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.77815125,30,,500,China,,6/15/2019,,Private,0.3,10000000,China,,,,,,11.30103,,,,,,,500,Anonymized,Anonymized,TRUE,,31,16.77815125,6.00E+16,,6.00E+16,8E+15,0.3,,2E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.84509804,30,,500,China,,6/15/2020,,Private,0.3,10000000,China,,,,,,11.36797679,,,,,,,500,Anonymized,Anonymized,TRUE,,92,16.84509804,7.00E+16,,7.00E+16,8E+15,0.3,,2.33333E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.77815125,30,,500,China,,11/15/2019,,Private,0.3,9000000,China,,,,,,11.30103,,,,,,,500,Anonymized,Anonymized,TRUE,,75,16.77815125,6.00E+16,,6.00E+16,8E+15,0.3,,2E+11,9000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.77815125,30,,500,China,,11/15/2019,,Private,0.3,10000000,China,,,,,,11.30103,,,,,,,500,Anonymized,Anonymized,TRUE,,65,16.77815125,6.00E+16,,6.00E+16,8E+15,0.3,,2E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.77815125,30,,500,China,,7/15/2020,,Public,0.3,10000000,China,,,,,,11.30103,,,,,,,500,Anonymized,Anonymized,TRUE,,135,16.77815125,6.00E+16,,6.00E+16,7E+15,0.3,,2E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.77815125,30,,500,China,,11/15/2019,,Private,0.3,10000000,China,,,,,,11.30103,,,TRUE,,,,500,Anonymized,Anonymized,,,65,16.77815125,6.00E+16,,6.00E+16,8E+15,0.3,,2E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.69897,30,,400,China,,11/15/2019,,Private,0.3,8000000,China,,,,,,11.22184875,,,,,,,400,Anonymized,Anonymized,TRUE,,91,16.69897,5.00E+16,,5.00E+16,6E+15,0.3,,1.66667E+11,8000000,,,,,,Oak Ridge NL Summit,0.01,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.69897,30,,400,China,,11/15/2019,,Private,0.3,8000000,China,,,,,,11.22184875,,,,,,,400,Anonymized,Anonymized,TRUE,,91,16.69897,5.00E+16,,5.00E+16,6E+15,0.3,,1.66667E+11,8000000,,,,,,Oak Ridge NL Summit,0.01,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.77815125,30,,500,China,,11/15/2019,,Private,0.3,10000000,China,,,,,,11.30103,,,,,,,500,Anonymized,Anonymized,TRUE,,65,16.77815125,6.00E+16,,6.00E+16,8E+15,0.3,,2E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.77815125,30,,500,China,,11/15/2019,,Private,0.3,10000000,China,,,,,,11.30103,,,,,,,500,Anonymized,Anonymized,TRUE,,74,16.77815125,6.00E+16,,6.00E+16,8E+15,0.3,,2E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.77815125,30,,500,China,,7/15/2020,,Private,0.3,10000000,China,,,,,,11.30103,,,TRUE,,,,500,Anonymized,Anonymized,,,134,16.77815125,6.00E+16,,6.00E+16,7E+15,0.3,,2E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Likely,Yes,16.77815125,30,,,China,,11/15/2017,,Private,1,,China,,,,,,,,,,,,,5000,Anonymized,Anonymized,TRUE,,5,,6.00E+16,,,6E+16,,1,,,,,,,,Meta 2017 V100 Cluster,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.77815125,30,,500,China,,6/15/2020,,Private,0.3,9000000,China,,,,,,11.30103,,,,,,,500,Anonymized,Anonymized,TRUE,,112,16.77815125,6.00E+16,,6.00E+16,8E+15,0.3,,2E+11,9000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.84509804,30,,500,China,,7/15/2019,,Public/Private,0.3,10000000,China,,,,,,11.36797679,,,TRUE,,,,500,Anonymized,Anonymized,,,40,16.84509804,7.00E+16,,7.00E+16,8E+15,0.3,,2.33333E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.69897,30,,400,China,,6/15/2018,,Private,0.3,8000000,China,,,,,,11.22184875,,,,,,,400,Anonymized,Anonymized,TRUE,,19,16.69897,5.00E+16,,5.00E+16,6E+15,0.3,,1.66667E+11,8000000,,,,,,Meta 2017 V100 Cluster,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.69897,30,,400,China,,11/15/2019,,Private,0.3,8000000,China,,,,,,11.22184875,,,,,,,400,Anonymized,Anonymized,TRUE,,91,16.69897,5.00E+16,,5.00E+16,6E+15,0.3,,1.66667E+11,8000000,,,,,,Oak Ridge NL Summit,0.01,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.84509804,30,,500,China,,11/15/2019,,Private,0.4,10000000,China,,,,,,11.24303805,,,,,,,500,Anonymized,Anonymized,TRUE,,57,16.84509804,7.00E+16,,7.00E+16,9E+15,0.4,,1.75E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.77815125,30,,500,China,,6/15/2020,,Private,0.3,9000000,China,,,,,,11.30103,,,,,,,500,Anonymized,Anonymized,TRUE,,112,16.77815125,6.00E+16,,6.00E+16,8E+15,0.3,,2E+11,9000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.77815125,30,,500,China,,7/15/2019,,Public,0.3,10000000,China,,,,,,11.30103,,,TRUE,,,,500,Anonymized,Anonymized,,,44,16.77815125,6.00E+16,,6.00E+16,8E+15,0.3,,2E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.77815125,30,,500,China,,11/15/2020,,Private,0.3,9000000,China,,,,,,11.30103,,,,,,,500,Anonymized,Anonymized,TRUE,,140,16.77815125,6.00E+16,,6.00E+16,8E+15,0.3,,2E+11,9000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.84509804,30,,500,China,,11/15/2019,,Private,0.3,10000000,China,,,,,,11.36797679,,,,,,,500,Anonymized,Anonymized,TRUE,,60,16.84509804,7.00E+16,,7.00E+16,8E+15,0.3,,2.33333E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.84509804,30,,500,China,,11/15/2020,,Private,0.3,10000000,China,,,,,,11.36797679,,,,,,,500,Anonymized,Anonymized,TRUE,,113,16.84509804,7.00E+16,,7.00E+16,8E+15,0.3,,2.33333E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.77815125,30,,500,China,,11/15/2019,,Public,0.3,9000000,China,,,,,,11.30103,,,,,,,500,Anonymized,Anonymized,TRUE,,86,16.77815125,6.00E+16,,6.00E+16,7E+15,0.3,,2E+11,9000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.77815125,30,,500,China,,11/15/2019,,Private,0.3,9000000,China,,,,,,11.30103,,,,,,,500,Anonymized,Anonymized,TRUE,,75,16.77815125,6.00E+16,,6.00E+16,8E+15,0.3,,2E+11,9000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.84509804,30,,500,China,,7/15/2020,,Private,0.3,10000000,China,,,,,,11.36797679,,,TRUE,,,,500,Anonymized,Anonymized,,,98,16.84509804,7.00E+16,,7.00E+16,8E+15,0.3,,2.33333E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Likely,Yes,16.69897,30,,,China,,11/15/2017,,Private,1,,China,,,,,,,,,,,,,4000,Anonymized,Anonymized,TRUE,,7,,5.00E+16,,,5E+16,,1,,,,,,,,Meta 2017 V100 Cluster,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.84509804,30,,500,China,,7/15/2020,,Private,0.3,10000000,China,,,,,,11.36797679,,,TRUE,,,,500,Anonymized,Anonymized,,,98,16.84509804,7.00E+16,,7.00E+16,8E+15,0.3,,2.33333E+11,10000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.77815125,30,,500,China,,11/15/2019,,Private,0.3,9000000,China,,,,,,11.30103,,,,,,,500,Anonymized,Anonymized,TRUE,,75,16.77815125,6.00E+16,,6.00E+16,8E+15,0.3,,2E+11,9000000,,,,,,Oak Ridge NL Summit,0.02,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.77815125,30,,1000,China,,8/15/2018,,Public,0.3,,China,,,,,,11.30103,,,,,,,1000,Anonymized,Anonymized,TRUE,,20,16.77815125,6.00E+16,,6.00E+16,3E+16,,0.3,2E+11,,,,,,,Oak Ridge NL Summit,0.02,,,,,
Eni HPC4 Phase 1,Existing,Confirmed,Yes,16.77290087,29.95401718,NVIDIA P100,3170,Italy,Eni,1/18/2018,,Private,2.4824,,"Via per la Corradina, 35, 27032 Ferrera Erbognone PV, Italy",Eni,P100,Calculated from Top500,1/18/2018,,10.37802911,,Eni HPC4 Phase 2,,,,,3170,NVIDIA,,TRUE,,8,16.77290087,5.93E+16,,5.93E+16,2.9481E+16,1.817361,2.4824,23879713181,,,,,,,Meta 2017 V100 Cluster,0.021556,https://web.archive.org/web/20250213075947/https://top500.org/system/179444/,https://web.archive.org/web/20241223213852/https://www.top500.org/news/eni-launches-186-petaflop-supercomputer/,,,
CSCS Piz Daint Phase 1,Existing,Confirmed,Yes,16.76276856,29.26326428,NVIDIA Tesla P100 PCIe 16GB,3040,Switzerland,ETH Domain,11/1/2016,,Public,1.811992,31773581.06,,"Swiss Universities,Academia",P100,Calculated from Top500,11/1/2016,,10.50461229,,CSCS Piz Daint Phase 2,,,,,3040,NVIDIA,,TRUE,,2,16.76276856,5.79E+16,,5.79E+16,2.8959E+16,1.811992,,31960406006,31773581.06,,,,,,Oak Ridge NL Titan,0.787518929,https://web.archive.org/web/20240925211711/https://www.cscs.ch/computers/piz-daint,https://web.archive.org/web/20240519145824/https://www.cscs.ch/publications/news/2017/piz-daint-one-of-the-most-powerful-supercomputers-in-the-world,,,
TACC Longhorn,Decommissioned,Confirmed,Yes,16.74818803,28.29711976,NVIDIA Tesla V100 SXM2,448,United States of America,University of Texas at Austin,11/1/2019,Decommissioned in November 2022,Public,0.29842176,9553864.359,"Austin, Texas","Academia,Researchers",V100,The researchers ran simulations of the coronavirus spike protein on 100 out of 448 of Longhorn’s NVIDIA V100 GPUs.,11/1/2019,,11.27335754,,,,,,,448,NVIDIA,,,,88,16.74818803,5.60E+16,,5.60E+16,7.02016E+15,0.29842176,,1.87654E+11,9553864.359,,,,,11/30/2022,Oak Ridge NL Summit,0.016203704,https://web.archive.org/web/20240804184834/https://insidehpc.com/2021/08/taccs-longhorn-gpu-subsystem-helps-researchers-detect-potential-covid-19-virus-vulnerability/,,,,
Continental DGX Supercomputer,Existing,Confirmed,Yes,16.74818803,28.29711976,NVIDIA V100,448,Germany,Continental AG,7/28/2020,,Private,0.2935296,8532054.694,"Frankfurt, Germany",Continental,V100,"53 Nvidia  DGX server, 448 NVIDIA Tesla V100 GPUs",7/28/2020,,11.28053612,,,,,,,448,NVIDIA,,TRUE,,139,16.74818803,5.60E+16,,5.60E+16,7.02016E+15,0.2935296,,1.90781E+11,8532054.694,,,,,,Oak Ridge NL Summit,0.016203704,https://archive.ph/Ms4E5,https://web.archive.org/web/20240417205902/https://www.continental.com/en-us/press/press-releases/continental-supercomputer-nvidia/,,,
"""Center for Advanced Intelligence Project, RIKEN, RAIDEN""",Existing,Confirmed,Yes,16.71600334,26.27589692,NVIDIA V100,416,Japan,RIKEN,6/1/2018,,Public,0.2008,8202244.176,"Japan, 〒103-0027 Tokyo, Chuo City, Nihonbashi, 1 Chome−4−1, Nihonbashi 1-chome Mitsui Building, 15階","RIKEN,Researchers",V100,Calculated from Top500,6/1/2018,,11.41323964,,,,,,,416,NVIDIA,,TRUE,,17,16.71600334,5.20E+16,,5.20E+16,6.51872E+15,0.28619136,0.2008,2.58964E+11,8202244.176,,,,,,Meta 2017 V100 Cluster,0.018909091,https://web.archive.org/web/20240528101359/https://www.top500.org/system/179413/,,,,
Paper on AlphaStar,Existing,Confirmed,Yes,16.67423634,23.86659929,Google TPU v3,384,,DeepMind,10/30/2019,,Private,0.187579392,3756982.983,,Google,TPU v3,,10/30/2019,,11.40105121,,,TRUE,Google TPUv3 POD Generic,,,384,Google,,,,62,16.67423634,4.72E+16,,4.72E+16,,0.187579392,,2.51797E+11,3756982.983,,,,,,Oak Ridge NL Summit,0.013666667,https://www.nature.com/articles/s41586-019-1724-z,,,,
Laboratório Nacional de Computação Científica Santos Dumont,Existing,Confirmed,Yes,16.66275783,23.24406266,NVIDIA Tesla V100 SXM2,368,Brazil,Laboratório Nacional de Computação Científica,11/1/2019,,Public,0.24513216,7847817.152,"Av. Getulio Vargas, 333 - Quitandinha, Petrópolis - RJ, 25651-075, Brazil","Academia,Researchers",V100,Calculated from Top500,11/1/2019,,11.27335754,,,,,,,368,NVIDIA,,TRUE,,99,16.66275783,4.60E+16,,4.60E+16,5.76656E+15,0.24513216,,1.87654E+11,7847817.152,,,,,,Oak Ridge NL Summit,0.013310185,https://web.archive.org/web/20240516065301/https://en.wikipedia.org/wiki/Santos_Dumont_(supercomputer),,,,
GSIC TSUBAME 3.0,Existing,Confirmed,Yes,16.66039414,23.11789793,NVIDIA Tesla P100 SXM2,2156,Japan,Tokyo Institute of Technology,11/1/2017,,Public,1.525,42551301.77,"Ookayama, Japan","Academia,Tokyo Tech",P100,Calculated from Top500,11/1/2017,,10.47712429,,,,,,,2156,NVIDIA,,TRUE,,9,16.66039414,4.58E+16,,4.58E+16,2.28752E+16,1.51855704,1.525,30000209836,42551301.77,,,,,,Meta 2017 V100 Cluster,0.01663648,https://web.archive.org/web/20230716170424/https://www.hpcwire.com/2017/02/16/tokyo-techs-tsubame-3-0-supercomputer/,,,,
RPI AiMOSx NPL Cluster,Existing,Confirmed,Yes,16.60205999,20.2122284,NVIDIA V100,320,United States of America,Rensselaer Polytechnic Institute,6/1/2020,,Public,0.209664,6110348.422,"Troy, New York","Researchers,Academia",V100,"40 nodes, 2x20 Core Xeon processors with 8 v100 Nvidia GPUS and 768 GB of RAM each",6/1/2020,Possibly actually has 608 V100s,11.28053612,,,TRUE,RPI AiMOS,,,320,NVIDIA,,,,147,16.60205999,4.00E+16,,4.00E+16,5.0144E+15,0.209664,,1.90781E+11,6110348.422,,,,,,Oak Ridge NL Summit,0.011574074,https://web.archive.org/web/20210814143702/https://cci.rpi.edu/center-capabilities,,,,
CSC Puhti,Existing,Confirmed,Yes,16.60205999,20.2122284,NVIDIA V100,320,Finland,CSC – IT Center for Science,9/3/2019,,Public,0.2131584,6247228.987,"Tehdaskatu 15, 87100 Kajaani, Finland","Academia,Researchers",V100,has 80 nodes with a total peak performance of 2.7 Petaflops. Each node has four Nvidia Volta V100 GPUs,9/3/2019,,11.27335754,,,,,,,320,NVIDIA,,TRUE,,65,16.60205999,4.00E+16,,4.00E+16,5.0144E+15,0.2131584,,1.87654E+11,6247228.987,,,,,,Oak Ridge NL Summit,0.011574074,https://archive.ph/2BTmI,,,,
University of Tsukuba Cygnus,Existing,Confirmed,Yes,16.60205999,20.2122284,NVIDIA V100,320,Japan,University of Tsukuba,4/30/2019,,Public,0.2131584,6326594.873,"Tsukuba, Japan",Academia,V100,Calculated from Top500,4/30/2019,,11.27335754,,,,,,,320,NVIDIA,,TRUE,,39,16.60205999,4.00E+16,,4.00E+16,5.0144E+15,0.2131584,,1.87654E+11,6326594.873,,,,,,Oak Ridge NL Summit,0.011574074,https://web.archive.org/web/20241006142717/https://www.top500.org/system/179688/,,,,
Immunity Bio Cluster,Existing,Likely,Yes,16.60205999,20.2122284,NVIDIA V100,320,United States of America,ImmunityBio,4/1/2020,,Private,0.209664,6142649.823,,ImmunityBio,V100,"Similarly, ImmunityBio has deployed its 320 GPU cluster, which has always been optimized for and dedicated to molecular modeling of proteins, antibodies, antivirals, and targeted small molecule drugs.",4/1/2020,Uncertain exactly what type of GPU it is,11.28053612,,,,,,,320,NVIDIA,,TRUE,,119,16.60205999,4.00E+16,,4.00E+16,5.0144E+15,0.209664,,1.90781E+11,6142649.823,,,,,,Oak Ridge NL Summit,0.011574074,https://web.archive.org/web/20240910193454/https://immunitybio.com/immunitybio-combines-supercomputing-power-with-microsoft-azure-to-target-infection-doorway-of-the-coronavirus/,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.60205999,20,,300,China,,11/15/2018,,Public,0.2,6000000,China,,,,,,11.30103,,,,,,,300,Anonymized,Anonymized,TRUE,,55,16.60205999,4.00E+16,,4.00E+16,5E+15,0.2,,2E+11,6000000,,,,,,Oak Ridge NL Summit,0.01,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.47712125,20,,200,China,,6/15/2018,,Private,0.2,5000000,China,,,,,,11.17609126,,,,,,,200,Anonymized,Anonymized,TRUE,,24,16.47712125,3.00E+16,,3.00E+16,4E+15,0.2,,1.5E+11,5000000,,,,,,Meta 2017 V100 Cluster,0.01,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.69897,20,,400,China,,6/15/2019,,Private,0.2,7000000,China,,,,,,11.39794001,,,,,,,400,Anonymized,Anonymized,TRUE,,44,16.69897,5.00E+16,,5.00E+16,6E+15,0.2,,2.5E+11,7000000,,,,,,Oak Ridge NL Summit,0.01,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.60205999,20,,300,China,,11/15/2018,,Private,0.2,6000000,China,,,,,,11.30103,,,,,,,300,Anonymized,Anonymized,TRUE,,37,16.60205999,4.00E+16,,4.00E+16,5E+15,0.2,,2E+11,6000000,,,,,,Oak Ridge NL Summit,0.01,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.60205999,20,,300,China,,11/15/2018,,Private,0.2,6000000,China,,,,,,11.30103,,,,,,,300,Anonymized,Anonymized,TRUE,,37,16.60205999,4.00E+16,,4.00E+16,5E+15,0.2,,2E+11,6000000,,,,,,Oak Ridge NL Summit,0.01,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.60205999,20,,300,China,,6/15/2019,,Private,0.2,6000000,China,,,,,,11.30103,,,,,,,300,Anonymized,Anonymized,TRUE,,49,16.60205999,4.00E+16,,4.00E+16,5E+15,0.2,,2E+11,6000000,,,,,,Oak Ridge NL Summit,0.01,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.60205999,20,,300,China,,11/15/2018,,Private,0.2,6000000,China,,,,,,11.30103,,,,,,,300,Anonymized,Anonymized,TRUE,,37,16.60205999,4.00E+16,,4.00E+16,5E+15,0.2,,2E+11,6000000,,,,,,Oak Ridge NL Summit,0.01,,,,,
Anonymized Chinese System,Existing,Likely,Yes,16.60205999,20,,300,China,,11/15/2018,,Public,0.2,6000000,China,,,,,,11.30103,,,,,,,300,Anonymized,Anonymized,TRUE,,54,16.60205999,4.00E+16,,4.00E+16,5E+15,0.2,,2E+11,6000000,,,,,,Oak Ridge NL Summit,0.01,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.60205999,20,,300,China,,11/15/2019,,Public/Private,0.3,7000000,China,,,,,,11.12493874,,,,,,,300,Anonymized,Anonymized,TRUE,,105,16.60205999,4.00E+16,,4.00E+16,5E+15,0.2,0.3,1.33333E+11,7000000,,,,,,Oak Ridge NL Summit,0.01,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.60205999,20,,300,China,,11/15/2018,,Private,0.2,6000000,China,,,,,,11.30103,,,,,,,300,Anonymized,Anonymized,TRUE,,37,16.60205999,4.00E+16,,4.00E+16,5E+15,0.2,,2E+11,6000000,,,,,,Oak Ridge NL Summit,0.01,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.69897,20,,3000,China,,3/15/2012,,Public,1,10000000,China,,,,,,,,,,,,,3000,Anonymized,Anonymized,TRUE,,1,,5.00E+16,5.00E+16,,3E+15,1,,,10000000,,,,,,Sugon 6000 in NSC Shenzhen,1,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.60205999,20,,300,China,,7/15/2019,,Public,0.2,7000000,China,,,,,,11.30103,,,TRUE,,,,300,Anonymized,Anonymized,,,62,16.60205999,4.00E+16,,4.00E+16,5E+15,0.2,,2E+11,7000000,,,,,,Oak Ridge NL Summit,0.01,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.60205999,20,,300,China,,11/15/2018,,Private,0.2,6000000,China,,,,,,11.30103,,,,,,,300,Anonymized,Anonymized,TRUE,,37,16.60205999,4.00E+16,,4.00E+16,5E+15,0.2,,2E+11,6000000,,,,,,Oak Ridge NL Summit,0.01,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.60205999,20,,300,China,,6/15/2019,,Private,0.2,6000000,China,,,,,,11.30103,,,,,,,300,Anonymized,Anonymized,TRUE,,68,16.60205999,4.00E+16,,4.00E+16,5E+15,0.2,,2E+11,6000000,,,,,,Oak Ridge NL Summit,0.01,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.60205999,20,,300,China,,11/15/2018,,Private,0.2,6000000,China,,,,,,11.30103,,,,,,,300,Anonymized,Anonymized,TRUE,,37,16.60205999,4.00E+16,,4.00E+16,5E+15,0.2,,2E+11,6000000,,,,,,Oak Ridge NL Summit,0.01,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.60205999,20,,300,China,,11/15/2018,,Private,0.2,6000000,China,,,,,,11.30103,,,,,,,300,Anonymized,Anonymized,TRUE,,37,16.60205999,4.00E+16,,4.00E+16,5E+15,0.2,,2E+11,6000000,,,,,,Oak Ridge NL Summit,0.01,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.60205999,20,,300,China,,11/15/2018,,Public,0.2,6000000,China,,,,,,11.30103,,,,,,,300,Anonymized,Anonymized,TRUE,,53,16.60205999,4.00E+16,,4.00E+16,5E+15,0.2,,2E+11,6000000,,,,,,Oak Ridge NL Summit,0.01,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.60205999,20,,300,China,,11/15/2018,,Public,0.2,6000000,China,,,,,,11.30103,,,,,,,300,Anonymized,Anonymized,TRUE,,55,16.60205999,4.00E+16,,4.00E+16,5E+15,0.2,,2E+11,6000000,,,,,,Oak Ridge NL Summit,0.01,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.60205999,20,,400,China,,11/15/2018,,Private,0.3,7000000,China,,,,,,11.12493874,,,,,,,400,Anonymized,Anonymized,TRUE,,34,16.60205999,4.00E+16,,4.00E+16,6E+15,0.2,0.3,1.33333E+11,7000000,,,,,,Oak Ridge NL Summit,0.01,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.60205999,20,,300,China,,11/15/2018,,Private,0.2,7000000,China,,,,,,11.30103,,,,,,,300,Anonymized,Anonymized,TRUE,,35,16.60205999,4.00E+16,,4.00E+16,5E+15,0.2,,2E+11,7000000,,,,,,Oak Ridge NL Summit,0.01,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.60205999,20,,300,China,,11/15/2018,,Private,0.2,6000000,China,,,,,,11.30103,,,,,,,300,Anonymized,Anonymized,TRUE,,37,16.60205999,4.00E+16,,4.00E+16,5E+15,0.2,,2E+11,6000000,,,,,,Oak Ridge NL Summit,0.01,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.60205999,20,,300,China,,6/15/2018,,Private,0.2,6000000,China,,,,,,11.30103,,,,,,,300,Anonymized,Anonymized,TRUE,,22,16.60205999,4.00E+16,,4.00E+16,5E+15,0.2,,2E+11,6000000,,,,,,Meta 2017 V100 Cluster,0.01,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.60205999,20,,300,China,,11/15/2018,,Public,0.2,6000000,China,,,,,,11.30103,,,,,,,300,Anonymized,Anonymized,TRUE,,37,16.60205999,4.00E+16,,4.00E+16,5E+15,0.2,,2E+11,6000000,,,,,,Oak Ridge NL Summit,0.01,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.60205999,20,,300,China,,11/15/2018,,Private,0.2,6000000,China,,,,,,11.30103,,,,,,,300,Anonymized,Anonymized,TRUE,,55,16.60205999,4.00E+16,,4.00E+16,5E+15,0.2,,2E+11,6000000,,,,,,Oak Ridge NL Summit,0.01,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.60205999,20,,300,China,,11/15/2018,,Public,0.2,7000000,China,,,,,,11.30103,,,,,,,300,Anonymized,Anonymized,TRUE,,36,16.60205999,4.00E+16,,4.00E+16,5E+15,0.2,,2E+11,7000000,,,,,,Oak Ridge NL Summit,0.01,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.60205999,20,,300,China,,11/15/2018,,Private,0.2,6000000,China,,,,,,11.30103,,,,,,,300,Anonymized,Anonymized,TRUE,,37,16.60205999,4.00E+16,,4.00E+16,5E+15,0.2,,2E+11,6000000,,,,,,Oak Ridge NL Summit,0.01,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.60205999,20,,300,China,,11/15/2018,,Private,0.2,6000000,China,,,,,,11.30103,,,,,,,300,Anonymized,Anonymized,TRUE,,37,16.60205999,4.00E+16,,4.00E+16,5E+15,0.2,,2E+11,6000000,,,,,,Oak Ridge NL Summit,0.01,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.60205999,20,,300,China,,7/15/2020,,Public,0.2,7000000,China,,,,,,11.30103,,,TRUE,,,,300,Anonymized,Anonymized,,,159,16.60205999,4.00E+16,,4.00E+16,5E+15,0.2,,2E+11,7000000,,,,,,Oak Ridge NL Summit,0.01,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.60205999,20,,300,China,,11/15/2018,,Private,0.2,6000000,China,,,,,,11.30103,,,,,,,300,Anonymized,Anonymized,TRUE,,37,16.60205999,4.00E+16,,4.00E+16,5E+15,0.2,,2E+11,6000000,,,,,,Oak Ridge NL Summit,0.01,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.60205999,20,,300,China,,11/15/2018,,Private,0.2,6000000,China,,,,,,11.30103,,,,,,,300,Anonymized,Anonymized,TRUE,,37,16.60205999,4.00E+16,,4.00E+16,5E+15,0.2,,2E+11,6000000,,,,,,Oak Ridge NL Summit,0.01,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.69897,20,,400,China,,11/15/2018,,Private,0.2,7000000,China,,,,,,11.39794001,,,,,,,400,Anonymized,Anonymized,TRUE,,33,16.69897,5.00E+16,,5.00E+16,6E+15,0.2,,2.5E+11,7000000,,,,,,Oak Ridge NL Summit,0.01,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.60205999,20,,300,China,,11/15/2018,,Public,0.2,6000000,China,,,,,,11.30103,,,,,,,300,Anonymized,Anonymized,TRUE,,52,16.60205999,4.00E+16,,4.00E+16,5E+15,0.2,,2E+11,6000000,,,,,,Oak Ridge NL Summit,0.01,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.60205999,20,,300,China,,11/15/2018,,Private,0.2,6000000,China,,,,,,11.30103,,,,,,,300,Anonymized,Anonymized,TRUE,,37,16.60205999,4.00E+16,,4.00E+16,5E+15,0.2,,2E+11,6000000,,,,,,Oak Ridge NL Summit,0.01,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,16.60205999,20,,300,China,,6/15/2019,,Private,0.2,6000000,China,,,,,,11.30103,,,,,,,300,Anonymized,Anonymized,TRUE,,68,16.60205999,4.00E+16,,4.00E+16,5E+15,0.2,,2E+11,6000000,,,,,,Oak Ridge NL Summit,0.01,,,,,
SENAI CIMATEC Ogbon Cimatec/Petrobras,Existing,Confirmed,Yes,16.59106461,19.70692269,NVIDIA Tesla V100 SXM2,312,Brazil,SENAI CIMATEC,11/1/2019,,Public,0.20782944,6653584.107,"Salvador, Brazil",Researchers,V100,Calculated from Top500,11/1/2019,,11.27335754,,,,,,,312,NVIDIA,,TRUE,,127,16.59106461,3.90E+16,,3.90E+16,4.88904E+15,0.20782944,,1.87654E+11,6653584.107,,,,,,Oak Ridge NL Summit,0.011284722,https://web.archive.org/web/20241210024406/https://www.top500.org/system/179703/,,,,
IBM DYEUS,Existing,Confirmed,Yes,16.56820172,18.69631127,NVIDIA V100,296,United States of America,IBM,11/18/2019,,Private,0.19717152,5758048.646,,The Weather Company,V100,the IBM POWER9 supercomputer sports 296 NVIDIA V100 GPUs to chew through so much atmospheric data,11/18/2019,,11.27335754,,,,,,,296,NVIDIA,,TRUE,,136,16.56820172,3.70E+16,,3.70E+16,4.63832E+15,0.19717152,,1.87654E+11,5758048.646,,,,,,Oak Ridge NL Summit,0.010706019,https://web.archive.org/web/20240807082029/https://blogs.nvidia.com/blog/new-ibm-supercomputer-optimized-for-gpus-to-bring-better-weather-predictions-worldwide/,,,,
NVIDIA SATURN V Volta,Existing,Confirmed,Yes,16.51851394,16.67508843,NVIDIA V100,264,United States of America,NVIDIA,11/1/2017,Seems likely that this cluster was folded into Saturn V Phase 2,Private,0.18594576,5219440.77,,NVIDIA,V100,Calculated from Top500,11/1/2017,,11.24912766,,NVIDIA SATURN V Phase 2,,,,,264,NVIDIA,,TRUE,,10,16.51851394,3.30E+16,,3.30E+16,4.13688E+15,0.18594576,,1.77471E+11,5219440.77,,,,,,Meta 2017 V100 Cluster,0.012,https://top500.org/system/179166/,,,,
JAMSTEC ZettaScaler-2.0 Gyoukou,Existing,Likely,Yes,16.40932435,12.96816574,PEZY-SC2,1600,Japan,Japan Agency for Marine-Earth Science and Technology,6/15/2017,"We know that the ZettaScaler 2.2 has 10k processors, can can calculate the number of processors in this supercomputer by scaling by the number of accelerator cores listed in Top500",Public,0.3158,,,Japan Agency for Marine-Earth Science and Technology,PEZY-SC2,Calculated from Top500,2017-06,,10.90991222,,JAMSTEC ZettaScaler-2.2 Gyoukou,,,,,1600,PEZY,,TRUE,,6,16.40932435,2.57E+16,,2.57E+16,1.31072E+16,0.6761664,0.3158,81266624446,,,,,,,Google TensorFlow Research Cloud,0.139236111,https://web.archive.org/web/20241222180018/https://en.wikipedia.org/wiki/Gyoukou,https://www.top500.org/system/179102/,,,
US Government Supercomputer 2,Existing,Confirmed,Yes,16.32204058,10.60705407,NVIDIA Tesla K40c,4160,United States of America,United States Government,6/1/2015,,Public,2.8859,,,US Government,K40,Calculated from Top500,6/1/2015,,,,,,,,,4160,NVIDIA,,TRUE,,3,,2.10E+16,,,2.09914E+16,2.0404384,2.8859,,,,,,,,Oak Ridge NL Titan,0.28545195,https://top500.org/system/178519/,,,,
US Government Supercomputer 1,Existing,Confirmed,Yes,16.32204058,10.60705407,NVIDIA Tesla K40c,4160,United States of America,United States Government,11/1/2014,,Public,2.8859,,,US Government,K40,Calculated from Top500,11/1/2014,,,,,,,,,4160,NVIDIA,,TRUE,,3,,2.10E+16,,,2.09914E+16,2.0404384,2.8859,,,,,,,,Oak Ridge NL Titan,0.28545195,https://top500.org/system/178445/,,,,
NVIDIA SATURN V Phase 1,Existing,Confirmed,Yes,16.26835328,9.373623042,NVIDIA P100,992,United States of America,NVIDIA,11/1/2016,,Private,0.5912816,122587888.5,,NVIDIA,P100,Calculated from Top500,11/1/2016,,10.49655891,,NVIDIA SATURN V Phase 2,,,,,992,NVIDIA,,TRUE,,6,16.26835328,1.86E+16,,1.86E+16,9.2256E+15,0.5912816,,31373206946,122587888.5,,,,,,Oak Ridge NL Titan,0.252258446,https://web.archive.org/web/20240528104941/https://insidehpc.com/2018/04/inside-new-nvidia-dgx-2-supercomputer-nvswitch/,https://top500.org/system/178928/,,,
Meta 2017 P100 Cluster,Existing,Confirmed,Yes,16.26835328,9.373623042,NVIDIA P100,992,United States of America,Meta AI,6/1/2017,,Private,0.6729,,,Meta,P100,Calculated from Top500,6/1/2017,,10.44040275,,,,,,,992,NVIDIA,,TRUE,,8,16.26835328,1.86E+16,,1.86E+16,9.2256E+15,0.5822544,0.6729,27567840690,,,,,,,Google TensorFlow Research Cloud,0.100642361,https://top500.org/system/179068/,,,,
GSIC TSUBAME 2.5,Existing,Confirmed,Yes,16.22066865,8.39890854,NVIDIA Tesla K20X,4224,Japan,Tokyo Institute of Technology,6/1/2014,"1408*3 = 4224
Top500 implies that there are at least 4140",Public,2.6928,63072982.3,"Ookayama, Japan","Academia,Tokyo Tech",K20X,"1,408 Compute Nodes; 3 GPUs [per compute node]",6/1/2014,,,,,,,,,4224,NVIDIA,,TRUE,,3,,1.66E+16,,,1.66214E+16,1.98726528,2.6928,,63072982.3,,,,,,Oak Ridge NL Titan,0.226027397,https://web.archive.org/web/20241008131001/https://www.gsic.titech.ac.jp/sites/default/files/99-yagura.pdf,https://top500.org/system/178249/,,,
Energy Company DD,Existing,Confirmed,Yes,16.14742056,7.095365336,NVIDIA Tesla K80,1728,United States of America,,6/15/2016,,Private,1.23596928,16418883.99,,,K80,Calculated from Top500,2016-06,,,,,,,,,1728,NVIDIA,,TRUE,,6,,1.40E+16,,,1.40417E+16,1.23596928,,,16418883.99,,,,,,Oak Ridge NL Titan,0.190947068,https://top500.org/system/178838/,,,,
Anonymized Chinese System,Existing,Likely,Yes,16,6,,2000,China,,6/15/2016,,Public,1,,China,,,,,,,,,,,,,2000,Anonymized,Anonymized,TRUE,,8,,1.00E+16,,,1E+16,1,,,,,,,,,Oak Ridge NL Titan,0.1,,,,,
Eni HPC2,Existing,Confirmed,Yes,16.07206599,5.965133906,NVIDIA Tesla K20X,3000,Italy,Eni,11/15/2014,,Private,1.41141,44712421.07,,Eni,K20X,Calculated from Top500,2014-11,,,,,,,,,3000,NVIDIA,,TRUE,,5,,1.18E+16,,,1.1805E+16,1.41141,,,44712421.07,,,,,,Oak Ridge NL Titan,0.160530822,https://top500.org/system/178425/,,,,
P3.16xlarge,Existing,Confirmed,,15.90308999,4.04244568,NVIDIA V100,64,,,3/27/2018,They use 8 P13.16xlarge instances together.,Private,0.04402944,,,,V100,,3/27/2018,,11.25934683,,,,,,,64,NVIDIA,,,,,15.90308999,8E+15,,8E+15,1.00288E+15,0.04402944,,1.81697E+11,,,,,,,,,https://aws.amazon.com/blogs/machine-learning/scalable-multi-node-deep-learning-training-using-gpus-in-the-aws-cloud/,Cluster performance database,https://instances.vantage.sh/aws/ec2/p3.16xlarge,,
Anonymized Chinese System,Existing,Confirmed,Yes,15.84509804,4,,7000,China,,11/15/2010,,Public,8,,China,,,,,,,,,,,,,7000,Anonymized,Anonymized,TRUE,,1,,7E+15,,,7E+15,3,8,,,,,,,,Tianhe-1A,1,,,,,
Moscow State University Lomonosov 2,Existing,Confirmed,Yes,15.87085506,3.753265286,NVIDIA Tesla K40m,1472,Russia,Moscow State Univeristy,6/15/2016,,Public,0.859837888,,,Moscow State University,K40m,Calculated from Top500,2016-06,,,,,,,,,1472,NVIDIA,,TRUE,,9,,7.42771E+15,,,7.42771E+15,0.859837888,,,,,,,,,Oak Ridge NL Titan,0.101006075,https://top500.org/system/178444/,,,,
Paper on Minibatch SGD,Existing,Confirmed,Yes,15.68008157,2.418999495,NVIDIA P100,256,United States of America,Meta AI,6/8/2017,,Private,0.1502592,,,,P100,"The training time of ResNet-101 is 92.5 minutes in our
implementation using 256 Tesla P100 GPUs and a minibatch size of 8k",6/8/2017,,10.5032405,,,TRUE,,,,256,NVIDIA,,,,16,15.68008157,4.7872E+15,,4.7872E+15,2.3808E+15,0.1502592,,31859613255,,,,,,,Google TensorFlow Research Cloud,0.025972222,https://arxiv.org/abs/1706.02677,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,15.30103,1,,2000,China,,11/15/2011,,Public,2,,China,,,,,,,,,,,,,2000,Anonymized,Anonymized,TRUE,,2,,2E+15,,,2E+15,0.9,2,,,,,,,,Tianhe-1A,0.3,,,,,
Tesla Buffalo Dojo,Planned,Confirmed,Yes,,,Tesla D1 Dojo,,United States of America,Tesla,,,Private,,,"1339 South Park Ave, Buffalo, NY 14220",Tesla,Dojo,"The governor is correct that this is a Dojo Supercomputer, but $500M, while a large sum, is only equivalent to a 10k H100 system from Nvidia. Tesla will spend more than that on Nvidia hardware this year.",Planned,NY governor announced plans for this in official press conference,,,,,,,,,Tesla,,,,,,,,,,,,,,,,$500M,,,,,https://web.archive.org/web/20240127130129/https://teslanorth.com/2024/01/27/tesla-ai-computer-project-buffalo/,,,,
Coreweave EcoDataCenter,Planned,Unlikely,Yes,,,NVIDIA GB200,,Sweden,CoreWeave,,"Unclear how many GPUs will be in this cluster, but likely thousands",Private,,,"Slaggvarpsvägen 21, 791 77 Falun, Sweden",Cloud,GB200,"As part of its European expansion strategy, CoreWeave will leverage thousands of NVIDIA Blackwell GPUs",Planned 2025,,,,,,,,,,NVIDIA,,,,,,,,,,,,,,,,,,,,,https://web.archive.org/web/20241216210211/https://ecodatacenter.tech/press/ecodatacenter-brings-the-worlds-most-advanced-ai-technology-to-sweden-3339095,,,,
Neevcloud planned GPUs,Planned,Likely,No,,,,40000,India,NeevCloud,,"Seems likely that this will be several clusters. Unclear how many GPUs will be in the largest cluster,",Private,,,India,Cloud,,"NeevCloud, aims to build an AI cloud infrastructure customized for Indian users, with a plan to incorporate 40,000 GPUs by 2026",Planned,,,,,,,,,40000,Unknown,,,,,,,,,,,,,,,,,,,,,https://web.archive.org/web/20250110180448/https://entrepreneurshipstudio.com/news/posts/neevcloud-launches-with-40-000-gpus-revolutionizing-ai-cloud-services-for-smes-in-india,,,,
IREN Sweetwater 2,Planned,Likely,Unclear,,,,,United States of America,IREN,,Located right next to Sweetwater 1,Private,600,,"Sweetwater, TX",,Uncertain,"""Sweetwater 2
2027 Energization
UNDER CONSTRUCTION
Power Capacity
600 MW""",Planned 2027,,,,,,,,,,Unknown,,,,,,,,,,,600,,,,,,,,,,https://web.archive.org/web/20250811194704/https://iren.com/data-centers/sweetwater,,,,
Tata Group GH200 supercomputer,Planned,Unlikely,Yes,,,NVIDIA GH200,,India,Tata Group,,,Private,,,India,"Tata Group,Cloud",GH200,"The first phase of this large-scale deployment is set to commence by the end of 2024, establishing one of the largest Nvidia Hopper GPU-based cloud supercomputers in India",Planned 2024,Very little information about GPU quantity,,,,,,,,,NVIDIA,,,,,,,,,,,,,,,,,,,,,https://web.archive.org/web/20241216183257/https://www.businessworld.in/article/tata-communications-to-set-up-nvidia-hopper-gpu-supercomputer-by-2024e-plans-blackwell-gpu-expansion-in-2025-537162,,,,
Anonymized Chinese System,Existing,Confirmed,,,,,30000,China,,4/15/2025,,,,,China,,,,,,,,,,,,,30000,Anonymized,Anonymized,,,,,,,,,,,,,,,,,,,,,,,,
Oracle Vantage Data Centers Frontier,Planned,Likely,No,,,,,United States of America,Oracle,,Seems to be multiple data centers on one large campus,Private,1400,,"Shackelford County, Texas",,Uncertain,"""1.4GW data center campus in Shackelford County, Texas""
""The campus will be home to 10 data centers""",Planned,,,,,,,,,,Unknown,,,,,,,,,,,1400,,,,,,,,,,https://web.archive.org/web/20250825192457/https://vantage-dc.com/news/vantage-data-centers-unveils-plans-for-frontier-a-25b-mega-campus-in-texas-to-meet-unprecedented-ai-demand/,,,,
OTP SambaNova Supercomputer,Planned,Unlikely,,,,,,,,,"Very unclear if this ever happened or how big it is planned on being. They say ""Europe's fastest AI supercomputer""",Private,,,Europe,,,,Planned,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,https://www.datacenterdynamics.com/en/news/otp-bank-selects-sambanova-systems-for-ai-supercomputer/,,,,
Alibaba Cloud in Saudi Arabia,Existing,Confirmed,No,,,,,Saudi Arabia,Alibaba,6/7/2022,,Private,,,Riyadh,Cloud,,"Alibaba Cloud and Saudi Telecom Company (STC) jointly announced in Riyadh, Saudi Arabia on Tuesday that two data centers in the city were officially opened. The facilities will be operated by Saudi Cloud Computing Company (SCCC), a joint venture established by both parties, and will first provide public cloud services to the Saudi Arabian market.",6/7/2022,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,https://pandaily.com/alibaba-clouds-jv-opens-two-data-centers-in-saudi-arabia/,,,,
Homer City Energy Campus,Planned,Likely,Unclear,,,,,United States of America,,,,Private,4500,,"Homer City, Indiana County, Pennsylvania",,Uncertain,"aim to convert the Homer City Generating Station, a decommissioned 2GW coal-burning power station near Homer City, Indiana County, Pennsylvania into a 4.5GW natural gas powered data center campus spanning more than 3,200 acres",Planned,,,,,,,,,,Unknown,,,,,,,,,,,4500,,,,,,,,,,,,,,
TACC Horizon,Planned,Likely,Yes,,,,,United States of America,University of Texas at Austin,,"Frontera currently has 800 GPUs, which are presumably V100s. For Horizon to have 100x it's AI compute, Horizon would need several thousand GPUs",Public,,,"1300 Louis Henna Blvd, Round Rock, TX 78664","Academia,Researchers",thousands,"Horizon, will provide 10x performance improvement for simulation over the current NSF Leadership-Class Computing system, Frontera, and meet the unique scientific requirements of the NSF community. For AI applications, the leap forward will be even larger, with more than 100x improvement over Frontera",Planned 2026,No clear data yet about number of chips or what type,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,https://web.archive.org/web/20240911080245/https://new.nsf.gov/news/nsf-announces-groundbreaking-computing-facility,https://web.archive.org/web/20240905170840/https://www.datacenterdynamics.com/en/news/tacc-chooses-sabey-data-centers-facility-for-new-supercomputer/,,,
Anonymized Chinese System,Existing,Confirmed,Yes,,,,2000,China,,4/15/2023,,Public/Private,,,China,,,,,,,,,,,,,2000,Anonymized,Anonymized,TRUE,,,,,,,,,,,,,,,,,,,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,,,,,China,,9/15/2021,,Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,TRUE,,,,,,,,,,,,,,,,,,,,,,,
Google south Columbus Ohio,Planned,Likely,Yes,,,,,United States of America,Google,,,Private,,,"5076 S High St, Lockbourne, OH 43137",Google,,,Planned,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,https://web.archive.org/web/20241119062654/https://semianalysis.com/2024/09/04/multi-datacenter-training-openais/,,,,
Alibaba Cloud in Australia,Decommissioned,Confirmed,,,,,,Australia,Alibaba,7/1/2016,"As part of Alibaba Cloud’s infrastructure strategy update, following careful assessment, we have decided to cease operations at our data centers in Australia and Indi",Private,,,Sydney,Cloud,,,2016,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,https://www.datacenterdynamics.com/en/news/alibaba-to-exit-data-centers-in-australia-and-india/,,,,
Anonymized Chinese System,Existing,Likely,Yes,,,,,China,,,,Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,TRUE,,,,,,,,,,,,,,,,,,,,,,,
IREN Childress,Planned,Likely,Unclear,,,,,United States of America,IREN,,,Private,750,,"Childress, TX",,Uncertain,"""750 MW
Total Capacity""",Planned,,,,,,,,,,Unknown,,,,,,,,,,,750,,,,,,,,,,https://web.archive.org/web/20250811205238/https://iren.com/data-centers/childress,,,,
Alibaba Cloud in UAE,Existing,Confirmed,No,,,,,United Arab Emirates,Alibaba,7/1/2016,,Private,,,Dubai,Cloud,,,2016,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,https://web.archive.org/web/20250820085405/https://www.alibabacloud.com/en/global-locations?_p_lc=1,,,,
IndiaAI Mission Supercomputer,Planned,Likely,Yes,,,,10000,India,,,"There were earlier talk about potentially getting 25k GPUs, which still seems to be on the table. Unclear if this will be one cluster or several",Public/Private,,,,Cloud,,"""The ecosystem will comprise AI compute infrastructure of 10,000 or more Graphics Processing Units (GPUs), built through public-private partnership.""
""In October 2023, the Indian government initiated a discussion regarding a proposal to set up 25,000 GPUs in the country, with the vision to make compute accessible to Indian companies engaged in the development and use of AI.... Subsequently, in March 2024, the cabinet approved a Rs. 10,372-crore outlay for the IndiaAI Mission, under which the government aims to establish compute capacity of at least 10,000 GPUs through the public-private partnership model.""",Planned,,,,,,,,,10000,Unknown,,,,,,,,,,,,,,,,,,,,,https://web.archive.org/web/20240902090402/https://pib.gov.in/PressReleaseIframePage.aspx?PRID=2012355,https://web.archive.org/web/20240919153825/https://carnegieendowment.org/posts/2024/04/a-primer-on-compute?lang=en,,,
Brazil Scala AI City Phase 1,Planned,Likely,Unclear,,,,,Brazil,Scala Data Centers,,,Private,54,,"Bom Retiro, Eldorado do Sul - State of Rio Grande do Sul, 92990-000, Brazil",,,It will see an initial 3 billion real investment (US$490m) and will offer 54MW of capacity,Planned 2026,Very few details about anything,,,,,,,,,Unknown,,,,,,,,,,,54,,,,,,,,,,https://web.archive.org/web/20241227122627/https://www.datacenterdynamics.com/en/news/brazilian-local-government-passes-law-for-scalas-ai-city/,,,,
Alibaba Cloud in Malaysia Phase 1,Existing,Confirmed,No,,,,,Malaysia,Alibaba,7/1/2017,,Private,,,Kuala Lumpur,Cloud,,,2017,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,https://web.archive.org/web/20250820085405/https://www.alibabacloud.com/en/global-locations?_p_lc=1,,,,
Anonymized Chinese System,Existing,Likely,Unclear,,,,7000,China,,4/15/2023,,Private,,,China,,,,,,,,,,,,,7000,Anonymized,Anonymized,,,,,,,,,,,,,,,,,,,,,,,,
Alibaba Cloud in Thailand Phase 1,Existing,Confirmed,Yes,,,,,Thailand,Alibaba,7/1/2022,,Private,,,Bangkok,Cloud,,,2022,,,,,,,,,,Unknown,,TRUE,,,,,,,,,,,,,,,,,,,https://web.archive.org/web/20250820085405/https://www.alibabacloud.com/en/global-locations?_p_lc=1,,,,
Brazil Scala AI City Phase 2,Planned,Unlikely,Unclear,,,,,Brazil,Scala Data Centers,,Planned date is a very rough guess,Private,4750,,"Bom Retiro, Eldorado do Sul - State of Rio Grande do Sul, 92990-000, Brazil",,Uncertain,"the entire campus is expected to have a capacity of 4.75GW at full build-out, once all phases are complete",Planned 2033,Very few details about anything,,Brazil Scala AI City Phase 1,,,,,,,Unknown,,,,,,,,,,,4750,,,,,,,,,,https://web.archive.org/web/20241227122627/https://www.datacenterdynamics.com/en/news/brazilian-local-government-passes-law-for-scalas-ai-city/,,,,
HPE GreenLake for LLMs Q01,Existing,Unlikely,Unclear,,,,1000,Canada, Hewlett Packard Enterprise,,,Private,,,"2280 Rue Albert-Dion, Lévis, QC G7A 5M9, Canada",Cloud,"H100,L40,L4,""thousands""",GreenLake for LLMs gives users access to AI-native architecture specifically designed for AI and simulation workloads on hundreds or thousands of GPUs and CPUs at once,Planned for end of 2023,Unclear what type of GPUs they use and how many,,,,,,,,1000,Unknown,,,,,,,,,,,,,,,,,,,,,https://web.archive.org/web/20240918133952/https://www.datacenterdynamics.com/en/news/hpe-greenlake-gets-ai-and-hpc-offering/,,,,
Alibaba Cloud in Thailand Phase 2,Existing,Confirmed,Yes,,,,,Thailand,Alibaba,1/15/2025,,Private,,,Bangkok,Cloud,,,2025-01,,,,,,,,,,Unknown,,TRUE,,,,,,,,,,,,,,,,,,,https://www.alibabacloud.com/blog/alibaba-cloud-celebrates-10-years-in-singapore-with-new-data-centers-and-ai-global-competency-center_602337,,,,
Anonymized Chinese System,Planned,Confirmed,Yes,,,,,China,,,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,,,,,,,,,,,,,,,,,,,,,
Google New Albany Ohio,Planned,Likely,Yes,,,,,United States of America,Google,,,Private,450,,"1101 Beech Rd SW, New Albany, OH 43054",Google,,"""450MW campus in Ohio to train Gemini, to expand further to 540MW""
""The New Albany cluster, shown below, is set to become one of Google’s largest and is already hosting TPU v4, v5, v6.""",Planned 2025,,,,,,,,,,Unknown,,,,,,,,,,,450,,,,,,,,,,https://www.youtube.com/watch?v=hobvps-H38o,https://web.archive.org/web/20241119062654/https://semianalysis.com/2024/09/04/multi-datacenter-training-openais/,,,
Anonymized Chinese System,Planned,Likely,Yes,,,,10000,China,,,,Private,,,China,,,,,,,,,,,,,10000,Anonymized,Anonymized,,,,,,,,,,,,,,,,,,,,,,,,
Anonymized Chinese System,Planned,Likely,Yes,,,,,China,,,,Public,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,,,,,,,,,,,,,,,,,,,,,
Microsoft Air Gapped Supercomputer,Existing,Confirmed,Yes,,,,,United States of America,Microsoft,5/7/2024,,Private,,,"Iowa, USA",US intelligence agencies,,"Microsoft Corp. has deployed a generative AI model entirely divorced from the internet, saying US intelligence agencies can now safely harness the powerful technology to analyze top-secret information.",5/7/2024,No details on specifics,,,,,,,,,Unknown,,TRUE,,,,,,,,,,,,,,,,,,,https://web.archive.org/web/20240626134614/https://qz.com/microsoft-offline-ai-service-chatgpt-gpt4-us-spies-1851462266,,,,
Google Lancaster Ohio,Planned,Likely,Yes,,,,,United States of America,Google,,,Private,,,"231 Whiley Rd, Lancaster, OH 43130",Google,,,Planned,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,https://web.archive.org/web/20241119062654/https://semianalysis.com/2024/09/04/multi-datacenter-training-openais/,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,,,,,Hong Kong,,9/15/2024,,Public/Private,,,Hong Kong,,,,,,,,,,,,,,Anonymized,Anonymized,TRUE,,,,,,,,,,,,,,,,,,,,,,,
Alibaba Cloud in Philippines Phase 2,Planned,Confirmed,Yes,,,,,Philippines (the),Alibaba,10/15/2025,,Private,,,Manila,Cloud,,,2025-10,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,https://www.alibabacloud.com/blog/alibaba-cloud-celebrates-10-years-in-singapore-with-new-data-centers-and-ai-global-competency-center_602337,,,,
Alibaba Cloud in Singapore,Existing,Confirmed,No,,,,,Singapore,Alibaba,7/2/2015,,Private,,,Singapore,Cloud,,,7/2/2015,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,https://www.alibabacloud.com/blog/alibaba-cloud-celebrates-10-years-in-singapore-with-new-data-centers-and-ai-global-competency-center_602337,,,,
Alibaba Cloud in South Korea Phase 2,Existing,Confirmed,Yes,,,,,Korea (Republic of),Alibaba,6/15/2025,,Private,,,Seoul,Cloud,,,2025-06,,,,,,,,,,Unknown,,TRUE,,,,,,,,,,,,,,,,,,,https://www.alibabacloud.com/blog/alibaba-cloud-celebrates-10-years-in-singapore-with-new-data-centers-and-ai-global-competency-center_602337,,,,
SingularityNET,Planned,Likely,No,,,,,,SingularityNET,,Seems like it's decentralized and spread around the world,Private,,,,,,"SingularityNET plans to build the first of several modular compute containers that will act as decentralized hubs for a network of devices from fellow AI developers Fetch.ai, Ocean Protocol, NuNet, HyperCycle, as well as itself. Thanks to their design, the containers can be placed and relocated worldwide.",Planned,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,https://web.archive.org/web/20240724001101/https://decrypt.co/241118/singularitynet-53-million-ai-supercomputer-data-centers,,,,
Crusoe Cheyenne Wyoming,Planned,Likely,Unclear,,,,,United States of America,Crusoe,,,Private,1800,,,,Uncertain,"""develop a 1.8GW AI data center campus located in southeast Wyoming""
""Reportedly designed to scale up to 10GW""",Planned,,,,,,,,,,Unknown,,,,,,,,,,,1800,,,,,,,,,,https://archive.ph/bVPBR,,,,
Google Omaha Nebraska,Planned,Likely,Yes,,,,,United States of America,Google,,,Private,,,"11110 State St, Omaha, NE 68142",Google,,,Planned,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,https://web.archive.org/web/20241119062654/https://semianalysis.com/2024/09/04/multi-datacenter-training-openais/,,,,
Cambridge Dawn Phase 2,Planned,Likely,Yes,,,,,United Kingdom of Great Britain and Northern Ireland,UK Research and Innovation,,Unclear if Phase 2 will actually happen,Public,,,"University of Cambridge, West Data Centre, Ada Lovelace Rd, Cambridge CB3 0QX, United Kingdom","scientists within Cambridge and across the UK in critical research fields such as clean energy,personalised medicine and climate",,"aiming to deliver a Phase 2 supercomputer in 2024 which will boast 10 times the level of performance. If taken forward, Dawn Phase 2 would significantly boost the UK AI capability",Planned Q4 2024,"Whether or not they're moving forward with phase 2 remains ""up in the air""",,Cambridge Dawn Phase 1,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,https://web.archive.org/web/20240913113027/https://www.datacenterdynamics.com/en/news/fastest-ai-supercomputer-in-the-uk-is-now-operational/,https://www.theregister.com/2023/11/13/intel_dawn_ai_uk/?utm_source=chatgpt.com,,,
Anonymized Chinese System,Planned,Likely,Unclear,,,,,China,,,,Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,,,,,,,,,,,,,,,,,,,,,
Alibaba Cloud in Silicon Valley,Existing,Confirmed,No,,,,,United States of America,Alibaba,7/1/2014,,Private,,,Silicon Valley,Cloud,,,2014,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,https://web.archive.org/web/20250820085405/https://www.alibabacloud.com/en/global-locations?_p_lc=1,,,,
Alibaba Cloud in Mexico,Existing,Confirmed,Yes,,,,,Mexico,Alibaba,2/15/2025,,Private,,,Querétaro,Cloud,,,2025-02,,,,,,,,,,Unknown,,TRUE,,,,,,,,,,,,,,,,,,,https://www.alibabacloud.com/blog/alibaba-cloud-celebrates-10-years-in-singapore-with-new-data-centers-and-ai-global-competency-center_602337,,,,
Anonymized Chinese System,Existing,Likely,Yes,,,,,China,,1/15/2024,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,TRUE,,,,,,,,,,,,,,,,,,,,,,,
Alibaba Cloud in Japan,Existing,Confirmed,No,,,,,Japan,Alibaba,7/1/2016,,Private,,,Tokyo,Cloud,,,2016,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,https://web.archive.org/web/20250820085405/https://www.alibabacloud.com/en/global-locations?_p_lc=1,,,,
Alibaba Cloud in United Kingdom,Existing,Confirmed,No,,,,,United Kingdom of Great Britain and Northern Ireland,Alibaba,7/1/2018,,Private,,,London,Cloud,,,2018,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,https://web.archive.org/web/20250820085405/https://www.alibabacloud.com/en/global-locations?_p_lc=1,,,,
NVIDIA Israel Blackwell Supercomputer,Planned,Likely,Yes,,,NVIDIA GB200,,Israel,NVIDIA,,Very unclear how big this will actually be,Private,30,,"J3P7+8P Eliakim, Israel",,GB200,"is expected to include ""thousands"" of graphics processing units",Planned,,,,,,,,,,NVIDIA,,,,,,,,,,,30,,,,,$500 million,,,,,https://web.archive.org/web/20250116151639/https://www.aibase.com/news/14757,,,,
Alibaba Cloud in Philippines Phase 1,Existing,Confirmed,Yes,,,,,Philippines (the),Alibaba,7/1/2021,,Private,,,Manila,Cloud,,,2021,,,,,,,,,,Unknown,,TRUE,,,,,,,,,,,,,,,,,,,https://web.archive.org/web/20250820085405/https://www.alibabacloud.com/en/global-locations?_p_lc=1,,,,
"""Crusoe Goodnight in Claude, Texas""",Planned,Likely,Unclear,,,,,United States of America,Crusoe,,,Private,1000,,"10001 Lima Rd, Claude, TX 79019",,Uncertain,"a two-phase, seven-building campus with more than 1 GW of planned capacity",Planned,,,,,,,,,,Unknown,,,,,,,,,,,1000,,,,,,,,,,https://web.archive.org/web/20250825192439/https://www.aterio.io/blog/crusoe-launches-usd29b-goodnight-campus-in-claude-mirroring-openai-s-stargate,,,,
Alibaba Cloud in Virginia,Existing,Confirmed,No,,,,,United States of America,Alibaba,7/1/2015,,Private,,,Virginia,Cloud,,,2015,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,https://web.archive.org/web/20250820085405/https://www.alibabacloud.com/en/global-locations?_p_lc=1,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,,,,,China,,10/15/2023,,Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,TRUE,,,,,,,,,,,,,,,,,,,,,,,
Google Lincoln Nebraska,Planned,Likely,Yes,,,,,United States of America,Google,,,Private,,,"W933+CH4 Lincoln, Nebraska",Google,,The other two sites are not as large yet but are ramping up fast: combining all four campuses will form a GW-scale AI training cluster by 2026. The Lincoln datacenter that is ~50 miles away will be Google’s largest individual site.,Planned,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,https://web.archive.org/web/20241119062654/https://semianalysis.com/2024/09/04/multi-datacenter-training-openais/,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,,,,,China,,9/15/2023,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,TRUE,,,,,,,,,,,,,,,,,,,,,,,
CoreWeave Hillsboro Oregon,Planned,Likely,Yes,,,,,United States of America,"CoreWeave,Digital Realty",,"Unclear when this became operational or will be, and what type of GPUs are used",Private,,,"Hillsboro, Oregon",CoreWeave,Uncertain,"CoreWeave has secured a 36 MW lease with Digital Realty at one of its data centers in Hillsboro, Oregon, to deploy tens of thousands of GPUs in a single facility",Planned,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,https://web.archive.org/web/20250126143127/https://dgtlinfra.com/coreweave-data-center-locations/,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,,,,,China,,5/15/2022,,Public,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,TRUE,,,,,,,,,,,,,,,,,,,,,,,
S. Korea 7th national supercomputer,Planned,Likely,Yes,,,,,Korea (Republic of),Ministry of Science and ICT,,"Unclear what precision they're using, or what type of chips",Public,,,Korea (Republic of),Researchers,,The country plans to start the development of its seventh supercomputer in 2025 with an exaflop of performance,Planned,Only able to find one news report,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,https://web.archive.org/web/20240117215326/https://www.kedglobal.com/artificial-intelligence/newsView/ked202305310010,,,,
Anonymized Chinese System,Planned,Likely,Unclear,,,,,China,,,,Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,,,,,,,,,,,,,,,,,,,,,
Singtel DC Tuas,Planned,Likely,Yes,,,NVIDIA H100 SXM5 80GB,,Singapore,Singtel,,"No information on how large this cluster will be, but the datacenter will be 58MW",Private,58,,"9 Tuas Ave 3
639408 Singapore",Cloud,"H100,GB200",Singtel’s GPUaaS will be powered by Nvidia H100 Tensor Core GPU-powered clusters that are operated in existing upgraded data centers in Singapore,Planned,,,,,,,,,,NVIDIA,,,,,,,,,,,58,,,,,,,,,,https://web.archive.org/web/20241210030636/https://www.datacenterdynamics.com/en/news/singtel-and-nscale-announce-partnership-to-boost-gpu-capacity-across-europe-and-southeast-asia/,,,,
Google Papillion Nebraska,Planned,Likely,Yes,,,,,United States of America,Google,,,Private,250,,"14706 Schram Rd, Omaha, NE 68138",Google,,"The Papillion campus shown below adds >250MW of capacity to Google’s operations around Omaha and Council Bluffs, which combined with the above totals north of 500MW of capacity in 2023, of which a large portion is allocated to TPUs",Planned,,,,,,,,,,Unknown,,,,,,,,,,,250,,,,,,,,,,https://web.archive.org/web/20241119062654/https://semianalysis.com/2024/09/04/multi-datacenter-training-openais/,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,,,,,China,,10/15/2024,,Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,TRUE,,,,,,,,,,,,,,,,,,,,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,,,,50000,China,,7/15/2013,,Public,20,200000000,China,,,,,,,,,,,,,50000,Anonymized,Anonymized,TRUE,,,,,,,,30,20,,200000000,,,,,,,,,,,,
Google Council Bluff Iowa,Planned,Likely,Yes,,,,,United States of America,Google,,,Private,,,"10410 Bunge Ave, Council Bluffs, IA 51503",Google,,,Planned,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,https://web.archive.org/web/20241119062654/https://semianalysis.com/2024/09/04/multi-datacenter-training-openais/,,,,
Alibaba Cloud in India,Decommissioned,Confirmed,,,,,,India,Alibaba,7/1/2018,"As part of Alibaba Cloud’s infrastructure strategy update, following careful assessment, we have decided to cease operations at our data centers in Australia and Indi",Private,,,Mumbai,Cloud,,,2018,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,https://www.datacenterdynamics.com/en/news/alibaba-to-exit-data-centers-in-australia-and-india/,,,,
IREN Sweetwater 1,Planned,Likely,Unclear,,,,,United States of America,IREN,,Located right next to Sweetwater 2,Private,1400,,"Sweetwater, TX",,Uncertain,"""2026 Energization
UNDER CONSTRUCTION
Power Capacity
1,400 MW""",Planned 2026,,,,,,,,,,Unknown,,,,,,,,,,,1400,,,,,,,,,,https://web.archive.org/web/20250811194704/https://iren.com/data-centers/sweetwater,,,,
Oracle $10B Saudi Arabia Investment,Planned,Likely,No,,,,,Saudi Arabia,Oracle,,Unclear how this money will be invested,Private,,,,Cloud,Uncertain,Oracle has committed to investing $14 billion in Saudi Arabia over the next 10 years to expand its cloud and AI offerings in the region.,Planned,Unclear what this investment will entail,,,,,,,,,Unknown,,,,,,,,,,,,,,14000000,,,,,,,http://web.archive.org/web/20250515192746/https://www.datacenterdynamics.com/en/news/oracle-commits-to-invest-14bn-in-saudi-arabia-over-next-10-years/,,,,
Alibaba Cloud in Indonesia,Existing,Confirmed,No,,,,,Indonesia,Alibaba,7/1/2018,,Private,,,Jakarta,Cloud,,,2018,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,https://web.archive.org/web/20250820085405/https://www.alibabacloud.com/en/global-locations?_p_lc=1,,,,
Graphcore Good Computer,Planned,Unlikely,Yes,,,,8192,,Graphcore,,"Unclear if this project is still happening, no news since 2022, Graphcore recently acquired by Softbank",Private,,,,,Bow IPU,"""This machine will contain 8,192 IPUs of a generation beyond the Bow processor, but further leveraging 3D wafer-on-wafer stacking technology""

""adding that the machine will deliver “over 10 exaflops” of floating-point performance""",completion unclear,"Unclear if this project is still happening, no news since 2022, Graphcore recently acquired by Softbank",,,,,,,,8192,Unknown,,,,,,,,,,,,,,,,$130M,,,,,https://web.archive.org/web/20220523232610/https://www.enterpriseai.news/2022/03/04/graphcore-announces-wafer-on-wafer-ipu-good-computer/,,,,
Telangana Yotta Hyderbad AI City Cluster Phase 2,Planned,Likely,Yes,,,,25000,India,Government of Telangana,,Unclear exactly which chips they plan to use,Public,50,,"Hyderabad, India",Cloud,,"Once fully built, the AI Supercomputer powered by 25,000 GPUs and the 50 MW AI Cloud Data Centre campus",Planned,,,Telangana Yotta H1 Hyderbad AI City Cluster Phase 1,,,,,,25000,Unknown,,,,,,,,,,,50,,,,,,,,,,https://web.archive.org/web/20240906230223/https://www.crn.in/news/government-of-telangana-partners-with-yotta-to-launch-indias-largest-ai-supercomputer-of-25000-high-performance-gpus-in-a-purpose-built-50-mw-ai-cloud-data-centre-campus-in-hyderabad/,,,,
Alibaba Cloud in South Korea Phase 1,Existing,Confirmed,Yes,,,,,Korea (Republic of),Alibaba,7/1/2022,,Private,,,Seoul,Cloud,,,2022,,,,,,,,,,Unknown,,TRUE,,,,,,,,,,,,,,,,,,,https://web.archive.org/web/20250820085405/https://www.alibabacloud.com/en/global-locations?_p_lc=1,,,,
Wonder Valley Datacenter Phase 1,Planned,Unlikely,Unclear,,,,,Canada,,,,Private,1400,,"Alberta, Canada",,Uncertain,"""the project's first phase will include developing a 1.4GW redundant power solution and subsequent annual rollout of redundant power in 1GW increments""
""generate and offer 7.5GW of low-cost power to hyperscalers over the next five-10 years""",Planned 2027,,,,,,,,,,Unknown,,,,,,,,,,,1400,,,,,,,,,,https://archive.ph/c5kTp,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,,,,,China,,9/15/2022,,Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,TRUE,,,,,,,,,,,,,,,,,,,,,,,
Alibaba Cloud in Germany,Existing,Confirmed,No,,,,,Germany,Alibaba,7/1/2016,,Private,,,Frankfurt,Cloud,,,2016,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,https://web.archive.org/web/20250820085405/https://www.alibabacloud.com/en/global-locations?_p_lc=1,,,,
Meta Temple Texas,Planned,Likely,Yes,,,,,United States of America,Meta AI,,"They were building one design for awhile then stopped, tore it down, and started building a new design",Private,,,"NW H K Dodgen Loop & Industrial Blvd
76502 Temple
Texas, USA",Meta,,,Planned,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,https://www.youtube.com/watch?v=hobvps-H38o,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,,,,,China,,11/15/2020,,Public,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,TRUE,,,,,,,,,,,,,,,,,,,,,,,
Anonymized Chinese System,Planned,Likely,Yes,,,,,China,,,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,,,,,,,,,,,,,,,,,,,,,
X/xAI Hillsboro Oregon,Planned,Likely,Yes,,,,,United States of America,Digital Realty,,Not totally confident in this report since I don't know the original source,Private,,,"Hillsboro, Oregon","X,xAI",Uncertain,"""X Corp leases over 50 megawatts (MW) of power capacity at Digital Realty-owned data centers in Hillsboro, Oregon""",Planned,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,https://web.archive.org/web/20250619023139/https://dgtlinfra.com/elon-musk-data-centers/,,,,
NHRI Taiwan Supercomputer (NHRI-1),Planned,Likely,Yes,,,,,Taiwan,National Health Research Institutes Taiwan,,Very few details about any specifics,Public,,,Taiwan,Biomedical research,,The National Health Research Institutes (NHRI) in Taiwan has announced a partnership with Nvidia and Asus to deliver the nation’s first biomedical supercomputer,Planned,Very few details,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,https://web.archive.org/web/20220605020815/https://www.hpcwire.com/2022/04/05/nvidia-asus-cloud-team-for-taiwanese-biomedical-supercomputer/,,,,
Anonymized Chinese System,Existing,Likely,Yes,,,,200000,China,,12/15/2023,,Public,80,,China,,,,,,,,,,,,,200000,Anonymized,Anonymized,TRUE,,,,,,,,80,,,,,,,,,,,,,,,
Anonymized Chinese System,Planned,Unlikely,Unclear,,,,,China,,,,Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,,,,,,,,,,,,,,,,,,,,,
Alibaba Cloud in Malaysia Phase 2,Existing,Confirmed,Yes,,,,,Malaysia,Alibaba,7/1/2025,,Private,,,Kuala Lumpur,Cloud,,,7/1/2025,,,,,,,,,,Unknown,,TRUE,,,,,,,,,,,,,,,,,,,https://www.alibabacloud.com/blog/alibaba-cloud-celebrates-10-years-in-singapore-with-new-data-centers-and-ai-global-competency-center_602337,,,,
Anonymized Chinese System,Existing,Confirmed,Yes,,,,500,China,,7/15/2018,,Public,,,China,,,,,,,,,,,,,500,Anonymized,Anonymized,TRUE,,,,,,,,,,,,,,,,,,,,,,,
SoftBank Planned GB200,Planned,Likely,Yes,,,NVIDIA GB200,,Japan,Softbank,,,Private,,,Japan,Softbank,GB200,"In addition to its DGX SuperPOD, SoftBank plans to build another NVIDIA-accelerated supercomputer to run extremely compute-intensive workloads. Initial plans for the supercomputer are based on an NVIDIA Grace Blackwell platform design featuring NVIDIA GB200 NVL72 multi-node, liquid-cooled, rack-scale systems that combine NVIDIA Blackwell GPUs with power-efficient Arm-based NVIDIA Grace™ CPUs",Planned,,,,,,,,,,NVIDIA,,,,,,,,,,,,,,,,,,,,,https://web.archive.org/web/20241116120654/https://nvidianews.nvidia.com/news/nvidia-and-softbank-accelerate-japans-journey-to-global-ai-powerhouse,,,,
Anonymized Chinese System,Planned,Likely,Yes,,,,,China,,,,Public/Private,,,China,,,,,,,,,,,,,,Anonymized,Anonymized,,,,,,,,,,,,,,,,,,,,,,,,
